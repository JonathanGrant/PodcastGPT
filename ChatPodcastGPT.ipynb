{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76a139c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "import tempfile\n",
    "import IPython\n",
    "import shutil\n",
    "import enum\n",
    "import jonlog\n",
    "import json\n",
    "from gtts import gTTS\n",
    "import uuid\n",
    "import datetime as dt\n",
    "import requests\n",
    "import concurrent.futures\n",
    "import base64\n",
    "from github import Github\n",
    "import time\n",
    "import threading\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import io\n",
    "import retrying\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import concurrent.futures\n",
    "import tempfile\n",
    "import shutil\n",
    "import subprocess\n",
    "import logging\n",
    "import pydub # Still needed for fallback merge or potential segment validation\n",
    "import IPython # For display in notebooks\n",
    "import pydub\n",
    "from xml.dom import minidom\n",
    "from xml.etree import ElementTree as ET\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import requests\n",
    "import vertexai\n",
    "import vertexai.preview.generative_models\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage as MistralChatMessage\n",
    "import anthropic\n",
    "import groq\n",
    "import google.generativeai\n",
    "\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    if 'IPKernelApp' in get_ipython().config:\n",
    "        from tqdm.notebook import tqdm\n",
    "    else:\n",
    "        from tqdm import tqdm\n",
    "except:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "logger = jonlog.getLogger()\n",
    "openai.api_key = os.environ.get(\"OPENAI_KEY\", None) or open('/Users/jong/.openai_key').read().strip()\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = 'summer2023-392312'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "937ac4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimited:\n",
    "    def __init__(self, max_per_minute):\n",
    "        self.max_per_minute = max_per_minute\n",
    "        self.current_minute = time.strftime('%M')\n",
    "        self.lock = threading.Lock()\n",
    "        self.calls = 0\n",
    "\n",
    "    def __call__(self, fn):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            run = False\n",
    "            with self.lock:\n",
    "                current_minute = time.strftime('%M')\n",
    "                if current_minute != self.current_minute:\n",
    "                    self.current_minute = current_minute\n",
    "                    self.calls = 0\n",
    "                if self.calls < self.max_per_minute:\n",
    "                    self.calls += 1\n",
    "                    run = True\n",
    "            if run:\n",
    "                return fn(*args, **kwargs)\n",
    "            else:\n",
    "                time.sleep(15)\n",
    "                return wrapper(*args, **kwargs)\n",
    "                    \n",
    "        return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6256344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElevenLabsTTS:\n",
    "    WOMAN = 'EXAVITQu4vr4xnSDxMaL'\n",
    "    MAN = 'VR6AewLTigWG4xSOukaG'\n",
    "    BRIT_WOMAN = 'jnBYJClnH7m3ddnEXkeh'\n",
    "    def __init__(self, voice_id=None):\n",
    "        api_key_fpath='/Users/jong/.elevenlabs_apikey'\n",
    "        with open(api_key_fpath) as f:\n",
    "            self.api_key = f.read().strip()\n",
    "        self._voice_id = voice_id or self.WOMAN\n",
    "        self.uri = \"https://api.elevenlabs.io/v1/text-to-speech/\" + self._voice_id\n",
    "        \n",
    "    @retrying.retry(stop_max_attempt_number=5, wait_fixed=2000)\n",
    "    def tts(self, text):\n",
    "        headers = {\n",
    "            \"accept\": \"audio/mpeg\",\n",
    "            \"xi-api-key\": self.api_key,\n",
    "        }\n",
    "        payload = {\n",
    "            \"text\": text,\n",
    "        }\n",
    "        return requests.post(self.uri, headers=headers, json=payload).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26f11de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GttsTTS:\n",
    "    WOMAN = 'us'\n",
    "    MAN   = 'co.in'\n",
    "    def __init__(self, voice_id=None):\n",
    "        self.tld = voice_id\n",
    "\n",
    "    @retrying.retry(stop_max_attempt_number=5, wait_fixed=2000)\n",
    "    def tts(self, text):\n",
    "        speech = gTTS(text=text, lang='en', tld=self.tld, slow=False)\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            temp_filename = f'{tmpdir}/audio'\n",
    "            speech.save(temp_filename)\n",
    "            with open(temp_filename, 'rb') as f:\n",
    "                return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e752b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAITTS:\n",
    "    \"\"\"\n",
    "    Generates speech from text using the OpenAI Text-to-Speech API.\n",
    "    See: https://platform.openai.com/docs/guides/text-to-speech\n",
    "    \"\"\"\n",
    "\n",
    "    VOICES = {\n",
    "        \"alloy\",\n",
    "        \"ash\",\n",
    "        \"ballad\",\n",
    "        \"coral\",\n",
    "        \"echo\",\n",
    "        \"fable\",\n",
    "        \"onyx\",\n",
    "        \"nova\",\n",
    "        \"sage\",\n",
    "        \"shimmer\",\n",
    "    }\n",
    "\n",
    "    MAN = \"onyx\"\n",
    "    WOMAN = \"coral\"\n",
    "\n",
    "    def __init__(self, voice='alloy', model='gpt-4o-mini-tts', api_key=None):\n",
    "        \"\"\"\n",
    "        Initializes the OpenAI TTS client.\n",
    "\n",
    "        Args:\n",
    "            voice (str): The voice to use. Available voices:\n",
    "                         alloy, ash, ballad, coral, echo, fable, onyx, nova, sage, shimmer\n",
    "                         Defaults to 'alloy'.\n",
    "            model (str): The TTS model to use. Available models:\n",
    "                         gpt-4o-mini-tts (recommended), tts-1, tts-1-hd\n",
    "                         Defaults to 'gpt-4o-mini-tts'.\n",
    "            api_key (str, optional): Your OpenAI API key. If None, the client\n",
    "                                     will try to use the OPENAI_API_KEY environment\n",
    "                                     variable. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.voice = voice\n",
    "        self.model = model\n",
    "        # Initialize the client once\n",
    "        # If api_key is provided, use it; otherwise, OpenAI() will look for env var\n",
    "        self.client = openai.OpenAI(api_key=api_key if api_key else openai.api_key) # Maintains compatibility if openai.api_key was set globally\n",
    "\n",
    "    @RateLimited(95) # Keep your original decorator\n",
    "    @jonlog.retry_with_logging() # Keep your original decorator\n",
    "    def tts(self, text: str, instructions: str = None):\n",
    "        \"\"\"\n",
    "        Converts text to speech.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text content to synthesize.\n",
    "            instructions (str, optional): Specific instructions for the speech\n",
    "                                          generation (e.g., tone, accent, speed).\n",
    "                                          Only applicable for models like gpt-4o-mini-tts.\n",
    "                                          Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            bytes: The raw audio content (typically MP3 format by default).\n",
    "\n",
    "        Raises:\n",
    "            openai.APIError: If the API request fails.\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            \"model\": self.model,\n",
    "            \"voice\": self.voice,\n",
    "            \"input\": text,\n",
    "        }\n",
    "        # Only add instructions if provided and the model supports it (implicitly assuming gpt-4o-mini-tts does)\n",
    "        if instructions:\n",
    "             # Check if the model likely supports instructions\n",
    "             # A simple check, you might want more robust logic if needed\n",
    "            if 'gpt-4o' in self.model:\n",
    "                params[\"instructions\"] = instructions\n",
    "            else:\n",
    "                # Optionally log a warning if instructions are provided for non-compatible models\n",
    "                print(f\"Warning: 'instructions' parameter provided but model '{self.model}' might not support it.\")\n",
    "                # Do not pass instructions for models like tts-1/tts-1-hd\n",
    "\n",
    "\n",
    "        response = self.client.audio.speech.create(**params)\n",
    "\n",
    "        # The response object itself doesn't have .content directly in v1+ anymore for the standard call\n",
    "        # You read the raw bytes from the response object itself if not streaming.\n",
    "        # However, the previous code returned response.content. Let's check the actual response object structure.\n",
    "        # According to docs and common usage for non-streaming, you get an httpx.Response.\n",
    "        # Let's assume response.content works as before or adapt if needed.\n",
    "        # If `response` is directly the httpx response object:\n",
    "        return response.content # This should contain the raw audio bytes\n",
    "\n",
    "        # If the structure changed significantly and response is NOT an httpx response,\n",
    "        # you might need to adjust how you get the bytes. For example, if it returned\n",
    "        # a custom object, you might need a different attribute or method.\n",
    "        # But `response.content` is standard for non-streaming httpx responses.\n",
    "\n",
    "\n",
    "    def tostring(self):\n",
    "        return f\"[OpenAITTS] voice='{self.voice}' model='{self.model}'\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.tostring()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.tostring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e9911f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWSPollyTTS:\n",
    "    WOMAN = 'Kimberly'\n",
    "    MAN = 'Matthew'\n",
    "    BRIT_WOMAN = 'Amy'\n",
    "\n",
    "    def __init__(self, voice_id=None):\n",
    "        self.client = boto3.client('polly', region_name=\"us-east-1\")\n",
    "        self._voice_id = voice_id or self.WOMAN\n",
    "\n",
    "    @RateLimited(95)\n",
    "    @jonlog.retry_with_logging()\n",
    "    def tts(self, text, ssml=False):\n",
    "        kwargs = {}\n",
    "        if ssml:\n",
    "            kwargs['TextType'] = 'ssml'\n",
    "        response = self.client.synthesize_speech(\n",
    "            Text=text,\n",
    "            OutputFormat='mp3',\n",
    "            VoiceId=self._voice_id,\n",
    "            Engine=\"neural\",\n",
    "            **kwargs,\n",
    "        )\n",
    "        # The audio stream containing the synthesized speech\n",
    "        audio_stream = response.get('AudioStream')\n",
    "        return audio_stream.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8cffd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import texttospeech_v1 as texttospeech\n",
    "\n",
    "class GoogleTTS:\n",
    "    WOMAN = 'en-US-Wavenet-F'\n",
    "    MAN = 'en-US-Wavenet-D'\n",
    "    BRIT_WOMAN = 'en-GB-Wavenet-A'\n",
    "\n",
    "    def __init__(self, voice_name=None):\n",
    "        self.client = texttospeech.TextToSpeechClient()\n",
    "        self._voice_name = voice_name or self.WOMAN\n",
    "\n",
    "    # Assuming the RateLimited and retry_with_logging decorators are defined elsewhere\n",
    "    @RateLimited(95)\n",
    "    @jonlog.retry_with_logging()\n",
    "    def tts(self, text):\n",
    "        try:\n",
    "            synthesis_input = texttospeech.SynthesisInput(text=text)\n",
    "            voice_params = texttospeech.VoiceSelectionParams(\n",
    "                language_code=self._voice_name[:5],  # Extracts the language code from the voice name\n",
    "                name=self._voice_name,\n",
    "                # ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL\n",
    "            )\n",
    "            audio_config = texttospeech.AudioConfig(\n",
    "                audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "            )\n",
    "            response = self.client.synthesize_speech(\n",
    "                input=synthesis_input,\n",
    "                voice=voice_params,\n",
    "                audio_config=audio_config\n",
    "            )\n",
    "        except:\n",
    "            logger.critical(f\"GoogleTTS error from {text=}\")\n",
    "            raise\n",
    "        return response.audio_content\n",
    "\n",
    "    @classmethod\n",
    "    def list_voices(cls):\n",
    "        data = texttospeech.TextToSpeechClient().list_voices()\n",
    "        voices = [\n",
    "            v.name for v in data.voices\n",
    "            if v.name[:2] == 'en'\n",
    "            and 'studio' not in v.name.lower()\n",
    "            and 'journey' not in v.name.lower()\n",
    "        ]\n",
    "        return voices\n",
    "\n",
    "    def tostring(self): return f\"[GoogleTTS] {self._voice_name=}\"\n",
    "    def __repr__(self): return self.tostring()\n",
    "    def __str__(self): return self.tostring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2653243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_voices(n=2, openai=True, aws=True, google=True):\n",
    "    possible = []\n",
    "    if openai:\n",
    "        possible += [OpenAITTS(voice=vid) for vid in OpenAITTS.VOICES]\n",
    "    if aws:\n",
    "        possible += [AWSPollyTTS(voice_id=vid) for vid in ['Kimberly', 'Matthew', 'Amy']]\n",
    "    if google:\n",
    "        possible += [GoogleTTS(vid) for vid in GoogleTTS.list_voices()]\n",
    "    return random.sample(possible, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0607c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWSChat:\n",
    "    MODELS = {\n",
    "        \"claude-instant\": \"anthropic.claude-instant-v1\",\n",
    "        \"claude-best\": \"anthropic.claude-v2:1\",\n",
    "        \"claude-3-sonnet\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        \"claude-3-haiku\": \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        \"claude-3.7-sonnet\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    }\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def consolidate_messages(cls, message_list):\n",
    "        if not message_list:\n",
    "            return []\n",
    "        consolidated = []\n",
    "        current_role = None\n",
    "        current_content = \"\"\n",
    "\n",
    "        for message in message_list:\n",
    "            role = message.get(\"role\")\n",
    "            content = message.get(\"content\", \"\")\n",
    "    \n",
    "            if role == \"system\":\n",
    "                role = \"user\"\n",
    "            if role == current_role:\n",
    "                current_content += \"\\n\" + content\n",
    "            else:\n",
    "                if current_role is not None:\n",
    "                    consolidated.append({\"role\": current_role, \"content\": current_content})\n",
    "                current_content = content\n",
    "                current_role = role\n",
    "    \n",
    "        if current_role is not None:\n",
    "            consolidated.append({\"role\": current_role, \"content\": current_content})\n",
    "    \n",
    "        return consolidated\n",
    "\n",
    "    @classmethod\n",
    "    def msg(cls, messages=None, model=\"anthropic.claude-3-haiku-20240307-v1:0\", **kwargs):\n",
    "        client = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "        try:\n",
    "            # The different model providers have individual request and response formats.\n",
    "            # For the format, ranges, and default values for Anthropic Claude, refer to:\n",
    "            # https://docs.anthropic.com/claude/reference/complete_post\n",
    "\n",
    "            # Claude requires you to enclose the prompt as follows:\n",
    "            # enclosed_prompt = \"Human: \" + prompt + \"\\n\\nAssistant:\"\n",
    "            # prompt = \"\\n\\n\".join(\n",
    "            #     [f'{\"\" if (msg[\"role\"] == \"system\" and model != cls.MODELS[\"claude-instant\"]) else (\"Human\" if msg[\"role\"] != \"assistant\" else \"Assistant\")}: {msg[\"content\"]}' for msg in messages] +\n",
    "            #     [\"Assistant:\"]\n",
    "            # )\n",
    "\n",
    "            if 'temperature' not in kwargs:\n",
    "                kwargs['temperature'] = 1\n",
    "            system = \"\\n\".join([m['content'] for m in messages if m['role'] == 'system'])\n",
    "            other_msgs = cls.consolidate_messages([m for m in messages if m['role'] != 'system'])\n",
    "            \n",
    "            body = {\n",
    "                \"system\": system,\n",
    "                \"messages\": other_msgs,\n",
    "                \"max_tokens\": 2048,\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                **kwargs\n",
    "            }\n",
    "            response = client.invoke_model(\n",
    "                modelId=model, body=json.dumps(body)\n",
    "            )\n",
    "            response_body = json.loads(response[\"body\"].read())\n",
    "            completion = response_body[\"content\"][0][\"text\"]\n",
    "            return completion\n",
    "        except ClientError as e:\n",
    "            logger.exception(f\"Couldn't invoke {model}\", e)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "217ee219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleChat:\n",
    "    MODELS = {\n",
    "        \"gemini-pro\": \"gemini-pro\",\n",
    "        \"gemini-1.5-flash\": \"gemini-1.5-flash-latest\",\n",
    "        \"gemini-2.5-pro-exp-03-25\": \"gemini-2.5-pro-exp-03-25\",\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_apikey(cls):\n",
    "        return os.environ.get(\"GEMINI_API_KEY\") or open(os.path.expanduser(\"~/.google_apikey\")).read().strip()\n",
    "\n",
    "    @classmethod\n",
    "    def consolidate_messages(cls, message_list, keep_system=False):\n",
    "        if not message_list:\n",
    "            return []\n",
    "        consolidated = []\n",
    "        current_role = None\n",
    "        current_content = \"\"\n",
    "\n",
    "        for message in message_list:\n",
    "            role = message.get(\"role\")\n",
    "            content = message.get(\"content\", \"\")\n",
    "    \n",
    "            if role == \"system\" and not keep_system:\n",
    "                role = \"user\"\n",
    "            if role == current_role:\n",
    "                current_content += \"\\n\" + content\n",
    "            else:\n",
    "                if current_role is not None:\n",
    "                    consolidated.append({\"role\": current_role, \"content\": current_content})\n",
    "                current_content = content\n",
    "                current_role = role\n",
    "\n",
    "        if current_role is not None:\n",
    "            consolidated.append({\"role\": current_role, \"content\": current_content})\n",
    "    \n",
    "        return consolidated\n",
    "\n",
    "    @classmethod\n",
    "    def msg(cls, messages=None, model_name=\"gemini-1.5-flash-latest\", **kwargs):\n",
    "        google.generativeai.configure(api_key=cls.get_apikey())\n",
    "        \n",
    "        # Create the model configuration\n",
    "        generation_config = {\n",
    "            \"temperature\": 1,\n",
    "            \"top_p\": 0.95,\n",
    "            \"top_k\": 64,\n",
    "            \"max_output_tokens\": 8192,\n",
    "            \"response_mime_type\": \"text/plain\",\n",
    "        }\n",
    "        # generation_config.update(kwargs)\n",
    "        safety_settings = [\n",
    "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "        ]\n",
    "\n",
    "        # Consolidate messages\n",
    "        consolidated_messages = cls.consolidate_messages(messages, keep_system=True)\n",
    "        final_msg, system_msg = \"Continue\", None\n",
    "        if consolidated_messages[-1]['role'] == 'user':\n",
    "            final_msg = consolidated_messages.pop(-1)['content']\n",
    "        if consolidated_messages[0]['role'] == 'system':\n",
    "            system_msg = consolidated_messages.pop(0)['content']\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = google.generativeai.GenerativeModel(\n",
    "            model_name=model_name,\n",
    "            safety_settings=safety_settings,\n",
    "            generation_config=generation_config,\n",
    "            system_instruction=system_msg,\n",
    "        )\n",
    "\n",
    "        # Create chat session history\n",
    "        history = []\n",
    "        for msg in consolidated_messages:\n",
    "            history.append({\n",
    "                \"role\": \"user\" if msg[\"role\"] != \"assistant\" else \"model\",\n",
    "                \"parts\": [msg[\"content\"]]\n",
    "            })\n",
    "        # Start chat session\n",
    "        chat_session = model.start_chat(history=history)\n",
    "        # Send message and get response\n",
    "        response = chat_session.send_message(final_msg)\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ab66077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralChat:\n",
    "    MODELS = {\n",
    "        \"mistral-medium\": \"mistral-medium\",\n",
    "        \"mistral-small\": \"mistral-small\",\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_apikey(cls):\n",
    "        return os.environ.get(\"MISTRAL_API_KEY\") or open('/Users/jong/.mistral_apikey').read().strip()\n",
    "\n",
    "    @classmethod\n",
    "    def consolidate_messages(cls, message_list):\n",
    "        if not message_list:\n",
    "            return []\n",
    "    \n",
    "        consolidated = []\n",
    "        current_role = None\n",
    "        current_content = \"\"\n",
    "    \n",
    "        for message in message_list:\n",
    "            role = message[\"role\"]\n",
    "            content = message[\"content\"]\n",
    "\n",
    "            if role == current_role:\n",
    "                current_content += \"\\n\" + content\n",
    "            else:\n",
    "                if current_role is not None:\n",
    "                    consolidated.append({\"role\": current_role, \"content\": current_content})\n",
    "                current_content = content\n",
    "                current_role = role\n",
    "    \n",
    "        if current_role is not None:\n",
    "            consolidated.append({\"role\": current_role, \"content\": current_content})\n",
    "\n",
    "        return consolidated\n",
    "\n",
    "    @classmethod\n",
    "    def msg(cls, messages=None, model=\"mistral-medium\", **kwargs):\n",
    "        client = MistralClient(api_key=cls.get_apikey())\n",
    "        if not any(msg['role'] == 'user' for msg in messages):\n",
    "            messages[-1]['role'] = 'user'\n",
    "        chat_response = client.chat(\n",
    "            model=model,\n",
    "            messages=[MistralChatMessage(**msg) for msg in cls.consolidate_messages(messages)],\n",
    "        )\n",
    "        return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "def95e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnthropicChat:\n",
    "    MODELS = {\n",
    "        \"claude-opus\": \"claude-3-opus-20240229\",    # Large\n",
    "        \"claude-sonnet\": \"claude-3-sonnet-20240229\", # Medium\n",
    "        \"claude-haiku\": \"claude-3-5-haiku-latest\", # Small\n",
    "        \"claude-sonnet-3.5\": \"claude-3-5-sonnet-20240620\", # Updated Sonnet\n",
    "        \"claude-sonnet-3.7\": \"claude-3-7-sonnet-20250219\", # Latest Sonnet\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def get_apikey(cls):\n",
    "        return os.environ.get(\"ANTHROPIC_APIKEY\") or open('/Users/jong/.anthropic_apikey').read().strip()\n",
    "    \n",
    "    @classmethod\n",
    "    def msg(cls, messages=None, model=\"claude-3-7-sonnet-20250219\", **kwargs):\n",
    "        messages = MistralChat.consolidate_messages(messages)\n",
    "        system = '\\n'.join([msg['content'] for msg in messages if msg['role'] == 'system'])\n",
    "        messages = [m for m in messages if m['role'] != 'system' and m['content']]\n",
    "        if not messages:\n",
    "            messages.append({'role': 'user', 'content': 'Continue.'})\n",
    "        \n",
    "        client = anthropic.Anthropic(api_key=cls.get_apikey())\n",
    "        \n",
    "        if 'max_tokens' not in kwargs:\n",
    "            kwargs['max_tokens'] = 4096\n",
    "            \n",
    "        message = client.messages.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            system=system,\n",
    "            **kwargs,\n",
    "        ).content[0].text\n",
    "        \n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40bb8573",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqChat:\n",
    "    MODELS = {\n",
    "        \"llama3\": \"Llama3-70b-8192\",\n",
    "        \"mixtral\": \"Mixtral-8x7b-32768\",\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_apikey(cls):\n",
    "        return os.environ.get(\"GROQ_APIKEY\") or open('/Users/jong/.groq_apikey').read().strip()\n",
    "\n",
    "    @classmethod\n",
    "    def msg(cls, messages=None, model=\"Mixtral-8x7b-32768\", **kwargs):\n",
    "        messages = MistralChat.consolidate_messages(messages)\n",
    "        system = '\\n'.join([msg['content'] for msg in messages if msg['role'] == 'system'])\n",
    "        messages = [m for m in messages if m['role'] != 'system' and m['content']]\n",
    "        if not messages:\n",
    "            messages.append({'role': 'user', 'content': 'Continue.'})\n",
    "        messages = [{'role': 'system', 'content': system}] + messages\n",
    "\n",
    "        client = groq.Groq(api_key=cls.get_apikey())\n",
    "        message = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=model,\n",
    "        ).choices[0].message.content\n",
    "        \n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3045d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TogetherChat:\n",
    "    MODELS = {\n",
    "        \"mixtral\": \"mistralai/Mixtral-8x22B-Instruct-v0.1\",\n",
    "        \"qwen\": \"Qwen/Qwen1.5-110B-Chat\",\n",
    "        \"databricks\": \"databricks/dbrx-instruct\",\n",
    "        \"dolphin\": \"cognitivecomputations/dolphin-2.5-mixtral-8x7b\",\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_apikey(cls):\n",
    "        return os.environ.get(\"TOGETHER_API_KEY\") or open(os.path.expanduser(\"~/.together_apikey\")).read().strip()\n",
    "\n",
    "    @classmethod\n",
    "    def msg(cls, messages=None, model=None, **kwargs):\n",
    "        if model is None:\n",
    "            model = random.choice(cls.MODELS.values())\n",
    "        # Prepare the messages for the API\n",
    "        consolidated_messages = GoogleChat.consolidate_messages(messages)\n",
    "        # Prepare the messages for the API\n",
    "        api_messages = [{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in consolidated_messages]\n",
    "        # Define the payload\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": api_messages,\n",
    "            \"temperature\": kwargs.get(\"temperature\", 0.8),\n",
    "            \"max_tokens\": kwargs.get(\"max_tokens\", 8000)\n",
    "        }\n",
    "        # Make the request to Together API\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {cls.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        response = requests.post(\"https://api.together.xyz/v1/chat/completions\", headers=headers, json=payload)\n",
    "        # Parse the response\n",
    "        response_data = response.json()\n",
    "        return response_data[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18fdfbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT_MODEL = 'gpt-4-1106-preview'\n",
    "# DEFAULT_LENGTH  = 80_000\n",
    "DEFAULT_MODEL = 'ANTHROPIC/claude-3-7-sonnet-20250219'\n",
    "DEFAULT_LENGTH  = 29_000\n",
    "\n",
    "class Chat:\n",
    "    class Model(enum.Enum):\n",
    "        GPT3_5 = \"gpt-3.5-turbo\"\n",
    "        GPT_4  = \"gpt-4-turbo-preview\"\n",
    "\n",
    "    def __init__(self, system, max_length=DEFAULT_LENGTH, default_model=None, messages=None):\n",
    "        self._system = system\n",
    "        self._max_length = max_length\n",
    "        self._default_model = default_model\n",
    "        self._history = [\n",
    "            {\"role\": \"system\", \"content\": self._system},\n",
    "        ]\n",
    "        if messages:\n",
    "            self._history += messages\n",
    "\n",
    "    @classmethod\n",
    "    def num_tokens_from_text(cls, text, model=DEFAULT_MODEL):\n",
    "        \"\"\"Returns the number of tokens used by some text.\"\"\"\n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(model)\n",
    "        except:\n",
    "            encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')  # Lol openai probably the same\n",
    "        return len(encoding.encode(text))\n",
    "    \n",
    "    @classmethod\n",
    "    def num_tokens_from_messages(cls, messages, model=DEFAULT_MODEL):\n",
    "        \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(model)\n",
    "        except:\n",
    "            encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')  # Lol openai probably the same\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":  # if there's a name, the role is omitted\n",
    "                    num_tokens += -1  # role is always required and always 1 token\n",
    "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "        return num_tokens\n",
    "\n",
    "    @retrying.retry(stop_max_attempt_number=5, wait_fixed=2000)\n",
    "    def _msg(self, *args, model=None, **kwargs):\n",
    "        if model is None:\n",
    "            if self._default_model is not None: model = self._default_model \n",
    "            else: model = DEFAULT_MODEL\n",
    "        logger.info(f'requesting chatcompletion {model=}...')\n",
    "        if model.startswith(\"AWS/\"):\n",
    "            model = model[4:]\n",
    "            resp = AWSChat.msg(\n",
    "                messages=self._history,\n",
    "                **kwargs\n",
    "            )\n",
    "        elif model.startswith(\"GOOGLE/\"):\n",
    "            model = model[7:]\n",
    "            resp = GoogleChat.msg(messages=self._history, model=model, **kwargs)\n",
    "        elif model.startswith(\"MISTRAL/\"):\n",
    "            model = model[8:]\n",
    "            resp = MistralChat.msg(messages=self._history, model=model, **kwargs)\n",
    "        elif model.startswith(\"ANTHROPIC/\"):\n",
    "            model = model[10:]\n",
    "            resp = AnthropicChat.msg(messages=self._history, model=model, **kwargs)\n",
    "        elif model.startswith(\"GROQ/\"):\n",
    "            model = model[5:]\n",
    "            resp = GroqChat.msg(messages=self._history, model=model, **kwargs)\n",
    "        elif model.startswith(\"TOGETHER/\"):\n",
    "            model = model[9:]\n",
    "            resp = TogetherChat.msg(messages=self._history, model=model, **kwargs)\n",
    "        else:\n",
    "            resp = openai.OpenAI(api_key=openai.api_key).chat.completions.create(\n",
    "                *args,\n",
    "                model=model,\n",
    "                messages=self._history,\n",
    "                **kwargs\n",
    "            ).choices[0].message.content\n",
    "        logger.info(f'received chatcompletion {model=}...')\n",
    "        return resp\n",
    "    \n",
    "    def message(self, next_msg=None, **kwargs):\n",
    "        # TODO: Optimize this if slow through easy caching\n",
    "        while len(self._history) > 1 and self.num_tokens_from_messages(self._history) > self._max_length:\n",
    "            logger.info(f'Popping message: {self._history.pop(1)}')\n",
    "        if next_msg is not None:\n",
    "            self._history.append({\"role\": \"user\", \"content\": next_msg})\n",
    "        logger.info(f'Currently at {self.num_tokens_from_messages(self._history)=} tokens in conversation')\n",
    "        resp = self._msg(**kwargs)\n",
    "        text = resp\n",
    "        self._history.append({\"role\": \"assistant\", \"content\": text})\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbd8392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PodcastChat(Chat):\n",
    "    def __init__(self, topic, podcast=\"award winning\", max_length=DEFAULT_LENGTH,\n",
    "                 hosts=['Tom', 'Jen'],\n",
    "                 host_voices=[OpenAITTS(voice='verse'), OpenAITTS(voice='ash')], # Example voices\n",
    "                 host_instructions=None, # <<< Added parameter\n",
    "                 extra_system=None, system=None):\n",
    "        \"\"\" (Docstring remains the same) \"\"\"\n",
    "        if system is None:\n",
    "            system = f\"\"\"You are an {podcast} podcast with hosts {hosts[0]} and {hosts[1]}.\n",
    "Respond with the hosts names before each line like {hosts[0]}: and {hosts[1]}:\"\"\".replace(\"\\n\", \" \")\n",
    "        if extra_system is not None:\n",
    "            system = '\\n'.join([system, extra_system])\n",
    "\n",
    "        super().__init__(system, max_length=max_length)\n",
    "        self._podcast = podcast\n",
    "        self._topic = topic\n",
    "        if len(hosts) != 2 or len(host_voices) != 2:\n",
    "            raise ValueError(\"Requires exactly two hosts and two corresponding voices.\")\n",
    "        self._hosts = hosts\n",
    "        # Updated history logic based on provided snippet\n",
    "        if host_instructions:\n",
    "             # Also include the main generation prompt along with instructions\n",
    "             self._history.append({\n",
    "                 \"role\": \"user\", \"content\": f\"\"\"Generate an informative, entertaining, and very detailed podcast episode about {topic}.\n",
    " Respond with the hosts names before each line like\\n\\n{hosts[0]}: ...\\nand\\n{hosts[1]}:...\\n\\nUse these specific instructions per host: {host_instructions}\"\"\"\n",
    "             })\n",
    "        else:\n",
    "            # Original prompt if no instructions\n",
    "            self._history.append({\n",
    "                \"role\": \"user\", \"content\": f\"\"\"Generate an informative, entertaining, and very detailed podcast episode about {topic}.\n",
    " Respond with the hosts names before each line like\\n\\n{hosts[0]}: ...\\nand\\n{hosts[1]}:...\\n\"\"\"\n",
    "            })\n",
    "        self._tts_h1, self._tts_h2 = host_voices\n",
    "        self._host_instructions = host_instructions or {} # <<< Store instructions, default to empty dict\n",
    "\n",
    "    def text2speech(self, text, spacing_ms=350): # spacing_ms still unused here\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as thread_pool:\n",
    "            segment_index = 0 # Use simple counter for segment order\n",
    "            submitted_futures = [] # Store futures in submission order\n",
    "\n",
    "            # Inner function remains the same, accepting index 'i' for logging\n",
    "            def write_audio(msg, i, voice, instructions=None, **kwargs):\n",
    "                logger.info(f'Requesting TTS segment_index={i} for voice={voice}')\n",
    "                can_accept_instructions = True\n",
    "\n",
    "                tts_result = None\n",
    "                if instructions and can_accept_instructions:\n",
    "                    logger.info(f\"  Passing instructions to segment_index={i}: '{instructions}'\")\n",
    "                    tts_result = voice.tts(msg, instructions=instructions)\n",
    "                else:\n",
    "                    if instructions and not can_accept_instructions:\n",
    "                         logger.warning(f\"  Instructions provided for segment_index={i} ('{instructions}') but {voice}.tts does not accept them. Calling without.\")\n",
    "                    tts_result = voice.tts(msg)\n",
    "                logger.info(f'Received TTS segment_index={i} for voice={voice}')\n",
    "                return tts_result # Return only the audio bytes\n",
    "\n",
    "            # --- Text processing and job submission ---\n",
    "            text = text.replace('\\n', '!!!LINEBREAK!!!').replace('\\\\', '').replace('\"', '')\n",
    "            currline, currname = \"\", self._hosts[0] # Start with the first host\n",
    "            name2voice = {self._hosts[0]: self._tts_h1, self._hosts[1]: self._tts_h2}\n",
    "\n",
    "            host1_prefix = f\"{self._hosts[0]}: \"\n",
    "            host2_prefix = f\"{self._hosts[1]}: \"\n",
    "\n",
    "            for line in text.split(\"!!!LINEBREAK!!!\"):\n",
    "                line_strip = line.strip()\n",
    "                if not line_strip: continue\n",
    "\n",
    "                new_host_found = False\n",
    "                potential_host = None\n",
    "                line_content = \"\"\n",
    "\n",
    "                if line.startswith(host1_prefix):\n",
    "                    potential_host = self._hosts[0]\n",
    "                    line_content = line[len(host1_prefix):]\n",
    "                    new_host_found = True\n",
    "                elif line.startswith(host2_prefix):\n",
    "                    potential_host = self._hosts[1]\n",
    "                    line_content = line[len(host2_prefix):]\n",
    "                    new_host_found = True\n",
    "                else:\n",
    "                    line_content = line # Treat as continuation\n",
    "\n",
    "                if new_host_found:\n",
    "                    if currline: # Process the previous host's accumulated lines\n",
    "                        text_to_speak = currline.strip()\n",
    "                        if text_to_speak: # Avoid submitting empty strings\n",
    "                             instruction = self._host_instructions.get(currname, None)\n",
    "                             logger.info(f\"Submitting segment {segment_index} for {currname}: '{text_to_speak[:50]}...'\")\n",
    "                             future = thread_pool.submit(\n",
    "                                 write_audio,\n",
    "                                 text_to_speak,\n",
    "                                 segment_index, # Pass the current segment index\n",
    "                                 name2voice[currname],\n",
    "                                 instructions=instruction\n",
    "                             )\n",
    "                             submitted_futures.append(future) # Store future in order\n",
    "                             segment_index += 1\n",
    "                    currline = line_content # Start new line content\n",
    "                    currname = potential_host # Set the new current host\n",
    "                else:\n",
    "                    currline += \" \" + line_content # Accumulate content\n",
    "\n",
    "            # Process the last accumulated line\n",
    "            if currline:\n",
    "                 text_to_speak = currline.strip()\n",
    "                 if text_to_speak: # Avoid submitting empty strings for the last segment\n",
    "                     instruction = self._host_instructions.get(currname, None)\n",
    "                     logger.info(f\"Submitting segment {segment_index} for {currname}: '{text_to_speak[:50]}...'\")\n",
    "                     future = thread_pool.submit(\n",
    "                         write_audio,\n",
    "                         text_to_speak,\n",
    "                         segment_index, # Pass the current segment index\n",
    "                         name2voice[currname],\n",
    "                         instructions=instruction\n",
    "                     )\n",
    "                     submitted_futures.append(future) # Store final future in order\n",
    "                     segment_index += 1\n",
    "\n",
    "            # --- Collect results in submission order ---\n",
    "            logger.info(f\"Waiting for {len(submitted_futures)} audio segments to complete...\")\n",
    "            audios = []\n",
    "            for i, future in enumerate(submitted_futures):\n",
    "                 try:\n",
    "                     # .result() will wait for the future to complete\n",
    "                     result = future.result()\n",
    "                     audios.append(result)\n",
    "                     logger.info(f\"Collected result for segment {i}\")\n",
    "                 except Exception as e:\n",
    "                     logger.error(f\"Error getting result for segment {i}: {e}\")\n",
    "                     # Decide how to handle errors: skip segment, raise error, etc.\n",
    "                     # For now, let's append None or empty bytes to avoid breaking merge_mp3s if possible\n",
    "                     audios.append(b\"\") # Append empty bytes on error\n",
    "\n",
    "            # --- Concatenate files ---\n",
    "            logger.info('Concatenating audio segments in order...')\n",
    "            audio = merge_mp3s(audios) # Pass the ordered list\n",
    "            logger.info('Done with audio synthesis and merging!')\n",
    "            IPython.display.display(IPython.display.Audio(audio, autoplay=False))\n",
    "            return audio\n",
    "\n",
    "    def step(self, msg=None, skip_aud=False, ret_aud=True, min_length=None, **kwargs):\n",
    "        # (Step method remains the same)\n",
    "        text_response = self.message(msg, **kwargs)\n",
    "        logger.info(f\"Generated text response:\\n{text_response}\")\n",
    "        if min_length is not None and len(text_response) < min_length:\n",
    "            raise ValueError(f\"Message [{text_response[:100]}...] is shorter than {min_length=}\")\n",
    "        if skip_aud:\n",
    "            return text_response\n",
    "        audio_response = self.text2speech(text_response)\n",
    "        if ret_aud:\n",
    "            return text_response, audio_response\n",
    "        return text_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50838834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PodcastXMLHandler:\n",
    "    def __init__(self):\n",
    "        self.root = ET.Element(\"channel\")  # 'channel' is typically used in podcast RSS feeds\n",
    "        self.tree = ET.ElementTree(self.root)\n",
    "\n",
    "    def to_xml(self, filepath):\n",
    "        self.tree.write(filepath, encoding='utf-8', xml_declaration=True, pretty_print=True)\n",
    "\n",
    "    @classmethod\n",
    "    def from_xml(cls, filepath):\n",
    "        self = cls()\n",
    "        self.tree = ET.parse(filepath)\n",
    "        self.root = self.tree.getroot()\n",
    "        return self\n",
    "\n",
    "    def contains_episode(self, episode_name):\n",
    "        for episode in self.root.findall('./channel/item'):\n",
    "            title = episode.find('title').text\n",
    "            if title == episode_name:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def remove_episodes_older_than(self, limit):\n",
    "        now = datetime.datetime.now()\n",
    "        parent_map = {c:p for p in self.root.iter() for c in p}\n",
    "        for episode in self.root.findall('./channel/item'):\n",
    "            pub_date = datetime.datetime.strptime(episode.find('pubDate').text, '%a, %d %b %Y %H:%M:%S %Z')  # RSS date format\n",
    "            if now - pub_date > limit:\n",
    "                parent_map[episode].remove(episode)\n",
    "\n",
    "    def add_episode(self, episode_details):\n",
    "        episode = ET.SubElement(self.root, './channel/item')\n",
    "        for key, value in episode_details.items():\n",
    "            ET.SubElement(episode, key).text = str(value)\n",
    "\n",
    "\"\"\"\n",
    "pd = PodcastXMLHandler.from_xml('/Users/jong/Downloads/podcast.xml')\n",
    "pd.contains_episode('cs.IR: Recent Research Papers on Data Science and Cybersecurity.')\n",
    "pd.remove_episodes_older_than(datetime.timedelta(days=30))\n",
    "pd.to_xml('/Users/jong/Downloads/podcast2.xml')\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8d798f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PodcastRSSFeed:\n",
    "    \"\"\"Class to handle rss feed operations using github pages.\"\"\"\n",
    "\n",
    "    def __init__(self, org, repo, xml_path, clean_timedelta=None):\n",
    "        self.org = org\n",
    "        self.repo = repo\n",
    "        self.xml_path = xml_path\n",
    "        self.local_xml_path = self.download_podcast_xml()\n",
    "        self.clean_timedelta = clean_timedelta\n",
    "\n",
    "    def get_file_base64(self, file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            return base64.b64encode(file.read()).decode('utf-8')\n",
    "\n",
    "    def download_podcast_xml(self):\n",
    "        outfile = tempfile.NamedTemporaryFile().name + '.xml'\n",
    "        raw_url = f'https://raw.githubusercontent.com/{self.org}/{self.repo}/main/{self.xml_path}'\n",
    "        response = requests.get(raw_url)\n",
    "        print(raw_url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response.text)\n",
    "        with open(outfile, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        return outfile\n",
    "\n",
    "    def update_podcast_xml(self, xml_data, file_name, episode_title, episode_description, file_length):\n",
    "        # Parse XML\n",
    "        root = ET.fromstring(xml_data)\n",
    "        channel = root.find('channel')\n",
    "\n",
    "        file_extension = os.path.splitext(file_name)[-1].lower()[1:]\n",
    "        content_type = 'audio/' + file_extension\n",
    "        \n",
    "        # Add new episode\n",
    "        item = ET.SubElement(channel, 'item')\n",
    "        ET.SubElement(item, 'title').text = episode_title\n",
    "        ET.SubElement(item, 'description').text = episode_description\n",
    "        ET.SubElement(item, 'pubDate').text = dt.datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT')\n",
    "        ET.SubElement(item, 'enclosure', {\n",
    "            'url': f'https://{self.org}.github.io/{file_name}',\n",
    "            'type': content_type,\n",
    "            'length': str(file_length),\n",
    "        })\n",
    "        ET.SubElement(item, 'guid').text = str(uuid.uuid4())\n",
    "\n",
    "        # Convert back to string and pretty-format\n",
    "        pretty_xml = minidom.parseString(ET.tostring(root)).toprettyxml(indent='  ')\n",
    "        # Remove extra newlines\n",
    "        pretty_xml = os.linesep.join([s for s in pretty_xml.splitlines() if s.strip()])\n",
    "\n",
    "        return pretty_xml\n",
    "\n",
    "    def remove_episodes_older_than(self, xml_data, limit):\n",
    "        now = dt.datetime.now()\n",
    "        root = ET.fromstring(xml_data)\n",
    "        parent_map = {c:p for p in root.iter() for c in p}\n",
    "        token = os.environ.get(\"GH_KEY\", None) or open(\"/Users/jong/.gh_token\").read().strip()\n",
    "        gh = Github(token)\n",
    "        made_changes = False\n",
    "        for episode in root.findall('./channel/item'):\n",
    "            pub_date = dt.datetime.strptime(episode.find('pubDate').text, '%a, %d %b %Y %H:%M:%S %Z')  # RSS date format\n",
    "            if now - pub_date > limit:\n",
    "                episode_path = episode.find('enclosure').attrib['url'].split('.github.io/', 1)[1]\n",
    "                logger.info(f\"Deleting old episode: {episode_path}\")\n",
    "                parent_map[episode].remove(episode)\n",
    "                made_changes = True\n",
    "                # Get the repository\n",
    "                try:\n",
    "                    repo = gh.get_user().get_repo(self.repo)\n",
    "                except:\n",
    "                    repo = gh.get_organization(self.org).get_repo(self.repo)\n",
    "                try:\n",
    "                    contents = repo.get_contents(episode_path)\n",
    "                    repo.delete_file(episode_path, \"remove due to date\", contents.sha)\n",
    "                except Exception as e:\n",
    "                    logger.exception(e)\n",
    "        # Convert back to string and pretty-format\n",
    "        pretty_xml = minidom.parseString(ET.tostring(root)).toprettyxml(indent='  ')\n",
    "        # Remove extra newlines\n",
    "        pretty_xml = os.linesep.join([s for s in pretty_xml.splitlines() if s.strip()])\n",
    "        # Upload\n",
    "        if made_changes:\n",
    "            try:\n",
    "                podcast_xml_sha = repo.get_contents(self.xml_path).sha\n",
    "                self.upload_to_github(self.xml_path, pretty_xml, f'Delete old episodes in podcast.xml', podcast_xml_sha)\n",
    "            except Exception as e:\n",
    "                logger.exception(e)\n",
    "        return pretty_xml\n",
    "    \n",
    "    def upload_episode(self, file_path, file_name, episode_title, episode_description):\n",
    "        # Authenticate with GitHub\n",
    "        token = os.environ.get(\"GH_KEY\", None) or open(\"/Users/jong/.gh_token\").read().strip()\n",
    "        gh = Github(token)\n",
    "\n",
    "        # Get the repository\n",
    "        try:\n",
    "            repo = gh.get_user().get_repo(self.repo)\n",
    "        except:\n",
    "            repo = gh.get_organization(self.org).get_repo(self.repo)\n",
    "\n",
    "        # Upload the audio file\n",
    "        podsha = None\n",
    "        try:\n",
    "            podsha = repo.get_contents(file_name).sha\n",
    "        except:\n",
    "            pass\n",
    "        with open(file_path, 'rb') as audio_file:\n",
    "            audio_data = audio_file.read()\n",
    "            self.upload_to_github(file_name, audio_data, f'Upload new episode: {file_name}', podsha)\n",
    "\n",
    "        # Update and upload the podcast.xml file\n",
    "        file_length = os.path.getsize(file_path)\n",
    "        podcast_xml = repo.get_contents(self.xml_path)\n",
    "        xml_data = base64.b64decode(podcast_xml.content).decode('utf-8')\n",
    "        xml_data = self.update_podcast_xml(xml_data, file_name, episode_title, episode_description, file_length)\n",
    "        self.upload_to_github(self.xml_path, xml_data, f'Update podcast.xml with new episode: {file_name}', podcast_xml.sha)\n",
    "\n",
    "    def upload_to_github(self, file_name, file_content, commit_message, sha=None):\n",
    "        # Prepare API request headers\n",
    "        token = os.environ.get(\"GH_KEY\", None) or open(\"/Users/jong/.gh_token\").read().strip()\n",
    "        gh = Github(token)\n",
    "        # Get the repository\n",
    "        try:\n",
    "            repo = gh.get_user().get_repo(self.repo)\n",
    "        except:\n",
    "            repo = gh.get_organization(self.org).get_repo(self.repo)\n",
    "\n",
    "        if sha:\n",
    "            repo.update_file(file_name, commit_message, file_content, sha)\n",
    "        else:\n",
    "            repo.create_file(file_name, commit_message, file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74d67676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episode:\n",
    "    def __init__(self, episode_type='narration', podcast_args=(\"JonathanGrant\", \"jonathangrant.github.io\", \"podcasts/podcast.xml\"), text_model=DEFAULT_MODEL, desc=\"hilarious\", **chat_kwargs):\n",
    "        \"\"\"\n",
    "        Kinds of episodes:\n",
    "            pure narration - simple TTS\n",
    "            simple podcast - Text to Podcast\n",
    "            complex podcast?\n",
    "        \"\"\"\n",
    "        self.episode_type = episode_type\n",
    "        self.chat = PodcastChat(**chat_kwargs)\n",
    "        self.chat_kwargs = chat_kwargs\n",
    "        # self.pod = PodcastRSSFeed(*podcast_args)\n",
    "        self.text_model = text_model\n",
    "        self.sounds = []\n",
    "        self.texts = []\n",
    "        self._desc = desc\n",
    "\n",
    "    def get_outline(self, n, topic=None):\n",
    "        if topic is None: topic = self.chat._topic\n",
    "        chat = Chat(f\"\"\"Write \n",
    "a concise plaintext outline with exactly {n} parts for an epsiode of the {self._desc} podcast titled {self.chat._podcast}.\n",
    "Only return the parts and nothing else.\n",
    "Do not include a conclusion or intro.\n",
    "Do not write more than {n} parts.\n",
    "Format it like this: 1. insert-title-here, 2. another-title-here, ...\"\"\".replace(\"\\n\", \" \"))\n",
    "        resp = chat.message(model=self.text_model)\n",
    "        chapter_pattern = re.compile(r'\\d+\\.\\s+.*')\n",
    "        chapters = chapter_pattern.findall(resp)\n",
    "        if not chapters:\n",
    "            logger.warning(f'Could not parse message for chapters! Message:\\n{resp}')\n",
    "        return chapters\n",
    "\n",
    "    def step(self, msg=None, nparts=3):\n",
    "        include = f\" Remember to respond with the hosts names like {self.chat._hosts[0]}: and {self.chat._hosts[1]}:\"\n",
    "        msg = msg or self.chat._topic\n",
    "        if self.episode_type == 'narration':\n",
    "            outline = self.get_outline(msg, nparts)\n",
    "            logger.info(f\"Outline: {outline}\")\n",
    "            intro_txt, intro_aud = self.chat.step(f\"Write the intro for a podcast about {msg}. The outline for the podcast is {', '.join(outline)}. Only write the introduction.{include}\", model=self.text_model)\n",
    "            self.sounds.append(intro_aud)\n",
    "            self.texts.append(intro_txt)\n",
    "            # Get parts\n",
    "            for part in outline:\n",
    "                logger.info(f\"Part: {part}\")\n",
    "                part_txt, part_aud = self.chat.step(f\"Write the next part: {part}.{include}\", model=self.text_model)\n",
    "                self.sounds.append(part_aud)\n",
    "                self.texts.append(part_txt)\n",
    "            # Get conclusion\n",
    "            logger.info(\"Conclusion\")\n",
    "            part_txt, part_aud = self.chat.step(f\"Write the conclusion. Remember, the outline was: {', '.join(outline)}.{include}\", model=self.text_model)\n",
    "            self.sounds.append(part_aud)\n",
    "            self.texts.append(part_txt)\n",
    "        elif self.episode_type == 'pure_tts':\n",
    "            outline = None\n",
    "            audio = self.chat.text2speech(\"\\n\".join([self.chat._hosts[i%2]+\": \"+x for i,x in enumerate(msg)]))\n",
    "            self.sounds.append(audio)\n",
    "            self.texts.extend(msg)\n",
    "        return outline, '\\n'.join(self.texts)\n",
    "\n",
    "    def upload(self, title, descr):\n",
    "        title_small = title.lower().replace(\" \", \"_\")[:16] + str(uuid.uuid4())  # I had a filename too long once\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            tmppath = os.path.join(tmpdir, \"audio_file.mp3\")\n",
    "            with open(tmppath, \"wb\") as f:\n",
    "                f.write(merge_mp3s(self.sounds))\n",
    "            self.pod.upload_episode(tmppath, f\"podcasts/audio/{title_small}.mp3\", title, descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02483944-6e4f-4395-88a1-98df3543bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodeJoe(Episode):\n",
    "    def get_outline(self, n, topic=None):\n",
    "        if topic is None: topic = self.chat._topic\n",
    "        chat = Chat(f\"\"\"Write \n",
    "    a concise plaintext outline with exactly {n} parts for an episode of the {self._desc} podcast titled {self.chat._podcast}.\n",
    "    Only return the parts and nothing else.\n",
    "    Do not include a conclusion or intro.\n",
    "    Do not write more than {n} parts.\n",
    "    Format it like this: 1. insert-title-here, 2. another-title-here, ...\"\"\".replace(\"\\n\", \" \"))\n",
    "        resp = chat.message(model=self.text_model)\n",
    "        chapter_pattern = re.compile(r'\\d+\\.\\s+.*')\n",
    "        chapters = chapter_pattern.findall(resp)\n",
    "        if not chapters:\n",
    "            logger.warning(f'Could not parse message for chapters! Message:\\n{resp}')\n",
    "        return chapters\n",
    "    \n",
    "    def step(self, msg=None, nparts=3):\n",
    "        msg = msg or self.chat._topic\n",
    "        hosts_format = f\"{self.chat._hosts[0]} and {self.chat._hosts[1]}\"\n",
    "        \n",
    "        if self.episode_type == 'narration':\n",
    "            outline = self.get_outline(nparts, msg)\n",
    "            logger.info(f\"Outline: {outline}\")\n",
    "            \n",
    "            # Generate intro script\n",
    "            prompt = f\"\"\"Write the {self._desc} ACTUAL DIALOGUE script for the introduction of a podcast episode about {msg}.\n",
    "    The hosts are {hosts_format}.\n",
    "    Write ONLY the back-and-forth conversation between the hosts.\n",
    "    Format each line with the speaker's name followed by a colon, like this:\n",
    "    {self.chat._hosts[0]}: [what they say]\n",
    "    {self.chat._hosts[1]}: [what they say]\"\"\"\n",
    "            \n",
    "            intro_txt, intro_aud = self.chat.step(prompt, model=self.text_model)\n",
    "            self.sounds.append(intro_aud)\n",
    "            self.texts.append(intro_txt)\n",
    "            \n",
    "            # Get parts\n",
    "            for part in outline:\n",
    "                logger.info(f\"Part: {part}\")\n",
    "                part_prompt = f\"\"\"Write the {self._desc} ACTUAL DIALOGUE script for this segment of the podcast: {part}\n",
    "    The hosts are {hosts_format}.\n",
    "    Write ONLY the back-and-forth conversation as it would happen in real time.\n",
    "    Format each line with the speaker's name followed by a colon, like this:\n",
    "    {self.chat._hosts[0]}: [what they say]\n",
    "    {self.chat._hosts[1]}: [what they say]\"\"\"\n",
    "                \n",
    "                part_txt, part_aud = self.chat.step(part_prompt, model=self.text_model)\n",
    "                self.sounds.append(part_aud)\n",
    "                self.texts.append(part_txt)\n",
    "            \n",
    "            # Get conclusion\n",
    "            conclusion_prompt = f\"\"\"Write the {self._desc} ACTUAL DIALOGUE script for the conclusion of this podcast.\n",
    "    The hosts are {hosts_format}.\n",
    "    Write ONLY the back-and-forth closing conversation.\n",
    "    Format each line with the speaker's name followed by a colon, like this:\n",
    "    {self.chat._hosts[0]}: [what they say]\n",
    "    {self.chat._hosts[1]}: [what they say]\"\"\"\n",
    "            \n",
    "            part_txt, part_aud = self.chat.step(conclusion_prompt, model=self.text_model)\n",
    "            self.sounds.append(part_aud)\n",
    "            self.texts.append(part_txt)\n",
    "        \n",
    "        elif self.episode_type == 'pure_tts':\n",
    "            outline = None\n",
    "            audio = self.chat.text2speech(\"\\n\".join([self.chat._hosts[i%2]+\": \"+x for i,x in enumerate(msg)]))\n",
    "            self.sounds.append(audio)\n",
    "            self.texts.extend(msg)\n",
    "        \n",
    "        return outline, '\\n'.join(self.texts)\n",
    "\n",
    "    def upload(self, title, descr):\n",
    "        title_small = title.lower().replace(\" \", \"_\")[:16] + str(uuid.uuid4())  # I had a filename too long once\n",
    "        tmppath = os.path.join('/Users/jong/Documents/', f\"{title_small}.mp3\")\n",
    "        with open(tmppath, \"wb\") as f:\n",
    "            f.write(merge_mp3s(self.sounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50026592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_mp3s_pydub(mp3_bytes_list):\n",
    "    \"\"\"\n",
    "    Merges multiple MP3 bytestrings into a single MP3 bytestring.\n",
    "    \n",
    "    :param mp3_bytes_list: List of MP3 bytestrings\n",
    "    :return: Merged MP3 as bytestring\n",
    "    \"\"\"\n",
    "    # Convert the first MP3 bytestring to an AudioSegment\n",
    "    combined = pydub.AudioSegment.from_file(io.BytesIO(mp3_bytes_list[0]), format=\"mp3\")\n",
    "\n",
    "    # Loop through the rest of the MP3 bytestrings and append them\n",
    "    for mp3_bytes in mp3_bytes_list[1:]:\n",
    "        next_segment = pydub.AudioSegment.from_file(io.BytesIO(mp3_bytes), format=\"mp3\")\n",
    "        combined += next_segment\n",
    "\n",
    "    # Export the combined audio to a bytestring\n",
    "    combined_buffer = io.BytesIO()\n",
    "    combined.export(combined_buffer, format=\"mp3\")\n",
    "    return combined_buffer.getvalue()\n",
    "\n",
    "def merge_mp3s(mp3_bytes_list, use_ffmpeg=True):\n",
    "    \"\"\"\n",
    "    Merges multiple MP3 bytestrings into a single MP3 bytestring,\n",
    "    preferring FFmpeg for efficiency if available and requested.\n",
    "\n",
    "    FFmpeg uses stream copy (-c copy), which is fast and low-resource,\n",
    "    but requires input MP3s to have compatible parameters (sample rate, channels).\n",
    "    If FFmpeg fails or is not used, it falls back to the pydub method.\n",
    "\n",
    "    :param mp3_bytes_list: List of MP3 bytestrings.\n",
    "    :param use_ffmpeg: Boolean, whether to attempt using FFmpeg first.\n",
    "    :return: Merged MP3 as bytestring, or None on error.\n",
    "    \"\"\"\n",
    "    if not mp3_bytes_list:\n",
    "        logger.warning(\"Received empty list for MP3 merging.\")\n",
    "        return b\"\"\n",
    "\n",
    "    # Filter out potentially invalid/empty segments early\n",
    "    valid_mp3_bytes = [bs for bs in mp3_bytes_list if isinstance(bs, bytes) and len(bs) > 100] # Basic size check\n",
    "    if not valid_mp3_bytes:\n",
    "        logger.warning(\"No valid MP3 byte strings found after filtering.\")\n",
    "        return b\"\"\n",
    "\n",
    "    # --- Attempt FFmpeg Method ---\n",
    "    ffmpeg_path = shutil.which('ffmpeg') if use_ffmpeg else None\n",
    "    if ffmpeg_path:\n",
    "        logger.info(f\"Attempting efficient merge using FFmpeg ({ffmpeg_path}) for {len(valid_mp3_bytes)} segments...\")\n",
    "        temp_files = []\n",
    "        list_file_path = None\n",
    "        output_file_path = None\n",
    "\n",
    "        try:\n",
    "            # Create temporary files for each MP3 segment\n",
    "            logger.debug(\"Creating temporary files for input segments...\")\n",
    "            for i, mp3_bytes in enumerate(valid_mp3_bytes):\n",
    "                # Suffix helps ffmpeg identify format sometimes, though not strictly necessary here\n",
    "                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\n",
    "                temp_file.write(mp3_bytes)\n",
    "                temp_files.append(temp_file.name)\n",
    "                temp_file.close() # Close the file handle\n",
    "\n",
    "            if not temp_files:\n",
    "                 raise ValueError(\"Failed to create any temporary input files.\")\n",
    "\n",
    "            # Create a temporary file list for FFmpeg's concat demuxer\n",
    "            logger.debug(\"Creating temporary file list...\")\n",
    "            with tempfile.NamedTemporaryFile(delete=False, mode='w', encoding='utf-8', suffix=\".txt\") as list_file:\n",
    "                list_file_path = list_file.name\n",
    "                for temp_fpath in temp_files:\n",
    "                    # FFmpeg requires 'file' keyword and proper quoting/escaping if paths have spaces/special chars\n",
    "                    # Using basic paths from NamedTemporaryFile should be safe, but use os.path.abspath if needed\n",
    "                    # Use forward slashes even on Windows for FFmpeg concat demuxer list file\n",
    "                    safe_path = temp_fpath.replace('\\\\', '/')\n",
    "                    list_file.write(f\"file '{safe_path}'\\n\")\n",
    "\n",
    "            # Create a temporary output file path\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as temp_out_file:\n",
    "                 output_file_path = temp_out_file.name\n",
    "\n",
    "            # Construct and run the FFmpeg command\n",
    "            # -f concat: Use the concat demuxer\n",
    "            # -safe 0: Necessary for using potentially complex/absolute paths in the list file\n",
    "            # -i list.txt: Input file list\n",
    "            # -c copy: Copy streams without re-encoding (FAST, LOW RESOURCE)\n",
    "            # -y: Overwrite output file without asking\n",
    "            command = [\n",
    "                ffmpeg_path,\n",
    "                '-y',\n",
    "                '-f', 'concat',\n",
    "                '-safe', '0',\n",
    "                '-i', list_file_path,\n",
    "                '-c', 'copy',\n",
    "                output_file_path\n",
    "            ]\n",
    "            logger.info(f\"Running FFmpeg command: {' '.join(command)}\")\n",
    "            result = subprocess.run(command, capture_output=True, text=True, check=False) # Use check=False to handle errors manually\n",
    "\n",
    "            # Check FFmpeg execution result\n",
    "            if result.returncode != 0:\n",
    "                logger.error(f\"FFmpeg failed with return code {result.returncode}.\")\n",
    "                logger.error(f\"FFmpeg stderr:\\n{result.stderr}\")\n",
    "                # Fallback or raise error - let's fallback for now\n",
    "                raise RuntimeError(\"FFmpeg concatenation failed.\")\n",
    "            else:\n",
    "                logger.info(\"FFmpeg merge successful.\")\n",
    "                # Read the merged content from the temporary output file\n",
    "                with open(output_file_path, 'rb') as f_out:\n",
    "                    merged_bytes = f_out.read()\n",
    "                return merged_bytes\n",
    "\n",
    "        except Exception as ffmpeg_err:\n",
    "            logger.warning(f\"FFmpeg merge failed: {ffmpeg_err}. Trying fallback to pydub method.\")\n",
    "            # Explicitly trigger fallback if FFmpeg fails midway\n",
    "            return merge_mp3s_pydub(valid_mp3_bytes) # Pass the filtered list\n",
    "\n",
    "        finally:\n",
    "            # Clean up temporary files\n",
    "            logger.debug(\"Cleaning up temporary files...\")\n",
    "            if list_file_path and os.path.exists(list_file_path):\n",
    "                os.remove(list_file_path)\n",
    "            if output_file_path and os.path.exists(output_file_path):\n",
    "                os.remove(output_file_path)\n",
    "            for temp_f in temp_files:\n",
    "                if os.path.exists(temp_f):\n",
    "                    os.remove(temp_f)\n",
    "            logger.debug(\"Temporary file cleanup finished.\")\n",
    "\n",
    "    else:\n",
    "        # --- Fallback to pydub Method ---\n",
    "        if use_ffmpeg:\n",
    "             logger.warning(\"FFmpeg not found or use_ffmpeg=False. Falling back to pydub merge (less efficient).\")\n",
    "        else:\n",
    "             logger.info(\"Using pydub merge method as requested.\")\n",
    "\n",
    "        return merge_mp3s_pydub(valid_mp3_bytes) # Pass the filtered list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9c22711",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIChain:\n",
    "    @classmethod\n",
    "    def serial_message(cls, chat=None, msgs=None, systems=None):\n",
    "        responses = []\n",
    "        systems = systems or [None] * len(msgs)\n",
    "        for msg, sys in tqdm(zip(msgs, systems), total=len(msgs), desc=\"Processing serial chat messages.\", unit=\"message\"):\n",
    "            if sys is not None:\n",
    "                chat._history[0]['content'] = sys\n",
    "            responses.append(chat.message(next_msg=msg))\n",
    "        return responses\n",
    "\n",
    "    @classmethod\n",
    "    def parallel_message(cls, chats=None, msgs=None, max_workers=8):\n",
    "        assert len(chats) == len(msgs), \"Lengths of chats and msgs must match.\"\n",
    "        responses = [None] * len(chats)\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as tpe:\n",
    "            futures2idx = {tpe.submit(chat.message, next_msg=msg): i for i, chat, msg in zip(range(9**99), chats, msgs)}\n",
    "            for future in tqdm(concurrent.futures.as_completed(futures2idx.keys()), total=len(futures2idx), desc=\"Processing parallel chats\", unit=\"chat\"):\n",
    "                responses[futures2idx[future]] = future.result()\n",
    "        return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6726fc1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import os\n",
    "# import re\n",
    "# import io\n",
    "# import concurrent.futures\n",
    "# import tempfile\n",
    "# import shutil\n",
    "# import subprocess\n",
    "# import logging\n",
    "# import pydub # Still needed for fallback merge or potential segment validation\n",
    "# import IPython # For display in notebooks\n",
    "# joe = \"\"\"Delivery: Methodical and rhythmic, with a steady conversational pace punctuated by moments of intense focus, spontaneous digressions, and deep, rumbling laughter that occasionally builds into breathless, high-pitched cackling.\n",
    "# Voice: Husky and resonant, carrying an approachable authority that becomes notably animated when discussing topics of personal interest, from psychedelic experiences to evolutionary biology or controversial viewpoints.\n",
    "# Tone: Persistently curious and democratically skeptical, blending street-smart pragmatism with genuine wonder at complex ideas. Effortlessly shifts between casual bro-talk, earnest philosophical inquiry, and wide-eyed astonishment.\n",
    "# Pronunciation: Direct and unadorned with subtle Massachusetts undertones, often stretching key syllables for emphasis and impact. Frequently deploys characteristic expressions like \"That's wild,\" \"A hundred percent,\" and \"Look into it\" with distinctive cadence and conviction.\"\"\"\n",
    "# joe2 = \"\"\"Delivery: Relaxed yet engaging, with natural conversational flow punctuated by thoughtful silences, sudden bursts of enthusiasm, and authentic, contagious laughter that often evolves into wheezing fits.\n",
    "# Voice: Gravelly and masculine, maintaining a casual confidence that intensifies when tackling subjects he's passionate about, from combat sports to psychedelics or fringe theories.\n",
    "# Tone: Inquisitive and unpretentious, combining everyman relatability with genuine intellectual curiosity. Seamlessly transitions between lighthearted humor, profound questions, and incredulous reactions.\n",
    "# Pronunciation: Straightforward with subtle regional inflections, frequently emphasizing certain words with elongated vowels. Regularly employs signature phrases like \"That's fascinating,\" \"Have you ever tried...\" and \"Jamie, pull that up\" with distinctive rhythm and emphasis.\"\"\"\n",
    "\n",
    "# hosts = ['Joe', 'Joh']\n",
    "\n",
    "# inst = {hosts[0]: joe, hosts[1]: joe2}\n",
    "# ep = EpisodeJoe(\n",
    "#     episode_type='narration',\n",
    "#     topic=\"Considering todays Logitech business, what kind of products will Logitech bring to market in three years when AI is dominating everything we do as humans, both work an play\",\n",
    "#     max_length=200_000,\n",
    "#     podcast=\"The Joe Rogan Experience\",\n",
    "#     text_model=DEFAULT_MODEL,\n",
    "#     host_instructions=inst,\n",
    "#     hosts=hosts,\n",
    "#     desc=\"\"\n",
    "# )\n",
    "# outline, txt = ep.step(nparts='1')\n",
    "# ep.upload(\"[Claude 4o] logihio\", \"Test7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "edcf7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import io\n",
    "# import concurrent.futures\n",
    "# import tempfile\n",
    "# import shutil\n",
    "# import subprocess\n",
    "# import logging\n",
    "# import pydub # Still needed for fallback merge or potential segment validation\n",
    "# import IPython # For display in notebooks\n",
    "\n",
    "# # Assume logger, OpenAITTS, merge_mp3s, merge_mp3s_pydub are defined above this point\n",
    "# # Example logger setup if needed:\n",
    "# try:\n",
    "#     logger\n",
    "# except NameError:\n",
    "#     logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "#     logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# class SeinfeldAudioGenerator:\n",
    "#     \"\"\"\n",
    "#     Generates audio for a Seinfeld script, handling character voices,\n",
    "#     inline instructions, TTS generation, merging, and saving.\n",
    "#     Stores intermediate results as instance attributes.\n",
    "#     \"\"\"\n",
    "#     # Default character voice definitions (can be overridden in __init__)\n",
    "#     DEFAULT_CHARACTER_VOICES = {\n",
    "#         \"JERRY\": {\n",
    "#             \"voice_name\": \"ash\",\n",
    "#             \"instructions\": \"Speak in a slightly high-pitched, observational, and often questioning tone, typical of a stand-up comedian. Emphasize ironic points with subtle sarcasm. Pace is generally moderate but can speed up during rants.\"\n",
    "#         },\n",
    "#         \"GEORGE\": {\n",
    "#             \"voice_name\": \"ballad\",\n",
    "#             \"instructions\": \"Sound like a neurotic, often whining or complaining character. Use a slightly nasal quality. Delivery can range from low and scheming to loud and panicky outbursts. Frequently uses upward inflections at the end of sentences, even when not asking a question.\"\n",
    "#         },\n",
    "#         \"ELAINE\": {\n",
    "#             \"voice_name\": \"sage\",\n",
    "#             \"instructions\": \"Speak with expressive, sometimes forceful energy. Tone can be sarcastic, exasperated, or enthusiastic. Use clear pronunciation but allow for moments of loud frustration or laughter. Pace is often quick and assertive.\"\n",
    "#         },\n",
    "#         \"KRAMER\": {\n",
    "#             \"voice_name\": \"verse\",\n",
    "#             \"instructions\": \"Use an eccentric, physically animated voice. Incorporate sudden bursts of energy, awkward pauses, clicks, and unique vocalizations. Delivery is unpredictable, often jerky, with wide variations in pitch and volume. Sound slightly manic and unconventional.\"\n",
    "#         },\n",
    "#         \"NEWMAN\": {\n",
    "#             \"voice_name\": \"coral\",\n",
    "#             \"instructions\": \"Manly voice. Speak with a sneering, often antagonistic tone, especially towards Jerry. Delivery should sound somewhat dramatic and self-important, with clear enunciation. Add a hint of passive-aggression.\"\n",
    "#         },\n",
    "#         \"DEFAULT\": {\n",
    "#             \"voice_name\": \"alloy\",\n",
    "#             \"instructions\": \"Speak in a neutral, standard American accent.\"\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#     def __init__(self, script_filepath, output_dir=None, tts_model='gpt-4o-mini-tts', max_workers=5, character_voices=None):\n",
    "#         \"\"\"\n",
    "#         Initializes the audio generator.\n",
    "\n",
    "#         Args:\n",
    "#             script_filepath (str): Path to the input text script file.\n",
    "#             output_dir (str, optional): Directory to save the output MP3 and debug segments.\n",
    "#                                         If None, uses the script's directory. Defaults to None.\n",
    "#             tts_model (str, optional): The OpenAI TTS model to use. Defaults to 'gpt-4o-mini-tts'.\n",
    "#             max_workers (int, optional): Max threads for concurrent TTS calls. Defaults to 5.\n",
    "#             character_voices (dict, optional): Override default character voice definitions.\n",
    "#                                                 Defaults to None (uses DEFAULT_CHARACTER_VOICES).\n",
    "#         \"\"\"\n",
    "#         self.script_filepath = script_filepath\n",
    "#         self.output_dir = output_dir\n",
    "#         self.tts_model = tts_model\n",
    "#         self.max_workers = max_workers\n",
    "#         self.character_voices = character_voices or self.DEFAULT_CHARACTER_VOICES.copy() # Use default if none provided\n",
    "\n",
    "#         # --- State Variables ---\n",
    "#         self.script_content_lines = []\n",
    "#         self.dialogue_segments = [] # List of tuples: (index, char_name, text_for_tts, tts_instance_key, combined_instructions)\n",
    "#         self.tts_instances = {} # Dict storing initialized TTS clients {voice_name: instance}\n",
    "#         self.raw_audio_results = {} # Dict storing successful results {index: audio_bytes}\n",
    "#         self.ordered_audio_bytes = [] # List of successful audio bytes in order\n",
    "#         self.merged_audio = None # Bytes of the final merged audio\n",
    "#         self.final_output_path = None # Path where the final audio is saved\n",
    "#         self.debug_segments_path = None # Path where debug segments were saved\n",
    "#         self.errors = [] # List to store errors encountered during processing\n",
    "\n",
    "#         # --- Regex (defined once) ---\n",
    "#         self.dialogue_pattern = re.compile(r'^\\s*([A-Z][A-Z\\s]+):\\s*(.*)')\n",
    "#         self.inline_instruction_pattern = re.compile(r'\\*\\((.*?)\\)\\*')\n",
    "#         self.inline_removal_pattern = re.compile(r'\\s*\\*\\([^)]*\\)\\*\\s*')\n",
    "\n",
    "#         # --- Determine Base Save Directory ---\n",
    "#         self.script_dir = os.path.dirname(self.script_filepath)\n",
    "#         self.base_save_directory = self.output_dir if self.output_dir else self.script_dir\n",
    "#         self.script_basename = os.path.splitext(os.path.basename(self.script_filepath))[0]\n",
    "\n",
    "\n",
    "#     def _log_error(self, message, exception=None):\n",
    "#         \"\"\"Helper to log errors and store them.\"\"\"\n",
    "#         full_message = f\"{message}{f': {exception}' if exception else ''}\"\n",
    "#         logger.error(full_message, exc_info=exception is not None)\n",
    "#         self.errors.append(full_message)\n",
    "\n",
    "#     def _initialize_tts(self):\n",
    "#         \"\"\"Initializes TTS instances for all required voices.\"\"\"\n",
    "#         logger.info(\"Initializing TTS instances...\")\n",
    "#         required_voices = {details['voice_name'] for details in self.character_voices.values()}\n",
    "\n",
    "#         for voice_name in required_voices:\n",
    "#             if voice_name not in self.tts_instances:\n",
    "#                 try:\n",
    "#                     # Assumes OpenAITTS class is globally available\n",
    "#                     self.tts_instances[voice_name] = OpenAITTS(voice=voice_name, model=self.tts_model)\n",
    "#                     logger.info(f\"Initialized TTS for voice '{voice_name}': {self.tts_instances[voice_name]}\")\n",
    "#                 except NameError:\n",
    "#                     self._log_error(\"OpenAITTS class definition not found. Cannot initialize TTS.\")\n",
    "#                     return False\n",
    "#                 except Exception as e:\n",
    "#                     self._log_error(f\"Error initializing TTS instance for '{voice_name}'\", e)\n",
    "#                     return False\n",
    "#         return True\n",
    "\n",
    "#     def parse_script(self):\n",
    "#         \"\"\"Reads the script file and parses dialogue segments.\"\"\"\n",
    "#         logger.info(f\"Parsing script file: {self.script_filepath}\")\n",
    "#         if not os.path.exists(self.script_filepath):\n",
    "#             self._log_error(f\"Script file not found: {self.script_filepath}\")\n",
    "#             return False\n",
    "\n",
    "#         try:\n",
    "#             with open(self.script_filepath, 'r', encoding='utf-8') as f:\n",
    "#                 self.script_content_lines = f.readlines()\n",
    "#         except Exception as e:\n",
    "#             self._log_error(f\"Error reading script file {self.script_filepath}\", e)\n",
    "#             return False\n",
    "\n",
    "#         self.dialogue_segments = [] # Reset if called multiple times\n",
    "#         segment_index = 0\n",
    "#         for line_num, line in enumerate(self.script_content_lines, 1):\n",
    "#             match = self.dialogue_pattern.match(line)\n",
    "#             if match:\n",
    "#                 character_name = match.group(1).strip()\n",
    "#                 dialogue_text = match.group(2).strip()\n",
    "\n",
    "#                 if not dialogue_text: continue # Skip empty\n",
    "\n",
    "#                 # Find voice config\n",
    "#                 char_config = self.character_voices.get(character_name, self.character_voices[\"DEFAULT\"])\n",
    "#                 if character_name not in self.character_voices:\n",
    "#                      logger.warning(f\"Line {line_num}: Character '{character_name}' not explicitly defined. Using default voice.\")\n",
    "\n",
    "#                 base_instructions = char_config[\"instructions\"]\n",
    "#                 tts_voice_name = char_config[\"voice_name\"] # Key to look up instance\n",
    "#                 final_instructions = base_instructions\n",
    "#                 text_for_tts = dialogue_text\n",
    "\n",
    "#                 # Check for inline instructions\n",
    "#                 inline_match = self.inline_instruction_pattern.search(dialogue_text)\n",
    "#                 if inline_match:\n",
    "#                     inline_instruction = inline_match.group(1).strip()\n",
    "#                     # You might want to adjust how instructions are combined:\n",
    "#                     final_instructions = f\"Follow this specific instruction: '{inline_instruction}'. Also adhere to the general character guidance: {base_instructions}\"\n",
    "#                     text_for_tts = self.inline_removal_pattern.sub(' ', dialogue_text).strip()\n",
    "#                     logger.info(f\"Line {line_num}: Found inline instruction '{inline_instruction}' for {character_name}. Cleaned text: '{text_for_tts[:50]}...'\")\n",
    "\n",
    "#                 if not text_for_tts:\n",
    "#                     logger.warning(f\"Line {line_num}: Dialogue text became empty after removing instruction for {character_name}. Skipping segment.\")\n",
    "#                     continue\n",
    "\n",
    "#                 self.dialogue_segments.append(\n",
    "#                     (segment_index, character_name, text_for_tts, tts_voice_name, final_instructions)\n",
    "#                 )\n",
    "#                 segment_index += 1\n",
    "\n",
    "#         if not self.dialogue_segments:\n",
    "#             self._log_error(\"No valid dialogue segments found or parsed in the script.\")\n",
    "#             return False\n",
    "\n",
    "#         logger.info(f\"Parsed {len(self.dialogue_segments)} dialogue segments.\")\n",
    "#         return True\n",
    "\n",
    "#     def _generate_speech_segment_task(self, index, char_name, text, tts_instance_key, instructions):\n",
    "#         \"\"\"Task function for generating single audio segment (used by thread pool).\"\"\"\n",
    "#         try:\n",
    "#             tts_instance = self.tts_instances.get(tts_instance_key)\n",
    "#             if not tts_instance:\n",
    "#                  raise ValueError(f\"TTS instance for key '{tts_instance_key}' not found.\")\n",
    "\n",
    "#             logger.info(f\"Requesting TTS for segment {index} ({char_name}): '{text[:50]}...' using {tts_instance}\")\n",
    "\n",
    "#             # Pass instructions based on instance type and model\n",
    "#             audio_bytes = None\n",
    "#             if isinstance(tts_instance, OpenAITTS) and 'gpt-4o' in tts_instance.model:\n",
    "#                 # logger.debug(f\"Segment {index} instructions: {instructions}\")\n",
    "#                 audio_bytes = tts_instance.tts(text, instructions=instructions)\n",
    "#                 logger.info(f\"Received TTS for segment {index} ({char_name}) with instructions.\")\n",
    "#             else:\n",
    "#                 if instructions and isinstance(tts_instance, OpenAITTS):\n",
    "#                     logger.warning(f\"Segment {index} ({char_name}): Instructions provided but model {tts_instance.model} might not support them. Calling TTS without.\")\n",
    "#                 elif instructions and not isinstance(tts_instance, OpenAITTS):\n",
    "#                     logger.warning(f\"Segment {index} ({char_name}): TTS instance is not OpenAITTS, instructions ignored.\")\n",
    "#                 audio_bytes = tts_instance.tts(text)\n",
    "#                 logger.info(f\"Received TTS for segment {index} ({char_name}) without instructions.\")\n",
    "\n",
    "#             # Validation\n",
    "#             if not audio_bytes or len(audio_bytes) < 100: # Check for empty or too small\n",
    "#                 logger.warning(f\"TTS returned invalid/empty audio for segment {index} ({char_name}). Size={len(audio_bytes) if audio_bytes else 0}. Skipping.\")\n",
    "#                 return index, None\n",
    "#             return index, audio_bytes\n",
    "\n",
    "#         except Exception as e:\n",
    "#             self._log_error(f\"Error in TTS task for segment {index} ({char_name})\", e)\n",
    "#             return index, None # Return index and None on error\n",
    "\n",
    "#     def generate_segment_audio(self):\n",
    "#         \"\"\"Generates audio for all parsed dialogue segments concurrently.\"\"\"\n",
    "#         if not self.dialogue_segments:\n",
    "#             self._log_error(\"Cannot generate audio, no dialogue segments parsed.\")\n",
    "#             return False\n",
    "#         if not self.tts_instances:\n",
    "#              self._log_error(\"Cannot generate audio, TTS instances not initialized.\")\n",
    "#              return False\n",
    "\n",
    "#         logger.info(f\"Generating audio for {len(self.dialogue_segments)} segments using up to {self.max_workers} workers...\")\n",
    "#         self.raw_audio_results = {} # Reset results\n",
    "#         futures_to_index = {}\n",
    "\n",
    "#         with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "#             for index, char_name, text_to_speak, tts_instance_key, combined_instructions in self.dialogue_segments:\n",
    "#                 future = executor.submit(\n",
    "#                     self._generate_speech_segment_task,\n",
    "#                     index, char_name, text_to_speak, tts_instance_key, combined_instructions\n",
    "#                 )\n",
    "#                 futures_to_index[future] = index\n",
    "\n",
    "#             for future in concurrent.futures.as_completed(futures_to_index):\n",
    "#                 original_index = futures_to_index[future]\n",
    "#                 try:\n",
    "#                     idx, audio_data = future.result()\n",
    "#                     if audio_data:\n",
    "#                         self.raw_audio_results[idx] = audio_data\n",
    "#                     # else: Error/Warning already logged in _generate_speech_segment_task\n",
    "#                 except Exception as e:\n",
    "#                      # This catches errors during future.result() itself, unlikely if task handles errors\n",
    "#                     self._log_error(f\"Error retrieving result for segment {original_index}\", e)\n",
    "\n",
    "#         if not self.raw_audio_results:\n",
    "#             logger.warning(\"No audio segments were successfully generated.\")\n",
    "#             # Don't necessarily return False, allow inspection of errors\n",
    "#             # Maybe return False if *zero* segments succeeded?\n",
    "#             return len(self.raw_audio_results) > 0\n",
    "\n",
    "#         logger.info(f\"Successfully generated audio for {len(self.raw_audio_results)} out of {len(self.dialogue_segments)} segments.\")\n",
    "#         return True\n",
    "\n",
    "#     def save_debug_segments(self):\n",
    "#         \"\"\"Saves successfully generated individual audio segments to disk.\"\"\"\n",
    "#         if not self.raw_audio_results:\n",
    "#             logger.info(\"No raw audio results to save as debug segments.\")\n",
    "#             return False\n",
    "\n",
    "#         self.debug_segments_path = os.path.join(self.base_save_directory, \"_debug_audio_segments\")\n",
    "#         logger.info(f\"Saving {len(self.raw_audio_results)} individual audio segments for debugging to: {self.debug_segments_path}\")\n",
    "\n",
    "#         try:\n",
    "#             os.makedirs(self.debug_segments_path, exist_ok=True)\n",
    "\n",
    "#             # Correctly iterate over the dictionary items\n",
    "#             for index, audio_bytes in self.raw_audio_results.items():\n",
    "#                 if not audio_bytes: continue # Should not happen if stored, but check\n",
    "\n",
    "#                 try:\n",
    "#                     # Retrieve character name for filename\n",
    "#                     # dialogue_segments: (idx, char_name, text, instance_key, instruction)\n",
    "#                     character_name = self.dialogue_segments[index][1]\n",
    "#                     safe_char_name = character_name.replace(' ', '_').strip()\n",
    "#                 except (IndexError, TypeError) as e: # Catch potential errors accessing dialogue_segments\n",
    "#                     self._log_error(f\"Error getting character name for debug segment index {index}\", e)\n",
    "#                     safe_char_name = \"UnknownChar\"\n",
    "\n",
    "#                 segment_filename = f\"segment_{index:03d}_{safe_char_name}.mp3\"\n",
    "#                 segment_filepath = os.path.join(self.debug_segments_path, segment_filename)\n",
    "\n",
    "#                 try:\n",
    "#                     with open(segment_filepath, 'wb') as seg_file:\n",
    "#                         seg_file.write(audio_bytes)\n",
    "#                 except IOError as e:\n",
    "#                     self._log_error(f\"Failed to save debug segment {segment_filepath}\", e)\n",
    "\n",
    "#             return True # Indicate that saving was attempted\n",
    "\n",
    "#         except Exception as e:\n",
    "#             self._log_error(f\"Could not create or write to debug directory {self.debug_segments_path}\", e)\n",
    "#             return False\n",
    "\n",
    "\n",
    "#     def merge_audio(self, use_ffmpeg=True):\n",
    "#         \"\"\"Merges the generated audio segments in order.\"\"\"\n",
    "#         if not self.raw_audio_results:\n",
    "#             self._log_error(\"No audio segments available to merge.\")\n",
    "#             return False\n",
    "\n",
    "#         logger.info(\"Preparing to merge audio segments...\")\n",
    "#         self.ordered_audio_bytes = []\n",
    "#         missing_count = 0\n",
    "#         for i in range(len(self.dialogue_segments)):\n",
    "#             audio_segment = self.raw_audio_results.get(i)\n",
    "#             if audio_segment:\n",
    "#                 self.ordered_audio_bytes.append(audio_segment)\n",
    "#             else:\n",
    "#                 missing_count += 1\n",
    "#                 # logger.warning(f\"Audio segment for original index {i} was missing or invalid, skipping in merge.\")\n",
    "\n",
    "#         if missing_count > 0:\n",
    "#              logger.warning(f\"{missing_count} audio segments were missing or invalid and will be skipped in the final merge.\")\n",
    "\n",
    "#         if not self.ordered_audio_bytes:\n",
    "#             self._log_error(\"No valid audio segments remained after ordering. Cannot merge.\")\n",
    "#             return False\n",
    "\n",
    "#         logger.info(f\"Merging {len(self.ordered_audio_bytes)} audio segments in order...\")\n",
    "#         try:\n",
    "#             # Assumes merge_mp3s function is globally available\n",
    "#             self.merged_audio = merge_mp3s(self.ordered_audio_bytes, use_ffmpeg=use_ffmpeg)\n",
    "#             if self.merged_audio:\n",
    "#                  logger.info(\"Audio segments merged successfully.\")\n",
    "#                  return True\n",
    "#             else:\n",
    "#                  # merge_mp3s should log its own errors, but we add one here too\n",
    "#                  self._log_error(\"Merging process returned no data.\")\n",
    "#                  return False\n",
    "#         except NameError:\n",
    "#             self._log_error(\"merge_mp3s function definition not found. Cannot merge audio.\")\n",
    "#             return False\n",
    "#         except Exception as e:\n",
    "#             self._log_error(\"Error during merging process\", e)\n",
    "#             return False\n",
    "\n",
    "#     def save_final_audio(self):\n",
    "#         \"\"\"Saves the merged audio to the final output file.\"\"\"\n",
    "#         if not self.merged_audio:\n",
    "#             self._log_error(\"No merged audio data available to save.\")\n",
    "#             return None # Return None for path if not saved\n",
    "\n",
    "#         output_filename = f\"{self.script_basename}_audio.mp3\"\n",
    "#         self.final_output_path = os.path.join(self.base_save_directory, output_filename)\n",
    "\n",
    "#         logger.info(f\"Saving final merged audio to: {self.final_output_path}\")\n",
    "#         try:\n",
    "#             os.makedirs(self.base_save_directory, exist_ok=True)\n",
    "#             with open(self.final_output_path, 'wb') as f_out:\n",
    "#                 f_out.write(self.merged_audio)\n",
    "#             logger.info(\"Final audio saved successfully.\")\n",
    "\n",
    "#             # Optional: Display audio in Jupyter environment\n",
    "#             self._display_audio_if_possible(self.final_output_path)\n",
    "\n",
    "#             return self.final_output_path # Return the path on success\n",
    "#         except Exception as e:\n",
    "#             self._log_error(f\"Error saving merged audio file to {self.final_output_path}\", e)\n",
    "#             self.final_output_path = None # Reset path on error\n",
    "#             return None\n",
    "\n",
    "#     def _display_audio_if_possible(self, audio_path):\n",
    "#         \"\"\"Try displaying audio in IPython if available.\"\"\"\n",
    "#         try:\n",
    "#             if 'IPython' in globals() and IPython.get_ipython():\n",
    "#                  IPython.display.display(IPython.display.Audio(audio_path))\n",
    "#         except Exception as e:\n",
    "#              logger.debug(f\"Could not display audio in IPython: {e}\")\n",
    "#              pass # Ignore if IPython display fails\n",
    "\n",
    "\n",
    "#     def process(self, save_debug=True, merge_with_ffmpeg=True):\n",
    "#         \"\"\"\n",
    "#         Runs the full audio generation pipeline:\n",
    "#         Initialize TTS -> Parse Script -> Generate Audio -> [Save Debug Segments] -> Merge Audio -> Save Final Audio\n",
    "\n",
    "#         Args:\n",
    "#             save_debug (bool): If True, save individual segments after generation.\n",
    "#             merge_with_ffmpeg (bool): If True, attempt to use FFmpeg for merging.\n",
    "\n",
    "#         Returns:\n",
    "#             str or None: The path to the final saved audio file if successful, otherwise None.\n",
    "#         \"\"\"\n",
    "#         self.errors = [] # Clear previous errors if re-processing\n",
    "\n",
    "#         if not self._initialize_tts():\n",
    "#             return None # Error already logged\n",
    "\n",
    "#         if not self.parse_script():\n",
    "#             return None # Error already logged\n",
    "\n",
    "#         if not self.generate_segment_audio() and not self.raw_audio_results:\n",
    "#              # If generation returns false AND there are zero results, stop.\n",
    "#              logger.error(\"Audio generation failed completely.\")\n",
    "#              return None\n",
    "\n",
    "#         if save_debug:\n",
    "#             self.save_debug_segments() # Attempt saving, don't stop pipeline if only this fails\n",
    "\n",
    "#         if not self.merge_audio(use_ffmpeg=merge_with_ffmpeg):\n",
    "#             # Merging failed, but self.raw_audio_results might still be useful\n",
    "#             logger.error(\"Audio merging failed. Check errors and debug segments if saved.\")\n",
    "#             return None # Stop before saving final file\n",
    "\n",
    "#         final_path = self.save_final_audio()\n",
    "#         # final_path will be None if saving failed\n",
    "\n",
    "#         if not final_path and self.errors:\n",
    "#              logger.error(\"Processing finished with errors. Final audio not saved.\")\n",
    "#         elif not final_path:\n",
    "#              logger.error(\"Processing finished, but final audio could not be saved for unknown reasons.\")\n",
    "#         else:\n",
    "#              logger.info(\"Processing finished successfully.\")\n",
    "\n",
    "#         return final_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4ed315c-64f4-42a0-b81a-cdd0b850bd94",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# script_file = '/Users/jong/Documents/PodcastGPT/tv_show_workspaces/20250330_191704/Seinfeld_hilarious_Seinfeld_episode_abo/final_script_3acts.txt'\n",
    "\n",
    "# # Instantiate the generator\n",
    "# generator = SeinfeldAudioGenerator(script_file) # , output_dir='/custom/output/path' if needed)\n",
    "\n",
    "# # Run the process\n",
    "# final_audio_path = generator.process(save_debug=True, merge_with_ffmpeg=True)\n",
    "\n",
    "# if final_audio_path:\n",
    "#     print(f\"Success! Final audio saved to: {final_audio_path}\")\n",
    "# else:\n",
    "#     print(\"Processing failed or did not complete.\")\n",
    "#     print(\"Check generator state for details:\")\n",
    "#     print(f\"  - Parsed Segments: {len(generator.dialogue_segments)}\")\n",
    "#     print(f\"  - Generated Audio Segments: {len(generator.raw_audio_results)}\")\n",
    "#     if generator.debug_segments_path:\n",
    "#         print(f\"  - Debug Segments saved to: {generator.debug_segments_path}\")\n",
    "#     if generator.ordered_audio_bytes and not generator.merged_audio:\n",
    "#         print(f\"  - Merging failed, but {len(generator.ordered_audio_bytes)} ordered segments exist.\")\n",
    "#     print(f\"  - Errors encountered: {len(generator.errors)}\")\n",
    "#     for i, err in enumerate(generator.errors):\n",
    "#         print(f\"    Error {i+1}: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "460b7ad2",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import io\n",
    "# import concurrent.futures\n",
    "# import tempfile\n",
    "# import shutil\n",
    "# import subprocess\n",
    "# import logging\n",
    "# import pydub # Still needed for fallback merge or potential segment validation\n",
    "# import IPython # For display in notebooks\n",
    "\n",
    "# # Assume logger, OpenAITTS, merge_mp3s, merge_mp3s_pydub are defined above this point\n",
    "# # Example logger setup if needed:\n",
    "# try:\n",
    "#     logger\n",
    "# except NameError:\n",
    "#     logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "#     logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# class SeinfeldAudioGenerator:\n",
    "#     \"\"\"\n",
    "#     Generates audio for a Seinfeld script, handling character voices,\n",
    "#     inline instructions (**adjective phrase**), TTS generation, merging, and saving.\n",
    "#     Stores intermediate results as instance attributes.\n",
    "#     \"\"\"\n",
    "#     # Default character voice definitions (can be overridden in __init__)\n",
    "#     DEFAULT_CHARACTER_VOICES = {\n",
    "#         \"JERRY\": {\n",
    "#             \"voice_name\": \"ash\",\n",
    "#             \"instructions\": \"Speak in a slightly high-pitched, observational, and often questioning tone, typical of a stand-up comedian. Emphasize ironic points with subtle sarcasm. Pace is generally moderate but can speed up during rants.\"\n",
    "#         },\n",
    "#         \"GEORGE\": {\n",
    "#             \"voice_name\": \"ballad\",\n",
    "#             \"instructions\": \"Sound like a neurotic, often whining or complaining character. Use a slightly nasal quality. Delivery can range from low and scheming to loud and panicky outbursts. Frequently uses upward inflections at the end of sentences, even when not asking a question.\"\n",
    "#         },\n",
    "#         \"ELAINE\": {\n",
    "#             \"voice_name\": \"sage\",\n",
    "#             \"instructions\": \"Speak with expressive, sometimes forceful energy. Tone can be sarcastic, exasperated, or enthusiastic. Use clear pronunciation but allow for moments of loud frustration or laughter. Pace is often quick and assertive.\"\n",
    "#         },\n",
    "#         \"KRAMER\": {\n",
    "#             \"voice_name\": \"verse\",\n",
    "#             \"instructions\": \"Use an eccentric, physically animated voice. Incorporate sudden bursts of energy, awkward pauses, clicks, and unique vocalizations. Delivery is unpredictable, often jerky, with wide variations in pitch and volume. Sound slightly manic and unconventional.\"\n",
    "#         },\n",
    "#         \"NEWMAN\": {\n",
    "#             \"voice_name\": \"coral\",\n",
    "#             \"instructions\": \"Manly voice. Speak with a sneering, often antagonistic tone, especially towards Jerry. Delivery should sound somewhat dramatic and self-important, with clear enunciation. Add a hint of passive-aggression.\"\n",
    "#         },\n",
    "#         \"DEFAULT\": {\n",
    "#             \"voice_name\": \"alloy\",\n",
    "#             \"instructions\": \"Speak in a neutral, standard American accent.\"\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#     def __init__(self, script_filepath, output_dir=None, tts_model='gpt-4o-mini-tts', max_workers=5, character_voices=None):\n",
    "#         \"\"\"\n",
    "#         Initializes the audio generator.\n",
    "\n",
    "#         Args:\n",
    "#             script_filepath (str): Path to the input text script file. Expected format:\n",
    "#                                     CHARACTER: **Optional instruction** Dialogue text\n",
    "#             output_dir (str, optional): Directory to save the output MP3 and debug segments.\n",
    "#             tts_model (str, optional): The OpenAI TTS model to use.\n",
    "#             max_workers (int, optional): Max threads for concurrent TTS calls.\n",
    "#             character_voices (dict, optional): Override default character voice definitions.\n",
    "#         \"\"\"\n",
    "#         self.script_filepath = script_filepath\n",
    "#         self.output_dir = output_dir\n",
    "#         self.tts_model = tts_model\n",
    "#         self.max_workers = max_workers\n",
    "#         self.character_voices = character_voices or self.DEFAULT_CHARACTER_VOICES.copy()\n",
    "\n",
    "#         # --- State Variables ---\n",
    "#         self.script_content_lines = []\n",
    "#         self.dialogue_segments = []\n",
    "#         self.tts_instances = {}\n",
    "#         self.raw_audio_results = {}\n",
    "#         self.ordered_audio_bytes = []\n",
    "#         self.merged_audio = None\n",
    "#         self.final_output_path = None\n",
    "#         self.debug_segments_path = None\n",
    "#         self.errors = []\n",
    "\n",
    "#         # --- Regex (Updated for **adjective phrase**) ---\n",
    "#         self.dialogue_pattern = re.compile(r'^\\s*([A-Z][A-Z\\s]+):\\s*(.*)')\n",
    "#         # Matches **anything non-greedy between double asterisks**\n",
    "#         self.inline_instruction_pattern = re.compile(r'\\*\\*(.*?)\\*\\*')\n",
    "#         # Removes the double asterisk pattern and surrounding whitespace\n",
    "#         self.inline_removal_pattern = re.compile(r'\\s*\\*\\*[^*]*\\*\\*\\s*')\n",
    "\n",
    "#         # --- Determine Base Save Directory ---\n",
    "#         self.script_dir = os.path.dirname(self.script_filepath)\n",
    "#         self.base_save_directory = self.output_dir if self.output_dir else self.script_dir\n",
    "#         self.script_basename = os.path.splitext(os.path.basename(self.script_filepath))[0]\n",
    "\n",
    "\n",
    "#     def _log_error(self, message, exception=None):\n",
    "#         \"\"\"Helper to log errors and store them.\"\"\"\n",
    "#         full_message = f\"{message}{f': {exception}' if exception else ''}\"\n",
    "#         logger.error(full_message, exc_info=exception is not None)\n",
    "#         self.errors.append(full_message)\n",
    "\n",
    "#     def _initialize_tts(self):\n",
    "#         \"\"\"Initializes TTS instances for all required voices.\"\"\"\n",
    "#         logger.info(\"Initializing TTS instances...\")\n",
    "#         required_voices = {details['voice_name'] for details in self.character_voices.values()}\n",
    "\n",
    "#         for voice_name in required_voices:\n",
    "#             if voice_name not in self.tts_instances:\n",
    "#                 try:\n",
    "#                     # Assumes OpenAITTS class is globally available\n",
    "#                     self.tts_instances[voice_name] = OpenAITTS(voice=voice_name, model=self.tts_model)\n",
    "#                     logger.info(f\"Initialized TTS for voice '{voice_name}': {self.tts_instances[voice_name]}\")\n",
    "#                 except NameError:\n",
    "#                     self._log_error(\"OpenAITTS class definition not found. Cannot initialize TTS.\")\n",
    "#                     return False\n",
    "#                 except Exception as e:\n",
    "#                     self._log_error(f\"Error initializing TTS instance for '{voice_name}'\", e)\n",
    "#                     return False\n",
    "#         return True\n",
    "\n",
    "#     def parse_script(self):\n",
    "#         \"\"\"Reads the script file and parses dialogue segments, handling **inline instructions**.\"\"\"\n",
    "#         logger.info(f\"Parsing script file: {self.script_filepath}\")\n",
    "#         if not os.path.exists(self.script_filepath):\n",
    "#             self._log_error(f\"Script file not found: {self.script_filepath}\")\n",
    "#             return False\n",
    "\n",
    "#         try:\n",
    "#             with open(self.script_filepath, 'r', encoding='utf-8') as f:\n",
    "#                 self.script_content_lines = f.readlines()\n",
    "#         except Exception as e:\n",
    "#             self._log_error(f\"Error reading script file {self.script_filepath}\", e)\n",
    "#             return False\n",
    "\n",
    "#         self.dialogue_segments = [] # Reset if called multiple times\n",
    "#         segment_index = 0\n",
    "#         for line_num, line in enumerate(self.script_content_lines, 1):\n",
    "#             match = self.dialogue_pattern.match(line)\n",
    "#             if match:\n",
    "#                 character_name = match.group(1).strip()\n",
    "#                 dialogue_text = match.group(2).strip()\n",
    "\n",
    "#                 if not dialogue_text: continue # Skip empty\n",
    "\n",
    "#                 # Find voice config\n",
    "#                 char_config = self.character_voices.get(character_name, self.character_voices[\"DEFAULT\"])\n",
    "#                 if character_name not in self.character_voices:\n",
    "#                      logger.warning(f\"Line {line_num}: Character '{character_name}' not explicitly defined. Using default voice.\")\n",
    "\n",
    "#                 base_instructions = char_config[\"instructions\"]\n",
    "#                 tts_voice_name = char_config[\"voice_name\"]\n",
    "#                 final_instructions = base_instructions\n",
    "#                 text_for_tts = dialogue_text\n",
    "#                 inline_instruction = None\n",
    "\n",
    "#                 # Check for **inline instructions**\n",
    "#                 # Use search to find the first occurrence\n",
    "#                 inline_match = self.inline_instruction_pattern.search(dialogue_text)\n",
    "#                 if inline_match:\n",
    "#                     # Extract the adjective/phrase inside **...**\n",
    "#                     inline_instruction = inline_match.group(1).strip()\n",
    "\n",
    "#                     # --- Combine instructions (Updated Format) ---\n",
    "#                     # Prepend the specific instruction for clarity to the TTS\n",
    "#                     final_instructions = f\"For this line, the delivery should be '{inline_instruction}'. General character guidance: {base_instructions}\"\n",
    "\n",
    "#                     # Remove the instruction pattern from the text to be spoken\n",
    "#                     text_for_tts = self.inline_removal_pattern.sub(' ', dialogue_text).strip()\n",
    "\n",
    "#                     logger.info(f\"Line {line_num}: Found inline instruction '**{inline_instruction}**' for {character_name}. Combined instructions. Cleaned text: '{text_for_tts[:50]}...'\")\n",
    "#                 # else: No inline instruction found, use base_instructions and original dialogue_text\n",
    "\n",
    "#                 if not text_for_tts:\n",
    "#                     logger.warning(f\"Line {line_num}: Dialogue text became empty after removing instruction for {character_name}. Skipping segment.\")\n",
    "#                     continue\n",
    "\n",
    "#                 self.dialogue_segments.append(\n",
    "#                     (segment_index, character_name, text_for_tts, tts_voice_name, final_instructions)\n",
    "#                 )\n",
    "#                 segment_index += 1\n",
    "\n",
    "#         if not self.dialogue_segments:\n",
    "#             self._log_error(\"No valid dialogue segments found or parsed in the script.\")\n",
    "#             return False\n",
    "\n",
    "#         logger.info(f\"Parsed {len(self.dialogue_segments)} dialogue segments.\")\n",
    "#         return True\n",
    "\n",
    "#     # --- Methods _generate_speech_segment_task, generate_segment_audio, ---\n",
    "#     # --- save_debug_segments, merge_audio, save_final_audio,          ---\n",
    "#     # --- _display_audio_if_possible, process remain UNCHANGED from     ---\n",
    "#     # --- the previous class version.                                   ---\n",
    "\n",
    "#     def _generate_speech_segment_task(self, index, char_name, text, tts_instance_key, instructions):\n",
    "#         \"\"\"Task function for generating single audio segment (used by thread pool).\"\"\"\n",
    "#         try:\n",
    "#             tts_instance = self.tts_instances.get(tts_instance_key)\n",
    "#             if not tts_instance:\n",
    "#                  raise ValueError(f\"TTS instance for key '{tts_instance_key}' not found.\")\n",
    "\n",
    "#             logger.info(f\"Requesting TTS for segment {index} ({char_name}): '{text[:50]}...' using {tts_instance}\")\n",
    "\n",
    "#             # Pass instructions based on instance type and model\n",
    "#             audio_bytes = None\n",
    "#             # Check specifically for OpenAITTS and a model known to support instructions well\n",
    "#             if isinstance(tts_instance, OpenAITTS) and ('gpt-4o' in tts_instance.model or 'tts-1' in tts_instance.model): # Check specific models if needed\n",
    "#                 # logger.debug(f\"Segment {index} instructions: {instructions}\")\n",
    "#                 audio_bytes = tts_instance.tts(text, instructions=instructions)\n",
    "#                 logger.info(f\"Received TTS for segment {index} ({char_name}) with instructions.\")\n",
    "#             else:\n",
    "#                 if instructions and isinstance(tts_instance, OpenAITTS):\n",
    "#                     logger.warning(f\"Segment {index} ({char_name}): Instructions provided but model {tts_instance.model} might not support them. Calling TTS without.\")\n",
    "#                 elif instructions and not isinstance(tts_instance, OpenAITTS):\n",
    "#                     logger.warning(f\"Segment {index} ({char_name}): TTS instance is not OpenAITTS, instructions ignored.\")\n",
    "#                 # Call the tts method without the 'instructions' keyword argument if not supported\n",
    "#                 audio_bytes = tts_instance.tts(text)\n",
    "#                 logger.info(f\"Received TTS for segment {index} ({char_name}) without instructions (or model doesn't support them).\")\n",
    "\n",
    "\n",
    "#             # Validation\n",
    "#             if not audio_bytes or len(audio_bytes) < 100: # Check for empty or too small\n",
    "#                 logger.warning(f\"TTS returned invalid/empty audio for segment {index} ({char_name}). Size={len(audio_bytes) if audio_bytes else 0}. Skipping.\")\n",
    "#                 return index, None\n",
    "#             return index, audio_bytes\n",
    "\n",
    "#         except Exception as e:\n",
    "#             self._log_error(f\"Error in TTS task for segment {index} ({char_name})\", e)\n",
    "#             return index, None # Return index and None on error\n",
    "\n",
    "#     def generate_segment_audio(self):\n",
    "#         \"\"\"Generates audio for all parsed dialogue segments concurrently.\"\"\"\n",
    "#         if not self.dialogue_segments:\n",
    "#             self._log_error(\"Cannot generate audio, no dialogue segments parsed.\")\n",
    "#             return False\n",
    "#         if not self.tts_instances:\n",
    "#              self._log_error(\"Cannot generate audio, TTS instances not initialized.\")\n",
    "#              return False\n",
    "\n",
    "#         logger.info(f\"Generating audio for {len(self.dialogue_segments)} segments using up to {self.max_workers} workers...\")\n",
    "#         self.raw_audio_results = {} # Reset results\n",
    "#         futures_to_index = {}\n",
    "\n",
    "#         with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "#             for index, char_name, text_to_speak, tts_instance_key, combined_instructions in self.dialogue_segments:\n",
    "#                 future = executor.submit(\n",
    "#                     self._generate_speech_segment_task,\n",
    "#                     index, char_name, text_to_speak, tts_instance_key, combined_instructions\n",
    "#                 )\n",
    "#                 futures_to_index[future] = index\n",
    "\n",
    "#             for future in concurrent.futures.as_completed(futures_to_index):\n",
    "#                 original_index = futures_to_index[future]\n",
    "#                 try:\n",
    "#                     idx, audio_data = future.result()\n",
    "#                     if audio_data:\n",
    "#                         self.raw_audio_results[idx] = audio_data\n",
    "#                     # else: Error/Warning already logged in _generate_speech_segment_task\n",
    "#                 except Exception as e:\n",
    "#                      # This catches errors during future.result() itself, unlikely if task handles errors\n",
    "#                     self._log_error(f\"Error retrieving result for segment {original_index}\", e)\n",
    "\n",
    "#         if not self.raw_audio_results:\n",
    "#             logger.warning(\"No audio segments were successfully generated.\")\n",
    "#             return len(self.raw_audio_results) > 0 # Return False if zero segments succeeded\n",
    "\n",
    "#         logger.info(f\"Successfully generated audio for {len(self.raw_audio_results)} out of {len(self.dialogue_segments)} segments.\")\n",
    "#         return True\n",
    "\n",
    "#     def save_debug_segments(self):\n",
    "#         \"\"\"Saves successfully generated individual audio segments to disk.\"\"\"\n",
    "#         if not self.raw_audio_results:\n",
    "#             logger.info(\"No raw audio results to save as debug segments.\")\n",
    "#             return False\n",
    "\n",
    "#         self.debug_segments_path = os.path.join(self.base_save_directory, \"_debug_audio_segments\")\n",
    "#         logger.info(f\"Saving {len(self.raw_audio_results)} individual audio segments for debugging to: {self.debug_segments_path}\")\n",
    "\n",
    "#         try:\n",
    "#             os.makedirs(self.debug_segments_path, exist_ok=True)\n",
    "\n",
    "#             for index, audio_bytes in self.raw_audio_results.items():\n",
    "#                 if not audio_bytes: continue\n",
    "\n",
    "#                 try:\n",
    "#                     character_name = self.dialogue_segments[index][1]\n",
    "#                     safe_char_name = character_name.replace(' ', '_').strip()\n",
    "#                 except (IndexError, TypeError) as e:\n",
    "#                     self._log_error(f\"Error getting character name for debug segment index {index}\", e)\n",
    "#                     safe_char_name = \"UnknownChar\"\n",
    "\n",
    "#                 segment_filename = f\"segment_{index:03d}_{safe_char_name}.mp3\"\n",
    "#                 segment_filepath = os.path.join(self.debug_segments_path, segment_filename)\n",
    "\n",
    "#                 try:\n",
    "#                     with open(segment_filepath, 'wb') as seg_file:\n",
    "#                         seg_file.write(audio_bytes)\n",
    "#                 except IOError as e:\n",
    "#                     self._log_error(f\"Failed to save debug segment {segment_filepath}\", e)\n",
    "\n",
    "#             return True\n",
    "\n",
    "#         except Exception as e:\n",
    "#             self._log_error(f\"Could not create or write to debug directory {self.debug_segments_path}\", e)\n",
    "#             return False\n",
    "\n",
    "\n",
    "#     def merge_audio(self, use_ffmpeg=True):\n",
    "#         \"\"\"Merges the generated audio segments in order.\"\"\"\n",
    "#         if not self.raw_audio_results:\n",
    "#             self._log_error(\"No audio segments available to merge.\")\n",
    "#             return False\n",
    "\n",
    "#         logger.info(\"Preparing to merge audio segments...\")\n",
    "#         self.ordered_audio_bytes = []\n",
    "#         missing_count = 0\n",
    "#         for i in range(len(self.dialogue_segments)):\n",
    "#             audio_segment = self.raw_audio_results.get(i)\n",
    "#             if audio_segment:\n",
    "#                 self.ordered_audio_bytes.append(audio_segment)\n",
    "#             else:\n",
    "#                 missing_count += 1\n",
    "\n",
    "#         if missing_count > 0:\n",
    "#              logger.warning(f\"{missing_count} audio segments were missing or invalid and will be skipped in the final merge.\")\n",
    "\n",
    "#         if not self.ordered_audio_bytes:\n",
    "#             self._log_error(\"No valid audio segments remained after ordering. Cannot merge.\")\n",
    "#             return False\n",
    "\n",
    "#         logger.info(f\"Merging {len(self.ordered_audio_bytes)} audio segments in order...\")\n",
    "#         try:\n",
    "#             # Assumes merge_mp3s function is globally available\n",
    "#             self.merged_audio = merge_mp3s(self.ordered_audio_bytes, use_ffmpeg=use_ffmpeg)\n",
    "#             if self.merged_audio:\n",
    "#                  logger.info(\"Audio segments merged successfully.\")\n",
    "#                  return True\n",
    "#             else:\n",
    "#                  self._log_error(\"Merging process returned no data.\")\n",
    "#                  return False\n",
    "#         except NameError:\n",
    "#             self._log_error(\"merge_mp3s function definition not found. Cannot merge audio.\")\n",
    "#             return False\n",
    "#         except Exception as e:\n",
    "#             self._log_error(\"Error during merging process\", e)\n",
    "#             return False\n",
    "\n",
    "#     def save_final_audio(self):\n",
    "#         \"\"\"Saves the merged audio to the final output file.\"\"\"\n",
    "#         if not self.merged_audio:\n",
    "#             self._log_error(\"No merged audio data available to save.\")\n",
    "#             return None\n",
    "\n",
    "#         output_filename = f\"{self.script_basename}_audio.mp3\"\n",
    "#         self.final_output_path = os.path.join(self.base_save_directory, output_filename)\n",
    "\n",
    "#         logger.info(f\"Saving final merged audio to: {self.final_output_path}\")\n",
    "#         try:\n",
    "#             os.makedirs(self.base_save_directory, exist_ok=True)\n",
    "#             with open(self.final_output_path, 'wb') as f_out:\n",
    "#                 f_out.write(self.merged_audio)\n",
    "#             logger.info(\"Final audio saved successfully.\")\n",
    "#             self._display_audio_if_possible(self.final_output_path)\n",
    "#             return self.final_output_path\n",
    "#         except Exception as e:\n",
    "#             self._log_error(f\"Error saving merged audio file to {self.final_output_path}\", e)\n",
    "#             self.final_output_path = None\n",
    "#             return None\n",
    "\n",
    "#     def _display_audio_if_possible(self, audio_path):\n",
    "#         \"\"\"Try displaying audio in IPython if available.\"\"\"\n",
    "#         try:\n",
    "#             # Check if running in an IPython environment\n",
    "#             shell = get_ipython().__class__.__name__\n",
    "#             if shell == 'ZMQInteractiveShell': # Jupyter notebook or qtconsole\n",
    "#                  from IPython.display import display, Audio\n",
    "#                  display(Audio(audio_path))\n",
    "#             # Add other shell types if needed, e.g., 'TerminalInteractiveShell' for ipython terminal\n",
    "#         except NameError:\n",
    "#              pass # Not in IPython\n",
    "#         except Exception as e:\n",
    "#              logger.debug(f\"Could not display audio in IPython: {e}\")\n",
    "\n",
    "\n",
    "#     def process(self, save_debug=True, merge_with_ffmpeg=True):\n",
    "#         \"\"\"Runs the full audio generation pipeline.\"\"\"\n",
    "#         self.errors = []\n",
    "\n",
    "#         if not self._initialize_tts():\n",
    "#             return None\n",
    "\n",
    "#         if not self.parse_script():\n",
    "#             return None\n",
    "\n",
    "#         # Check if generation failed completely\n",
    "#         if not self.generate_segment_audio() and not self.raw_audio_results:\n",
    "#             logger.error(\"Audio generation failed completely.\")\n",
    "#             return None\n",
    "\n",
    "#         if save_debug:\n",
    "#             self.save_debug_segments()\n",
    "\n",
    "#         if not self.merge_audio(use_ffmpeg=merge_with_ffmpeg):\n",
    "#             logger.error(\"Audio merging failed. Check errors and debug segments if saved.\")\n",
    "#             return None\n",
    "\n",
    "#         final_path = self.save_final_audio()\n",
    "\n",
    "#         if not final_path and self.errors:\n",
    "#              logger.error(\"Processing finished with errors. Final audio not saved.\")\n",
    "#         elif not final_path:\n",
    "#              logger.error(\"Processing finished, but final audio could not be saved.\")\n",
    "#         else:\n",
    "#              logger.info(\"Processing finished successfully.\")\n",
    "\n",
    "#         return final_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d31e9cc-843c-4af2-9aec-7fb37fb5d755",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# script_file = '/Users/jong/Documents/PodcastGPT/tv_show_workspaces/20250330_191704/Seinfeld_hilarious_Seinfeld_episode_abo/final_script_3acts.txt'\n",
    "\n",
    "# # Instantiate the generator\n",
    "# generator = SeinfeldAudioGenerator(script_file) # , output_dir='/custom/output/path' if needed)\n",
    "\n",
    "# # Run the process\n",
    "# final_audio_path = generator.process(save_debug=True, merge_with_ffmpeg=True)\n",
    "\n",
    "# if final_audio_path:\n",
    "#     print(f\"Success! Final audio saved to: {final_audio_path}\")\n",
    "# else:\n",
    "#     print(\"Processing failed or did not complete.\")\n",
    "#     print(\"Check generator state for details:\")\n",
    "#     print(f\"  - Parsed Segments: {len(generator.dialogue_segments)}\")\n",
    "#     print(f\"  - Generated Audio Segments: {len(generator.raw_audio_results)}\")\n",
    "#     if generator.debug_segments_path:\n",
    "#         print(f\"  - Debug Segments saved to: {generator.debug_segments_path}\")\n",
    "#     if generator.ordered_audio_bytes and not generator.merged_audio:\n",
    "#         print(f\"  - Merging failed, but {len(generator.ordered_audio_bytes)} ordered segments exist.\")\n",
    "#     print(f\"  - Errors encountered: {len(generator.errors)}\")\n",
    "#     for i, err in enumerate(generator.errors):\n",
    "#         print(f\"    Error {i+1}: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7453426d-5bfd-479c-9fd4-deba1ddfbcd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
