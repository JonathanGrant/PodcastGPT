{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a139c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "import tempfile\n",
    "import IPython\n",
    "import enum\n",
    "import jonlog\n",
    "import json\n",
    "from gtts import gTTS\n",
    "import uuid\n",
    "import datetime as dt\n",
    "import requests\n",
    "import concurrent.futures\n",
    "import base64\n",
    "from github import Github\n",
    "import time\n",
    "import threading\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import io\n",
    "import retrying\n",
    "import pydub\n",
    "from xml.dom import minidom\n",
    "from xml.etree import ElementTree as ET\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import requests\n",
    "import vertexai\n",
    "import vertexai.preview.generative_models\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage as MistralChatMessage\n",
    "import anthropic\n",
    "import groq\n",
    "import google.generativeai\n",
    "\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    if 'IPKernelApp' in get_ipython().config:\n",
    "        from tqdm.notebook import tqdm\n",
    "    else:\n",
    "        from tqdm import tqdm\n",
    "except:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "logger = jonlog.getLogger()\n",
    "openai.api_key = os.environ.get(\"OPENAI_KEY\", None) or open('/Users/jong/.openai_key').read().strip()\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = 'summer2023-392312'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "937ac4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimited:\n",
    "    def __init__(self, max_per_minute):\n",
    "        self.max_per_minute = max_per_minute\n",
    "        self.current_minute = time.strftime('%M')\n",
    "        self.lock = threading.Lock()\n",
    "        self.calls = 0\n",
    "\n",
    "    def __call__(self, fn):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            run = False\n",
    "            with self.lock:\n",
    "                current_minute = time.strftime('%M')\n",
    "                if current_minute != self.current_minute:\n",
    "                    self.current_minute = current_minute\n",
    "                    self.calls = 0\n",
    "                if self.calls < self.max_per_minute:\n",
    "                    self.calls += 1\n",
    "                    run = True\n",
    "            if run:\n",
    "                return fn(*args, **kwargs)\n",
    "            else:\n",
    "                time.sleep(15)\n",
    "                return wrapper(*args, **kwargs)\n",
    "                    \n",
    "        return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6256344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElevenLabsTTS:\n",
    "    WOMAN = 'EXAVITQu4vr4xnSDxMaL'\n",
    "    MAN = 'VR6AewLTigWG4xSOukaG'\n",
    "    BRIT_WOMAN = 'jnBYJClnH7m3ddnEXkeh'\n",
    "    def __init__(self, voice_id=None):\n",
    "        api_key_fpath='/Users/jong/.elevenlabs_apikey'\n",
    "        with open(api_key_fpath) as f:\n",
    "            self.api_key = f.read().strip()\n",
    "        self._voice_id = voice_id or self.WOMAN\n",
    "        self.uri = \"https://api.elevenlabs.io/v1/text-to-speech/\" + self._voice_id\n",
    "        \n",
    "    @retrying.retry(stop_max_attempt_number=5, wait_fixed=2000)\n",
    "    def tts(self, text):\n",
    "        headers = {\n",
    "            \"accept\": \"audio/mpeg\",\n",
    "            \"xi-api-key\": self.api_key,\n",
    "        }\n",
    "        payload = {\n",
    "            \"text\": text,\n",
    "        }\n",
    "        return requests.post(self.uri, headers=headers, json=payload).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26f11de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GttsTTS:\n",
    "    WOMAN = 'us'\n",
    "    MAN   = 'co.in'\n",
    "    def __init__(self, voice_id=None):\n",
    "        self.tld = voice_id\n",
    "\n",
    "    @retrying.retry(stop_max_attempt_number=5, wait_fixed=2000)\n",
    "    def tts(self, text):\n",
    "        speech = gTTS(text=text, lang='en', tld=self.tld, slow=False)\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            temp_filename = f'{tmpdir}/audio'\n",
    "            speech.save(temp_filename)\n",
    "            with open(temp_filename, 'rb') as f:\n",
    "                return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e752b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAITTS:\n",
    "    \"\"\"https://platform.openai.com/docs/guides/text-to-speech\"\"\"\n",
    "    WOMAN = 'nova'\n",
    "    MAN = 'echo'\n",
    "    def __init__(self, voice_id=None, model='tts-1'):\n",
    "        \"\"\"Voices:\n",
    "        alloy, echo, fable, onyx, nova, and shimmer\n",
    "        Models:\n",
    "        tts-1, tts-1-hd\n",
    "        \"\"\"\n",
    "        self.voice = voice_id\n",
    "        self.model = model\n",
    "\n",
    "    @RateLimited(95)\n",
    "    @jonlog.retry_with_logging()\n",
    "    def tts(self, text):\n",
    "        response = openai.OpenAI(api_key=openai.api_key).audio.speech.create(\n",
    "          model=self.model,\n",
    "          voice=self.voice,\n",
    "          input=text\n",
    "        )\n",
    "        return response.content\n",
    "\n",
    "    def tostring(self): return f\"[OpenAITTS] {self.voice=} {self.model=}\"\n",
    "    def __repr__(self): return self.tostring()\n",
    "    def __str__(self): return self.tostring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e9911f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWSPollyTTS:\n",
    "    WOMAN = 'Kimberly'\n",
    "    MAN = 'Matthew'\n",
    "    BRIT_WOMAN = 'Amy'\n",
    "\n",
    "    def __init__(self, voice_id=None):\n",
    "        self.client = boto3.client('polly', region_name=\"us-east-1\")\n",
    "        self._voice_id = voice_id or self.WOMAN\n",
    "\n",
    "    @RateLimited(95)\n",
    "    @jonlog.retry_with_logging()\n",
    "    def tts(self, text, ssml=False):\n",
    "        kwargs = {}\n",
    "        if ssml:\n",
    "            kwargs['TextType'] = 'ssml'\n",
    "        response = self.client.synthesize_speech(\n",
    "            Text=text,\n",
    "            OutputFormat='mp3',\n",
    "            VoiceId=self._voice_id,\n",
    "            Engine=\"neural\",\n",
    "            **kwargs,\n",
    "        )\n",
    "        # The audio stream containing the synthesized speech\n",
    "        audio_stream = response.get('AudioStream')\n",
    "        return audio_stream.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8cffd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import texttospeech_v1 as texttospeech\n",
    "\n",
    "class GoogleTTS:\n",
    "    WOMAN = 'en-US-Wavenet-F'\n",
    "    MAN = 'en-US-Wavenet-D'\n",
    "    BRIT_WOMAN = 'en-GB-Wavenet-A'\n",
    "\n",
    "    def __init__(self, voice_name=None):\n",
    "        self.client = texttospeech.TextToSpeechClient()\n",
    "        self._voice_name = voice_name or self.WOMAN\n",
    "\n",
    "    # Assuming the RateLimited and retry_with_logging decorators are defined elsewhere\n",
    "    @RateLimited(95)\n",
    "    @jonlog.retry_with_logging()\n",
    "    def tts(self, text):\n",
    "        try:\n",
    "            synthesis_input = texttospeech.SynthesisInput(text=text)\n",
    "            voice_params = texttospeech.VoiceSelectionParams(\n",
    "                language_code=self._voice_name[:5],  # Extracts the language code from the voice name\n",
    "                name=self._voice_name,\n",
    "                # ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL\n",
    "            )\n",
    "            audio_config = texttospeech.AudioConfig(\n",
    "                audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "            )\n",
    "            response = self.client.synthesize_speech(\n",
    "                input=synthesis_input,\n",
    "                voice=voice_params,\n",
    "                audio_config=audio_config\n",
    "            )\n",
    "        except:\n",
    "            logger.critical(f\"GoogleTTS error from {text=}\")\n",
    "            raise\n",
    "        return response.audio_content\n",
    "\n",
    "    @classmethod\n",
    "    def list_voices(cls):\n",
    "        data = texttospeech.TextToSpeechClient().list_voices()\n",
    "        voices = [\n",
    "            v.name for v in data.voices\n",
    "            if v.name[:2] == 'en'\n",
    "            and 'studio' not in v.name.lower()\n",
    "            and 'journey' not in v.name.lower()\n",
    "        ]\n",
    "        return voices\n",
    "\n",
    "    def tostring(self): return f\"[GoogleTTS] {self._voice_name=}\"\n",
    "    def __repr__(self): return self.tostring()\n",
    "    def __str__(self): return self.tostring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2653243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_voices(n=2, openai=True, aws=True, google=True):\n",
    "    possible = []\n",
    "    if openai:\n",
    "        possible += [OpenAITTS(voice_id=vid) for vid in ['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']]\n",
    "    if aws:\n",
    "        possible += [AWSPollyTTS(voice_id=vid) for vid in ['Kimberly', 'Matthew', 'Amy']]\n",
    "    if google:\n",
    "        possible += [GoogleTTS(vid) for vid in GoogleTTS.list_voices()]\n",
    "    return random.sample(possible, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0607c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWSChat:\n",
    "    MODELS = {\n",
    "        \"claude-instant\": \"anthropic.claude-instant-v1\",\n",
    "        \"claude-best\": \"anthropic.claude-v2:1\",\n",
    "        \"claude-3-sonnet\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        \"claude-3-haiku\": \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    }\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def consolidate_messages(cls, message_list):\n",
    "        if not message_list:\n",
    "            return []\n",
    "        consolidated = []\n",
    "        current_role = None\n",
    "        current_content = \"\"\n",
    "\n",
    "        for message in message_list:\n",
    "            role = message.get(\"role\")\n",
    "            content = message.get(\"content\", \"\")\n",
    "    \n",
    "            if role == \"system\":\n",
    "                role = \"user\"\n",
    "            if role == current_role:\n",
    "                current_content += \"\\n\" + content\n",
    "            else:\n",
    "                if current_role is not None:\n",
    "                    consolidated.append({\"role\": current_role, \"content\": current_content})\n",
    "                current_content = content\n",
    "                current_role = role\n",
    "    \n",
    "        if current_role is not None:\n",
    "            consolidated.append({\"role\": current_role, \"content\": current_content})\n",
    "    \n",
    "        return consolidated\n",
    "\n",
    "    @classmethod\n",
    "    def msg(cls, messages=None, model=\"anthropic.claude-3-haiku-20240307-v1:0\", **kwargs):\n",
    "        client = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "        try:\n",
    "            # The different model providers have individual request and response formats.\n",
    "            # For the format, ranges, and default values for Anthropic Claude, refer to:\n",
    "            # https://docs.anthropic.com/claude/reference/complete_post\n",
    "\n",
    "            # Claude requires you to enclose the prompt as follows:\n",
    "            # enclosed_prompt = \"Human: \" + prompt + \"\\n\\nAssistant:\"\n",
    "            # prompt = \"\\n\\n\".join(\n",
    "            #     [f'{\"\" if (msg[\"role\"] == \"system\" and model != cls.MODELS[\"claude-instant\"]) else (\"Human\" if msg[\"role\"] != \"assistant\" else \"Assistant\")}: {msg[\"content\"]}' for msg in messages] +\n",
    "            #     [\"Assistant:\"]\n",
    "            # )\n",
    "\n",
    "            if 'temperature' not in kwargs:\n",
    "                kwargs['temperature'] = 1\n",
    "            system = \"\\n\".join([m['content'] for m in messages if m['role'] == 'system'])\n",
    "            other_msgs = cls.consolidate_messages([m for m in messages if m['role'] != 'system'])\n",
    "            \n",
    "            body = {\n",
    "                \"system\": system,\n",
    "                \"messages\": other_msgs,\n",
    "                \"max_tokens\": 2048,\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                **kwargs\n",
    "            }\n",
    "            response = client.invoke_model(\n",
    "                modelId=model, body=json.dumps(body)\n",
    "            )\n",
    "            response_body = json.loads(response[\"body\"].read())\n",
    "            completion = response_body[\"content\"][0][\"text\"]\n",
    "            return completion\n",
    "        except ClientError as e:\n",
    "            logger.exception(f\"Couldn't invoke {model}\", e)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217ee219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleChat:\n",
    "    MODELS = {\n",
    "        \"gemini-pro\": \"gemini-pro\",\n",
    "        \"gemini-1.5-flash\": \"gemini-1.5-flash-latest\",\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_apikey(cls):\n",
    "        return os.environ.get(\"GEMINI_API_KEY\") or open(os.path.expanduser(\"~/.google_apikey\")).read().strip()\n",
    "\n",
    "    @classmethod\n",
    "    def consolidate_messages(cls, message_list):\n",
    "        if not message_list:\n",
    "            return []\n",
    "        consolidated = []\n",
    "        current_role = None\n",
    "        current_content = \"\"\n",
    "\n",
    "        for message in message_list:\n",
    "            role = message.get(\"role\")\n",
    "            content = message.get(\"content\", \"\")\n",
    "    \n",
    "            if role == \"system\":\n",
    "                role = \"user\"\n",
    "            if role == current_role:\n",
    "                current_content += \"\\n\" + content\n",
    "            else:\n",
    "                if current_role is not None:\n",
    "                    consolidated.append({\"role\": current_role, \"content\": current_content})\n",
    "                current_content = content\n",
    "                current_role = role\n",
    "\n",
    "        if current_role is not None:\n",
    "            consolidated.append({\"role\": current_role, \"content\": current_content})\n",
    "    \n",
    "        return consolidated\n",
    "\n",
    "    @classmethod\n",
    "    def msg(cls, messages=None, model_name=\"gemini-1.5-flash-latest\", **kwargs):\n",
    "        google.generativeai.configure(api_key=cls.get_apikey())\n",
    "        \n",
    "        # Create the model configuration\n",
    "        generation_config = {\n",
    "            \"temperature\": 1,\n",
    "            \"top_p\": 0.95,\n",
    "            \"top_k\": 64,\n",
    "            \"max_output_tokens\": 8192,\n",
    "            \"response_mime_type\": \"text/plain\",\n",
    "        }\n",
    "        # generation_config.update(kwargs)\n",
    "        safety_settings = [\n",
    "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "        ]\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = google.generativeai.GenerativeModel(\n",
    "            model_name=model_name,\n",
    "            safety_settings=safety_settings,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "        \n",
    "        # Consolidate messages\n",
    "        consolidated_messages = cls.consolidate_messages(messages)\n",
    "        final_msg = \"Continue\"\n",
    "        if consolidated_messages[-1]['role'] == 'user':\n",
    "            final_msg = consolidated_messages.pop(-1)['content']\n",
    "        # Create chat session history\n",
    "        history = []\n",
    "        for msg in consolidated_messages:\n",
    "            history.append({\n",
    "                \"role\": \"user\" if msg[\"role\"] != \"assistant\" else \"model\",\n",
    "                \"parts\": [msg[\"content\"]]\n",
    "            })\n",
    "        # Start chat session\n",
    "        chat_session = model.start_chat(history=history)\n",
    "        # Send message and get response\n",
    "        response = chat_session.send_message(final_msg)\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab66077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralChat:\n",
    "    MODELS = {\n",
    "        \"mistral-medium\": \"mistral-medium\",\n",
    "        \"mistral-small\": \"mistral-small\",\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_apikey(cls):\n",
    "        return os.environ.get(\"MISTRAL_API_KEY\") or open('/Users/jong/.mistral_apikey').read().strip()\n",
    "\n",
    "    @classmethod\n",
    "    def consolidate_messages(cls, message_list):\n",
    "        if not message_list:\n",
    "            return []\n",
    "    \n",
    "        consolidated = []\n",
    "        current_role = None\n",
    "        current_content = \"\"\n",
    "    \n",
    "        for message in message_list:\n",
    "            role = message[\"role\"]\n",
    "            content = message[\"content\"]\n",
    "\n",
    "            if role == current_role:\n",
    "                current_content += \"\\n\" + content\n",
    "            else:\n",
    "                if current_role is not None:\n",
    "                    consolidated.append({\"role\": current_role, \"content\": current_content})\n",
    "                current_content = content\n",
    "                current_role = role\n",
    "    \n",
    "        if current_role is not None:\n",
    "            consolidated.append({\"role\": current_role, \"content\": current_content})\n",
    "\n",
    "        return consolidated\n",
    "\n",
    "    @classmethod\n",
    "    def msg(cls, messages=None, model=\"mistral-medium\", **kwargs):\n",
    "        client = MistralClient(api_key=cls.get_apikey())\n",
    "        if not any(msg['role'] == 'user' for msg in messages):\n",
    "            messages[-1]['role'] = 'user'\n",
    "        chat_response = client.chat(\n",
    "            model=model,\n",
    "            messages=[MistralChatMessage(**msg) for msg in cls.consolidate_messages(messages)],\n",
    "        )\n",
    "        return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def95e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnthropicChat:\n",
    "    MODELS = {\n",
    "        \"claude-opus\": \"claude-3-opus-20240229\",   # Large\n",
    "        \"claude-sonnet\": \"claude-3-sonnet-20240229\", # Medium\n",
    "        \"claude-haiku\": \"claude-3-haiku-20240307\",  # Small\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_apikey(cls):\n",
    "        return os.environ.get(\"ANTHROPIC_APIKEY\") or open('/Users/jong/.anthropic_apikey').read().strip()\n",
    "\n",
    "    @classmethod\n",
    "    def msg(cls, messages=None, model=\"claude-3-haiku-20240307\", **kwargs):\n",
    "        messages = MistralChat.consolidate_messages(messages)\n",
    "        system = '\\n'.join([msg['content'] for msg in messages if msg['role'] == 'system'])\n",
    "        messages = [m for m in messages if m['role'] != 'system' and m['content']]\n",
    "        if not messages:\n",
    "            messages.append({'role': 'user', 'content': 'Continue.'})\n",
    "        client = anthropic.Anthropic(api_key=cls.get_apikey())\n",
    "        if 'max_tokens' not in kwargs:\n",
    "            kwargs['max_tokens'] = 4096\n",
    "        message = client.messages.create(\n",
    "            model=model,\n",
    "            # max_tokens=4096,\n",
    "            messages=messages,\n",
    "            system=system,\n",
    "            # temperature=0\n",
    "            **kwargs,\n",
    "        ).content[0].text\n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bb8573",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqChat:\n",
    "    MODELS = {\n",
    "        \"llama3\": \"Llama3-70b-8192\",\n",
    "        \"mixtral\": \"Mixtral-8x7b-32768\",\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_apikey(cls):\n",
    "        return os.environ.get(\"GROQ_APIKEY\") or open('/Users/jong/.groq_apikey').read().strip()\n",
    "\n",
    "    @classmethod\n",
    "    def msg(cls, messages=None, model=\"Mixtral-8x7b-32768\", **kwargs):\n",
    "        messages = MistralChat.consolidate_messages(messages)\n",
    "        system = '\\n'.join([msg['content'] for msg in messages if msg['role'] == 'system'])\n",
    "        messages = [m for m in messages if m['role'] != 'system' and m['content']]\n",
    "        if not messages:\n",
    "            messages.append({'role': 'user', 'content': 'Continue.'})\n",
    "        messages = [{'role': 'system', 'content': system}] + messages\n",
    "\n",
    "        client = groq.Groq(api_key=cls.get_apikey())\n",
    "        message = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=model,\n",
    "        ).choices[0].message.content\n",
    "        \n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3045d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TogetherChat:\n",
    "    MODELS = {\n",
    "        \"mixtral\": \"mistralai/Mixtral-8x22B-Instruct-v0.1\",\n",
    "        \"qwen\": \"Qwen/Qwen1.5-110B-Chat\",\n",
    "        \"databricks\": \"databricks/dbrx-instruct\",\n",
    "        \"dolphin\": \"cognitivecomputations/dolphin-2.5-mixtral-8x7b\",\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_apikey(cls):\n",
    "        return os.environ.get(\"TOGETHER_API_KEY\") or open(os.path.expanduser(\"~/.together_apikey\")).read().strip()\n",
    "\n",
    "    @classmethod\n",
    "    def msg(cls, messages=None, model=None, **kwargs):\n",
    "        if model is None:\n",
    "            model = random.choice(cls.MODELS.values())\n",
    "        # Prepare the messages for the API\n",
    "        consolidated_messages = GoogleChat.consolidate_messages(messages)\n",
    "        # Prepare the messages for the API\n",
    "        api_messages = [{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in consolidated_messages]\n",
    "        # Define the payload\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": api_messages,\n",
    "            \"temperature\": kwargs.get(\"temperature\", 0.8),\n",
    "            \"max_tokens\": kwargs.get(\"max_tokens\", 8000)\n",
    "        }\n",
    "        # Make the request to Together API\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {cls.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        response = requests.post(\"https://api.together.xyz/v1/chat/completions\", headers=headers, json=payload)\n",
    "        # Parse the response\n",
    "        response_data = response.json()\n",
    "        return response_data[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82247138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TogetherChat.msg([{'role': 'system', 'content': 'Give me a list of insane commercial genres'}])\n",
    "# !pip install -U together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fdfbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT_MODEL = 'gpt-4-1106-preview'\n",
    "# DEFAULT_LENGTH  = 80_000\n",
    "DEFAULT_MODEL = 'gpt-3.5-turbo'\n",
    "DEFAULT_LENGTH  = 29_000\n",
    "\n",
    "class Chat:\n",
    "    class Model(enum.Enum):\n",
    "        GPT3_5 = \"gpt-3.5-turbo\"\n",
    "        GPT_4  = \"gpt-4-turbo-preview\"\n",
    "\n",
    "    def __init__(self, system, max_length=DEFAULT_LENGTH, default_model=None, messages=None):\n",
    "        self._system = system\n",
    "        self._max_length = max_length\n",
    "        self._default_model = default_model\n",
    "        self._history = [\n",
    "            {\"role\": \"system\", \"content\": self._system},\n",
    "        ]\n",
    "        if messages:\n",
    "            self._history += messages\n",
    "\n",
    "    @classmethod\n",
    "    def num_tokens_from_text(cls, text, model=DEFAULT_MODEL):\n",
    "        \"\"\"Returns the number of tokens used by some text.\"\"\"\n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(model)\n",
    "        except:\n",
    "            encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')  # Lol openai probably the same\n",
    "        return len(encoding.encode(text))\n",
    "    \n",
    "    @classmethod\n",
    "    def num_tokens_from_messages(cls, messages, model=DEFAULT_MODEL):\n",
    "        \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(model)\n",
    "        except:\n",
    "            encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')  # Lol openai probably the same\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":  # if there's a name, the role is omitted\n",
    "                    num_tokens += -1  # role is always required and always 1 token\n",
    "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "        return num_tokens\n",
    "\n",
    "    @retrying.retry(stop_max_attempt_number=5, wait_fixed=2000)\n",
    "    def _msg(self, *args, model=None, **kwargs):\n",
    "        if model is None:\n",
    "            if self._default_model is not None: model = self._default_model \n",
    "            else: model = DEFAULT_MODEL\n",
    "        logger.info(f'requesting chatcompletion {model=}...')\n",
    "        if model.startswith(\"AWS/\"):\n",
    "            model = model[4:]\n",
    "            resp = AWSChat.msg(\n",
    "                messages=self._history,\n",
    "                **kwargs\n",
    "            )\n",
    "        elif model.startswith(\"GOOGLE/\"):\n",
    "            model = model[7:]\n",
    "            resp = GoogleChat.msg(messages=self._history, model=model, **kwargs)\n",
    "        elif model.startswith(\"MISTRAL/\"):\n",
    "            model = model[8:]\n",
    "            resp = MistralChat.msg(messages=self._history, model=model, **kwargs)\n",
    "        elif model.startswith(\"ANTHROPIC/\"):\n",
    "            model = model[10:]\n",
    "            resp = AnthropicChat.msg(messages=self._history, model=model, **kwargs)\n",
    "        elif model.startswith(\"GROQ/\"):\n",
    "            model = model[5:]\n",
    "            resp = GroqChat.msg(messages=self._history, model=model, **kwargs)\n",
    "        elif model.startswith(\"TOGETHER/\"):\n",
    "            model = model[9:]\n",
    "            resp = TogetherChat.msg(messages=self._history, model=model, **kwargs)\n",
    "        else:\n",
    "            resp = openai.OpenAI(api_key=openai.api_key).chat.completions.create(\n",
    "                *args,\n",
    "                model=model,\n",
    "                messages=self._history,\n",
    "                **kwargs\n",
    "            ).choices[0].message.content\n",
    "        logger.info(f'received chatcompletion {model=}...')\n",
    "        return resp\n",
    "    \n",
    "    def message(self, next_msg=None, **kwargs):\n",
    "        # TODO: Optimize this if slow through easy caching\n",
    "        while len(self._history) > 1 and self.num_tokens_from_messages(self._history) > self._max_length:\n",
    "            logger.info(f'Popping message: {self._history.pop(1)}')\n",
    "        if next_msg is not None:\n",
    "            self._history.append({\"role\": \"user\", \"content\": next_msg})\n",
    "        logger.info(f'Currently at {self.num_tokens_from_messages(self._history)=} tokens in conversation')\n",
    "        resp = self._msg(**kwargs)\n",
    "        text = resp\n",
    "        self._history.append({\"role\": \"assistant\", \"content\": text})\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd8392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PodcastChat(Chat):\n",
    "    def __init__(self, topic, podcast=\"award winning\", max_length=DEFAULT_LENGTH, hosts=['Tom', 'Jen'], host_voices=[AWSPollyTTS(AWSPollyTTS.MAN), OpenAITTS(OpenAITTS.WOMAN)], extra_system=None, system=None):\n",
    "        if system is None:\n",
    "            system = f\"\"\"You are an {podcast} podcast with hosts {hosts[0]} and {hosts[1]}.\n",
    "Respond with the hosts names before each line like {hosts[0]}: and {hosts[1]}:\"\"\".replace(\"\\n\", \" \")\n",
    "        if extra_system is not None:\n",
    "            system = '\\n'.join([system, extra_system])\n",
    "        super().__init__(system, max_length=max_length)\n",
    "        self._podcast = podcast\n",
    "        self._topic = topic\n",
    "        self._hosts = hosts\n",
    "        self._history.append({\n",
    "            \"role\": \"user\", \"content\": f\"\"\"Generate an informative, entertaining, and very detailed podcast episode about {topic}.\n",
    "Make sure to teach complex topics in an intuitive way.\"\"\".replace(\"\\n\", \" \")\n",
    "        })\n",
    "        self._tts_h1, self._tts_h2 = host_voices\n",
    "\n",
    "    def text2speech(self, text, spacing_ms=350):\n",
    "        tmpdir = '/tmp'\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as thread_pool:\n",
    "            i = 0\n",
    "            jobs = []\n",
    "            def write_audio(msg, i, voice, **kwargs):\n",
    "                logger.info(f'requesting tts {i=} {voice=}')\n",
    "                s = voice.tts(msg)\n",
    "                logger.info(f'received tts {i=} {voice=}')\n",
    "                return s\n",
    "\n",
    "            text = text.replace('\\n', '!!!LINEBREAK!!!').replace('\\\\', '').replace('\"', '')\n",
    "            # Build text one at a time\n",
    "            currline, currname = \"\", self._hosts[0]\n",
    "            name2tld = {self._hosts[0]: 'co.uk', self._hosts[1]: 'com'}\n",
    "            name2voice = {self._hosts[0]: self._tts_h1, self._hosts[1]: self._tts_h2}\n",
    "            audios = []\n",
    "            for line in text.split(\"!!!LINEBREAK!!!\"):\n",
    "                if not line.strip(): continue\n",
    "                if line.startswith(f\"{self._hosts[0]}: \") or line.startswith(f\"{self._hosts[1]}: \"):\n",
    "                    if currline:\n",
    "                        jobs.append(thread_pool.submit(write_audio, currline, i, name2voice[currname], lang='en', tld=name2tld[currname]))\n",
    "                        i += 1\n",
    "                    currline = line[4:]\n",
    "                    currname = line[:3]\n",
    "                else:\n",
    "                    currline += line\n",
    "            if currline:\n",
    "                jobs.append(thread_pool.submit(write_audio, currline, i, name2voice[currname], lang='en', tld=name2tld[currname]))\n",
    "                i+=1\n",
    "            # Concat files\n",
    "            audios = [job.result() for job in jobs]\n",
    "            logger.info('concatting audio')\n",
    "            audio = merge_mp3s(audios)\n",
    "            logger.info('done with audio!')\n",
    "            IPython.display.display(IPython.display.Audio(audio, autoplay=False))\n",
    "            return audio\n",
    "            \n",
    "    def step(self, msg=None, skip_aud=False, ret_aud=True, min_length=None, **kwargs):\n",
    "        msg = self.message(msg, **kwargs)\n",
    "        if min_length is not None and len(msg) < min_length:\n",
    "            raise ValueError(f\"Message [{msg}] is shorter than {min_length=}\")\n",
    "        if skip_aud: return msg\n",
    "        aud = self.text2speech(msg)\n",
    "        if ret_aud: return msg, aud\n",
    "        return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50838834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PodcastXMLHandler:\n",
    "    def __init__(self):\n",
    "        self.root = ET.Element(\"channel\")  # 'channel' is typically used in podcast RSS feeds\n",
    "        self.tree = ET.ElementTree(self.root)\n",
    "\n",
    "    def to_xml(self, filepath):\n",
    "        self.tree.write(filepath, encoding='utf-8', xml_declaration=True, pretty_print=True)\n",
    "\n",
    "    @classmethod\n",
    "    def from_xml(cls, filepath):\n",
    "        self = cls()\n",
    "        self.tree = ET.parse(filepath)\n",
    "        self.root = self.tree.getroot()\n",
    "        return self\n",
    "\n",
    "    def contains_episode(self, episode_name):\n",
    "        for episode in self.root.findall('./channel/item'):\n",
    "            title = episode.find('title').text\n",
    "            if title == episode_name:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def remove_episodes_older_than(self, limit):\n",
    "        now = datetime.datetime.now()\n",
    "        parent_map = {c:p for p in self.root.iter() for c in p}\n",
    "        for episode in self.root.findall('./channel/item'):\n",
    "            pub_date = datetime.datetime.strptime(episode.find('pubDate').text, '%a, %d %b %Y %H:%M:%S %Z')  # RSS date format\n",
    "            if now - pub_date > limit:\n",
    "                parent_map[episode].remove(episode)\n",
    "\n",
    "    def add_episode(self, episode_details):\n",
    "        episode = ET.SubElement(self.root, './channel/item')\n",
    "        for key, value in episode_details.items():\n",
    "            ET.SubElement(episode, key).text = str(value)\n",
    "\n",
    "\"\"\"\n",
    "pd = PodcastXMLHandler.from_xml('/Users/jong/Downloads/podcast.xml')\n",
    "pd.contains_episode('cs.IR: Recent Research Papers on Data Science and Cybersecurity.')\n",
    "pd.remove_episodes_older_than(datetime.timedelta(days=30))\n",
    "pd.to_xml('/Users/jong/Downloads/podcast2.xml')\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d798f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PodcastRSSFeed:\n",
    "    \"\"\"Class to handle rss feed operations using github pages.\"\"\"\n",
    "\n",
    "    def __init__(self, org, repo, xml_path, clean_timedelta=None):\n",
    "        self.org = org\n",
    "        self.repo = repo\n",
    "        self.xml_path = xml_path\n",
    "        self.local_xml_path = self.download_podcast_xml()\n",
    "        self.clean_timedelta = clean_timedelta\n",
    "\n",
    "    def get_file_base64(self, file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            return base64.b64encode(file.read()).decode('utf-8')\n",
    "\n",
    "    def download_podcast_xml(self):\n",
    "        outfile = tempfile.NamedTemporaryFile().name + '.xml'\n",
    "        raw_url = f'https://raw.githubusercontent.com/{self.org}/{self.repo}/main/{self.xml_path}'\n",
    "        response = requests.get(raw_url)\n",
    "        print(raw_url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response.text)\n",
    "        with open(outfile, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        return outfile\n",
    "\n",
    "    def update_podcast_xml(self, xml_data, file_name, episode_title, episode_description, file_length):\n",
    "        # Parse XML\n",
    "        root = ET.fromstring(xml_data)\n",
    "        channel = root.find('channel')\n",
    "\n",
    "        file_extension = os.path.splitext(file_name)[-1].lower()[1:]\n",
    "        content_type = 'audio/' + file_extension\n",
    "        \n",
    "        # Add new episode\n",
    "        item = ET.SubElement(channel, 'item')\n",
    "        ET.SubElement(item, 'title').text = episode_title\n",
    "        ET.SubElement(item, 'description').text = episode_description\n",
    "        ET.SubElement(item, 'pubDate').text = dt.datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT')\n",
    "        ET.SubElement(item, 'enclosure', {\n",
    "            'url': f'https://{self.org}.github.io/{file_name}',\n",
    "            'type': content_type,\n",
    "            'length': str(file_length),\n",
    "        })\n",
    "        ET.SubElement(item, 'guid').text = str(uuid.uuid4())\n",
    "\n",
    "        # Convert back to string and pretty-format\n",
    "        pretty_xml = minidom.parseString(ET.tostring(root)).toprettyxml(indent='  ')\n",
    "        # Remove extra newlines\n",
    "        pretty_xml = os.linesep.join([s for s in pretty_xml.splitlines() if s.strip()])\n",
    "\n",
    "        return pretty_xml\n",
    "\n",
    "    def remove_episodes_older_than(self, xml_data, limit):\n",
    "        now = dt.datetime.now()\n",
    "        root = ET.fromstring(xml_data)\n",
    "        parent_map = {c:p for p in root.iter() for c in p}\n",
    "        token = os.environ.get(\"GH_KEY\", None) or open(\"/Users/jong/.gh_token\").read().strip()\n",
    "        gh = Github(token)\n",
    "        made_changes = False\n",
    "        for episode in root.findall('./channel/item'):\n",
    "            pub_date = dt.datetime.strptime(episode.find('pubDate').text, '%a, %d %b %Y %H:%M:%S %Z')  # RSS date format\n",
    "            if now - pub_date > limit:\n",
    "                episode_path = episode.find('enclosure').attrib['url'].split('.github.io/', 1)[1]\n",
    "                logger.info(f\"Deleting old episode: {episode_path}\")\n",
    "                parent_map[episode].remove(episode)\n",
    "                made_changes = True\n",
    "                # Get the repository\n",
    "                try:\n",
    "                    repo = gh.get_user().get_repo(self.repo)\n",
    "                except:\n",
    "                    repo = gh.get_organization(self.org).get_repo(self.repo)\n",
    "                try:\n",
    "                    contents = repo.get_contents(episode_path)\n",
    "                    repo.delete_file(episode_path, \"remove due to date\", contents.sha)\n",
    "                except Exception as e:\n",
    "                    logger.exception(e)\n",
    "        # Convert back to string and pretty-format\n",
    "        pretty_xml = minidom.parseString(ET.tostring(root)).toprettyxml(indent='  ')\n",
    "        # Remove extra newlines\n",
    "        pretty_xml = os.linesep.join([s for s in pretty_xml.splitlines() if s.strip()])\n",
    "        # Upload\n",
    "        if made_changes:\n",
    "            try:\n",
    "                podcast_xml_sha = repo.get_contents(self.xml_path).sha\n",
    "                self.upload_to_github(self.xml_path, pretty_xml, f'Delete old episodes in podcast.xml', podcast_xml_sha)\n",
    "            except Exception as e:\n",
    "                logger.exception(e)\n",
    "        return pretty_xml\n",
    "    \n",
    "    def upload_episode(self, file_path, file_name, episode_title, episode_description):\n",
    "        # Authenticate with GitHub\n",
    "        token = os.environ.get(\"GH_KEY\", None) or open(\"/Users/jong/.gh_token\").read().strip()\n",
    "        gh = Github(token)\n",
    "\n",
    "        # Get the repository\n",
    "        try:\n",
    "            repo = gh.get_user().get_repo(self.repo)\n",
    "        except:\n",
    "            repo = gh.get_organization(self.org).get_repo(self.repo)\n",
    "\n",
    "        # Upload the audio file\n",
    "        podsha = None\n",
    "        try:\n",
    "            podsha = repo.get_contents(file_name).sha\n",
    "        except:\n",
    "            pass\n",
    "        with open(file_path, 'rb') as audio_file:\n",
    "            audio_data = audio_file.read()\n",
    "            self.upload_to_github(file_name, audio_data, f'Upload new episode: {file_name}', podsha)\n",
    "\n",
    "        # Update and upload the podcast.xml file\n",
    "        file_length = os.path.getsize(file_path)\n",
    "        podcast_xml = repo.get_contents(self.xml_path)\n",
    "        xml_data = base64.b64decode(podcast_xml.content).decode('utf-8')\n",
    "        xml_data = self.update_podcast_xml(xml_data, file_name, episode_title, episode_description, file_length)\n",
    "        self.upload_to_github(self.xml_path, xml_data, f'Update podcast.xml with new episode: {file_name}', podcast_xml.sha)\n",
    "\n",
    "    def upload_to_github(self, file_name, file_content, commit_message, sha=None):\n",
    "        # Prepare API request headers\n",
    "        token = os.environ.get(\"GH_KEY\", None) or open(\"/Users/jong/.gh_token\").read().strip()\n",
    "        gh = Github(token)\n",
    "        # Get the repository\n",
    "        try:\n",
    "            repo = gh.get_user().get_repo(self.repo)\n",
    "        except:\n",
    "            repo = gh.get_organization(self.org).get_repo(self.repo)\n",
    "\n",
    "        if sha:\n",
    "            repo.update_file(file_name, commit_message, file_content, sha)\n",
    "        else:\n",
    "            repo.create_file(file_name, commit_message, file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d67676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episode:\n",
    "    def __init__(self, episode_type='narration', podcast_args=(\"JonathanGrant\", \"jonathangrant.github.io\", \"podcasts/podcast.xml\"), text_model=DEFAULT_MODEL, **chat_kwargs):\n",
    "        \"\"\"\n",
    "        Kinds of episodes:\n",
    "            pure narration - simple TTS\n",
    "            simple podcast - Text to Podcast\n",
    "            complex podcast?\n",
    "        \"\"\"\n",
    "        self.episode_type = episode_type\n",
    "        self.chat = PodcastChat(**chat_kwargs)\n",
    "        self.chat_kwargs = chat_kwargs\n",
    "        self.pod = PodcastRSSFeed(*podcast_args)\n",
    "        self.text_model = text_model\n",
    "        self.sounds = []\n",
    "        self.texts = []\n",
    "\n",
    "    def get_outline(self, n, topic=None):\n",
    "        if topic is None: topic = self.chat._topic\n",
    "        chat = Chat(f\"\"\"Write \n",
    "a concise plaintext outline with exactly {n} parts for a podcast titled {self.chat._podcast}.\n",
    "Only return the parts and nothing else.\n",
    "Do not include a conclusion or intro.\n",
    "Do not write more than {n} parts.\n",
    "Format it like this: 1. insert-title-here, 2. another-title-here, ...\"\"\".replace(\"\\n\", \" \"))\n",
    "        resp = chat.message(model=self.text_model)\n",
    "        chapter_pattern = re.compile(r'\\d+\\.\\s+.*')\n",
    "        chapters = chapter_pattern.findall(resp)\n",
    "        if not chapters:\n",
    "            logger.warning(f'Could not parse message for chapters! Message:\\n{resp}')\n",
    "        return chapters\n",
    "\n",
    "    def step(self, msg=None, nparts=3):\n",
    "        include = f\" Remember to respond with the hosts names like {self.chat._hosts[0]}: and {self.chat._hosts[1]}:\"\n",
    "        msg = msg or self.chat._topic\n",
    "        if self.episode_type == 'narration':\n",
    "            outline = self.get_outline(msg, nparts)\n",
    "            logger.info(f\"Outline: {outline}\")\n",
    "            intro_txt, intro_aud = self.chat.step(f\"Write the intro for a podcast about {msg}. The outline for the podcast is {', '.join(outline)}. Only write the introduction.{include}\", model=self.text_model)\n",
    "            self.sounds.append(intro_aud)\n",
    "            self.texts.append(intro_txt)\n",
    "            # Get parts\n",
    "            for part in outline:\n",
    "                logger.info(f\"Part: {part}\")\n",
    "                part_txt, part_aud = self.chat.step(f\"Write the next part: {part}.{include}\", model=self.text_model)\n",
    "                self.sounds.append(part_aud)\n",
    "                self.texts.append(part_txt)\n",
    "            # Get conclusion\n",
    "            logger.info(\"Conclusion\")\n",
    "            part_txt, part_aud = self.chat.step(f\"Write the conclusion. Remember, the outline was: {', '.join(outline)}.{include}\", model=self.text_model)\n",
    "            self.sounds.append(part_aud)\n",
    "            self.texts.append(part_txt)\n",
    "        elif self.episode_type == 'pure_tts':\n",
    "            outline = None\n",
    "            audio = self.chat.text2speech(\"\\n\".join([self.chat._hosts[i%2]+\": \"+x for i,x in enumerate(msg)]))\n",
    "            self.sounds.append(audio)\n",
    "            self.texts.extend(msg)\n",
    "        return outline, '\\n'.join(self.texts)\n",
    "\n",
    "    def upload(self, title, descr):\n",
    "        title_small = title.lower().replace(\" \", \"_\")[:16] + str(uuid.uuid4())  # I had a filename too long once\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            tmppath = os.path.join(tmpdir, \"audio_file.mp3\")\n",
    "            with open(tmppath, \"wb\") as f:\n",
    "                f.write(merge_mp3s(self.sounds))\n",
    "            self.pod.upload_episode(tmppath, f\"podcasts/audio/{title_small}.mp3\", title, descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50026592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_mp3s(mp3_bytes_list):\n",
    "    \"\"\"\n",
    "    Merges multiple MP3 bytestrings into a single MP3 bytestring.\n",
    "    \n",
    "    :param mp3_bytes_list: List of MP3 bytestrings\n",
    "    :return: Merged MP3 as bytestring\n",
    "    \"\"\"\n",
    "    # Convert the first MP3 bytestring to an AudioSegment\n",
    "    combined = pydub.AudioSegment.from_file(io.BytesIO(mp3_bytes_list[0]), format=\"mp3\")\n",
    "\n",
    "    # Loop through the rest of the MP3 bytestrings and append them\n",
    "    for mp3_bytes in mp3_bytes_list[1:]:\n",
    "        next_segment = pydub.AudioSegment.from_file(io.BytesIO(mp3_bytes), format=\"mp3\")\n",
    "        combined += next_segment\n",
    "\n",
    "    # Export the combined audio to a bytestring\n",
    "    combined_buffer = io.BytesIO()\n",
    "    combined.export(combined_buffer, format=\"mp3\")\n",
    "    return combined_buffer.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c22711",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIChain:\n",
    "    @classmethod\n",
    "    def serial_message(cls, chat=None, msgs=None, systems=None):\n",
    "        responses = []\n",
    "        systems = systems or [None] * len(msgs)\n",
    "        for msg, sys in tqdm(zip(msgs, systems), total=len(msgs), desc=\"Processing serial chat messages.\", unit=\"message\"):\n",
    "            if sys is not None:\n",
    "                chat._history[0]['content'] = sys\n",
    "            responses.append(chat.message(next_msg=msg))\n",
    "        return responses\n",
    "\n",
    "    @classmethod\n",
    "    def parallel_message(cls, chats=None, msgs=None, max_workers=8):\n",
    "        assert len(chats) == len(msgs), \"Lengths of chats and msgs must match.\"\n",
    "        responses = [None] * len(chats)\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as tpe:\n",
    "            futures2idx = {tpe.submit(chat.message, next_msg=msg): i for i, chat, msg in zip(range(9**99), chats, msgs)}\n",
    "            for future in tqdm(concurrent.futures.as_completed(futures2idx.keys()), total=len(futures2idx), desc=\"Processing parallel chats\", unit=\"chat\"):\n",
    "                responses[futures2idx[future]] = future.result()\n",
    "        return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6726fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ep = Episode(\n",
    "#     episode_type='narration',\n",
    "#     topic=\"Hidden History: Unraveling 3 of History's Funniest Mysteries from the 1st Century\",\n",
    "#     max_length=29_000,\n",
    "#     # text_model='gpt-4-1106-preview',\n",
    "#     text_model='GOOGLE/gemini-pro',\n",
    "# )\n",
    "# outline, txt = ep.step(nparts='3')\n",
    "# ep.upload(\"[Google Gemini] Hidden History: Unraveling 3 of History's Funniest Mysteries from the 1st Century\", \"Hidden History: Unraveling 3 of History's Funniest Mysteries from the 1st Century\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf7e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b7ad2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
