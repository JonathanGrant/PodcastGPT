{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "251565c8-29cd-4315-bafb-630fe4cbbe51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "import tempfile\n",
    "import IPython\n",
    "import enum\n",
    "import jonlog\n",
    "import json\n",
    "from gtts import gTTS\n",
    "import uuid\n",
    "import datetime as dt\n",
    "import requests\n",
    "import concurrent.futures\n",
    "import base64\n",
    "from github import Github\n",
    "import time\n",
    "import threading\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import retrying\n",
    "import pydub\n",
    "from xml.dom import minidom\n",
    "from xml.etree import ElementTree as ET\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "# import vertexai\n",
    "# import vertexai.preview.generative_models\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage as MistralChatMessage\n",
    "logger = jonlog.getLogger()\n",
    "openai.api_key = os.environ.get(\"OPENAI_KEY\", None) or open('/Users/jong/.openai_key').read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "526459ff-9ad2-4e56-b11d-dc522fe319b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimited:\n",
    "    def __init__(self, max_per_minute):\n",
    "        self.max_per_minute = max_per_minute\n",
    "        self.current_minute = time.strftime('%M')\n",
    "        self.lock = threading.Lock()\n",
    "        self.calls = 0\n",
    "\n",
    "    def __call__(self, fn):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            run = False\n",
    "            with self.lock:\n",
    "                current_minute = time.strftime('%M')\n",
    "                if current_minute != self.current_minute:\n",
    "                    self.current_minute = current_minute\n",
    "                    self.calls = 0\n",
    "                if self.calls < self.max_per_minute:\n",
    "                    self.calls += 1\n",
    "                    run = True\n",
    "            if run:\n",
    "                return fn(*args, **kwargs)\n",
    "            else:\n",
    "                time.sleep(15)\n",
    "                return wrapper(*args, **kwargs)\n",
    "                    \n",
    "        return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2e5b9a9-23d5-429c-8e86-32b3d012b9d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ElevenLabsTTS:\n",
    "    WOMAN = 'EXAVITQu4vr4xnSDxMaL'\n",
    "    MAN = 'VR6AewLTigWG4xSOukaG'\n",
    "    BRIT_WOMAN = 'jnBYJClnH7m3ddnEXkeh'\n",
    "    def __init__(self, voice_id=None):\n",
    "        api_key_fpath='/Users/jong/.elevenlabs_apikey'\n",
    "        with open(api_key_fpath) as f:\n",
    "            self.api_key = f.read().strip()\n",
    "        self._voice_id = voice_id or self.WOMAN\n",
    "        self.uri = \"https://api.elevenlabs.io/v1/text-to-speech/\" + self._voice_id\n",
    "        \n",
    "    @retrying.retry(stop_max_attempt_number=5, wait_fixed=2000)\n",
    "    def tts(self, text):\n",
    "        headers = {\n",
    "            \"accept\": \"audio/mpeg\",\n",
    "            \"xi-api-key\": self.api_key,\n",
    "        }\n",
    "        payload = {\n",
    "            \"text\": text,\n",
    "        }\n",
    "        return requests.post(self.uri, headers=headers, json=payload).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "507cf5f9-cc95-44be-afb2-8c09b2a060d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GttsTTS:\n",
    "    WOMAN = 'us'\n",
    "    MAN   = 'co.in'\n",
    "    def __init__(self, voice_id=None):\n",
    "        self.tld = voice_id\n",
    "\n",
    "    @retrying.retry(stop_max_attempt_number=5, wait_fixed=2000)\n",
    "    def tts(self, text):\n",
    "        speech = gTTS(text=text, lang='en', tld=self.tld, slow=False)\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            temp_filename = f'{tmpdir}/audio'\n",
    "            speech.save(temp_filename)\n",
    "            with open(temp_filename, 'rb') as f:\n",
    "                return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1f7accf3-a13d-448e-927d-d47d7a3961e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAITTS:\n",
    "    \"\"\"https://platform.openai.com/docs/guides/text-to-speech\"\"\"\n",
    "    WOMAN = 'nova'\n",
    "    MAN = 'echo'\n",
    "    def __init__(self, voice_id=None, model='tts-1'):\n",
    "        \"\"\"Voices:\n",
    "        alloy, echo, fable, onyx, nova, and shimmer\n",
    "        Models:\n",
    "        tts-1, tts-1-hd\n",
    "        \"\"\"\n",
    "        self.voice = voice_id\n",
    "        self.model = model\n",
    "\n",
    "    @RateLimited(95)\n",
    "    @jonlog.retry_with_logging()\n",
    "    def tts(self, text):\n",
    "        response = openai.OpenAI(api_key=openai.api_key).audio.speech.create(\n",
    "          model=self.model,\n",
    "          voice=self.voice,\n",
    "          input=text\n",
    "        )\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02e805e7-4928-45dd-9860-4c7f33697d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWSPollyTTS:\n",
    "    WOMAN = 'Kimberly'\n",
    "    MAN = 'Matthew'\n",
    "    BRIT_WOMAN = 'Amy'\n",
    "\n",
    "    def __init__(self, voice_id=None):\n",
    "        self.client = boto3.client('polly', region_name=\"us-east-1\")\n",
    "        self._voice_id = voice_id or self.WOMAN\n",
    "\n",
    "    @RateLimited(95)\n",
    "    @jonlog.retry_with_logging()\n",
    "    def tts(self, text):\n",
    "        response = self.client.synthesize_speech(\n",
    "            Text=text,\n",
    "            OutputFormat='mp3',\n",
    "            VoiceId=self._voice_id,\n",
    "            Engine=\"neural\",\n",
    "        )\n",
    "        # The audio stream containing the synthesized speech\n",
    "        audio_stream = response.get('AudioStream')\n",
    "        return audio_stream.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "46f8d0ca-2a36-4b5a-b39c-d19e0f94d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWSChat:\n",
    "    MODELS = {\n",
    "        \"claude-instant\": \"anthropic.claude-instant-v1\",\n",
    "        \"claude-best\": \"anthropic.claude-v2:1\",\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def msg(cls, messages=None, model=\"anthropic.claude-instant-v1\", **kwargs):\n",
    "        client = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "        try:\n",
    "            # The different model providers have individual request and response formats.\n",
    "            # For the format, ranges, and default values for Anthropic Claude, refer to:\n",
    "            # https://docs.anthropic.com/claude/reference/complete_post\n",
    "\n",
    "            # Claude requires you to enclose the prompt as follows:\n",
    "            # enclosed_prompt = \"Human: \" + prompt + \"\\n\\nAssistant:\"\n",
    "            prompt = \"\\n\\n\".join(\n",
    "                [f'{\"\" if (msg[\"role\"] == \"system\" and model == cls.MODELS[\"claude-best\"]) else (\"Human\" if msg[\"role\"] != \"assistant\" else \"Assistant\")}: {msg[\"content\"]}' for msg in messages] +\n",
    "                [\"Assistant:\"]\n",
    "            )\n",
    "\n",
    "            if 'temperature' not in kwargs:\n",
    "                kwargs['temperature'] = 1\n",
    "            body = {\n",
    "                \"prompt\": prompt,\n",
    "                \"max_tokens_to_sample\": 2048,\n",
    "                **kwargs\n",
    "            }\n",
    "            response = client.invoke_model(\n",
    "                modelId=model, body=json.dumps(body)\n",
    "            )\n",
    "            response_body = json.loads(response[\"body\"].read())\n",
    "            completion = response_body[\"completion\"]\n",
    "            return completion\n",
    "        except ClientError as e:\n",
    "            logger.exception(f\"Couldn't invoke {model}\", e)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2aaeccd5-5c89-44ec-9d65-4afe4b95b6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GoogleChat:\n",
    "#     MODELS = {\n",
    "#         \"gemini-pro\": \"gemini-pro\",\n",
    "#     }\n",
    "\n",
    "#     @classmethod\n",
    "#     def consolidate_messages(cls, message_list):\n",
    "#         if not message_list:\n",
    "#             return []\n",
    "    \n",
    "#         consolidated = []\n",
    "#         current_role = None\n",
    "#         current_content = \"\"\n",
    "    \n",
    "#         for message in message_list:\n",
    "#             role = message.get(\"role\")\n",
    "#             content = message.get(\"content\", \"\")\n",
    "    \n",
    "#             if role == \"system\":\n",
    "#                 role = \"user\"\n",
    "#             if role == current_role:\n",
    "#                 current_content += \"\\n\" + content\n",
    "#             else:\n",
    "#                 if current_role is not None:\n",
    "#                     consolidated.append({\"role\": current_role, \"content\": current_content})\n",
    "#                 current_content = content\n",
    "#                 current_role = role\n",
    "    \n",
    "#         if current_role is not None:\n",
    "#             consolidated.append({\"role\": current_role, \"content\": current_content})\n",
    "    \n",
    "#         return consolidated\n",
    "\n",
    "#     @classmethod\n",
    "#     def msg(cls, messages=None, model=\"gemini-pro\", **kwargs):\n",
    "#         vertexai.init(project='summer2023-392312', location='us-central1')\n",
    "#         model = vertexai.preview.generative_models.GenerativeModel(model)\n",
    "#         contents = [vertexai.generative_models._generative_models.Content(\n",
    "#             role=\"user\" if msg[\"role\"] != \"assistant\" else \"model\",\n",
    "#             parts=[vertexai.generative_models._generative_models.Part.from_text(msg[\"content\"])]\n",
    "#         ) for msg in cls.consolidate_messages(messages)]\n",
    "#         response = model.generate_content(contents=contents)\n",
    "#         return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e1682e3e-2215-49aa-9160-7d5809e423e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralChat:\n",
    "    MODELS = {\n",
    "        \"mistral-medium\": \"mistral-medium\",\n",
    "        \"mistral-small\": \"mistral-small\",\n",
    "    }\n",
    "    api_key = os.environ.get(\"MISTRAL_API_KEY\") or open('/Users/jong/.mistral_apikey').read().strip()\n",
    "\n",
    "    @classmethod\n",
    "    def consolidate_messages(cls, message_list):\n",
    "        if not message_list:\n",
    "            return []\n",
    "    \n",
    "        consolidated = []\n",
    "        current_role = None\n",
    "        current_content = \"\"\n",
    "    \n",
    "        for message in message_list:\n",
    "            role = message[\"role\"]\n",
    "            content = message[\"content\"]\n",
    "\n",
    "            if role == current_role:\n",
    "                current_content += \"\\n\" + content\n",
    "            else:\n",
    "                if current_role is not None:\n",
    "                    consolidated.append({\"role\": current_role, \"content\": current_content})\n",
    "                current_content = content\n",
    "                current_role = role\n",
    "    \n",
    "        if current_role is not None:\n",
    "            consolidated.append({\"role\": current_role, \"content\": current_content})\n",
    "\n",
    "        return consolidated\n",
    "\n",
    "    @classmethod\n",
    "    def msg(cls, messages=None, model=\"mistral-medium\", **kwargs):\n",
    "        client = MistralClient(api_key=cls.api_key)\n",
    "        if not any(msg['role'] == 'user' for msg in messages):\n",
    "            messages[-1]['role'] = 'user'\n",
    "        chat_response = client.chat(\n",
    "            model=model,\n",
    "            messages=[MistralChatMessage(**msg) for msg in cls.consolidate_messages(messages)],\n",
    "        )\n",
    "        return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a2ea46ab-a973-4ad2-852b-2613274dd9e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_MODEL = 'gpt-4-1106-preview'\n",
    "DEFAULT_LENGTH  = 80_000\n",
    "\n",
    "class Chat:\n",
    "    class Model(enum.Enum):\n",
    "        GPT3_5 = \"gpt-3.5-turbo\"\n",
    "        GPT_4  = \"gpt-4-1106-preview\"\n",
    "\n",
    "    def __init__(self, system, max_length=DEFAULT_LENGTH):\n",
    "        self._system = system\n",
    "        self._max_length = max_length\n",
    "        self._history = [\n",
    "            {\"role\": \"system\", \"content\": self._system},\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def num_tokens_from_text(cls, text, model=DEFAULT_MODEL):\n",
    "        \"\"\"Returns the number of tokens used by some text.\"\"\"\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        return len(encoding.encode(text))\n",
    "    \n",
    "    @classmethod\n",
    "    def num_tokens_from_messages(cls, messages, model=DEFAULT_MODEL):\n",
    "        \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":  # if there's a name, the role is omitted\n",
    "                    num_tokens += -1  # role is always required and always 1 token\n",
    "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "        return num_tokens\n",
    "\n",
    "    @retrying.retry(stop_max_attempt_number=5, wait_fixed=2000)\n",
    "    def _msg(self, *args, model=DEFAULT_MODEL, **kwargs):\n",
    "        logger.info(f'requesting chatcompletion {model=}...')\n",
    "        if model.startswith(\"AWS/\"):\n",
    "            model = model[4:]\n",
    "            resp = AWSChat.msg(\n",
    "                messages=self._history,\n",
    "                **kwargs\n",
    "            )\n",
    "        # elif model.startswith(\"GOOGLE/\"):\n",
    "        #     model = model[7:]\n",
    "        #     resp = GoogleChat.msg(messages=self._history, model=model, **kwargs)\n",
    "        elif model.startswith(\"MISTRAL/\"):\n",
    "            model = model[8:]\n",
    "            resp = MistralChat.msg(messages=self._history, model=model, **kwargs)\n",
    "        else:\n",
    "            resp = openai.OpenAI(api_key=openai.api_key).chat.completions.create(\n",
    "                *args,\n",
    "                model=model,\n",
    "                messages=self._history,\n",
    "                **kwargs\n",
    "            ).choices[0].message.content\n",
    "        logger.info(f'received chatcompletion {model=}...')\n",
    "        return resp\n",
    "    \n",
    "    def message(self, next_msg=None, **kwargs):\n",
    "        # TODO: Optimize this if slow through easy caching\n",
    "        while len(self._history) > 1 and self.num_tokens_from_messages(self._history) > self._max_length:\n",
    "            logger.info(f'Popping message: {self._history.pop(1)}')\n",
    "        if next_msg is not None:\n",
    "            self._history.append({\"role\": \"user\", \"content\": next_msg})\n",
    "        logger.info(f'Currently at {self.num_tokens_from_messages(self._history)=} tokens in conversation')\n",
    "        resp = self._msg(**kwargs)\n",
    "        text = resp\n",
    "        self._history.append({\"role\": \"assistant\", \"content\": text})\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "14a0eef7-3b50-4ca7-9210-2042d7adbe00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PodcastChat(Chat):\n",
    "    def __init__(self, topic, podcast=\"award winning\", max_length=DEFAULT_LENGTH, hosts=['Tom', 'Jen'], host_voices=[AWSPollyTTS(AWSPollyTTS.MAN), AWSPollyTTS(AWSPollyTTS.WOMAN)], extra_system=None):\n",
    "        system = f\"\"\"You are an {podcast} podcast with hosts {hosts[0]} and {hosts[1]}.\n",
    "Respond with the hosts names before each line like {hosts[0]}: and {hosts[1]}:\"\"\".replace(\"\\n\", \" \")\n",
    "        if extra_system is not None:\n",
    "            system = '\\n'.join([system, extra_system])\n",
    "        super().__init__(system, max_length=max_length)\n",
    "        self._podcast = podcast\n",
    "        self._topic = topic\n",
    "        self._hosts = hosts\n",
    "        self._history.append({\n",
    "            \"role\": \"user\", \"content\": f\"\"\"Generate an informative, entertaining, and very detailed podcast episode about {topic}.\n",
    "Make sure to teach complex topics in an intuitive way.\"\"\".replace(\"\\n\", \" \")\n",
    "        })\n",
    "        self._tts_h1, self._tts_h2 = host_voices\n",
    "\n",
    "    def text2speech(self, text, spacing_ms=350):\n",
    "        tmpdir = '/tmp'\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as thread_pool:\n",
    "            i = 0\n",
    "            jobs = []\n",
    "            def write_audio(msg, i, voice, **kwargs):\n",
    "                logger.info(f'requesting tts {i=} {voice=}')\n",
    "                s = voice.tts(msg)\n",
    "                logger.info(f'received tts {i=} {voice=}')\n",
    "                return s\n",
    "\n",
    "            text = text.replace('\\n', '!!!LINEBREAK!!!').replace('\\\\', '').replace('\"', '')\n",
    "            # Build text one at a time\n",
    "            currline, currname = \"\", self._hosts[0]\n",
    "            name2tld = {self._hosts[0]: 'co.uk', self._hosts[1]: 'com'}\n",
    "            name2voice = {self._hosts[0]: self._tts_h1, self._hosts[1]: self._tts_h2}\n",
    "            audios = []\n",
    "            for line in text.split(\"!!!LINEBREAK!!!\"):\n",
    "                if not line.strip(): continue\n",
    "                if line.startswith(f\"{self._hosts[0]}: \") or line.startswith(f\"{self._hosts[1]}: \"):\n",
    "                    if currline:\n",
    "                        jobs.append(thread_pool.submit(write_audio, currline, i, name2voice[currname], lang='en', tld=name2tld[currname]))\n",
    "                        i += 1\n",
    "                    currline = line[4:]\n",
    "                    currname = line[:3]\n",
    "                else:\n",
    "                    currline += line\n",
    "            if currline:\n",
    "                jobs.append(thread_pool.submit(write_audio, currline, i, name2voice[currname], lang='en', tld=name2tld[currname]))\n",
    "                i+=1\n",
    "            # Concat files\n",
    "            audios = [job.result() for job in jobs]\n",
    "            logger.info('concatting audio')\n",
    "            audio = merge_mp3s(audios)\n",
    "            logger.info('done with audio!')\n",
    "            IPython.display.display(IPython.display.Audio(audio, autoplay=False))\n",
    "            return audio\n",
    "            \n",
    "    def step(self, msg=None, skip_aud=False, ret_aud=True, **kwargs):\n",
    "        msg = self.message(msg, **kwargs)\n",
    "        if skip_aud: return msg\n",
    "        aud = self.text2speech(msg)\n",
    "        if ret_aud: return msg, aud\n",
    "        return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6cb12748-5a8a-4b27-b688-2e7bc583be17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PodcastRSSFeed:\n",
    "    \"\"\"Class to handle rss feed operations using github pages.\"\"\"\n",
    "\n",
    "    def __init__(self, org, repo, xml_path):\n",
    "        self.org = org\n",
    "        self.repo = repo\n",
    "        self.xml_path = xml_path\n",
    "        self.local_xml_path = self.download_podcast_xml()\n",
    "\n",
    "    def get_file_base64(self, file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            return base64.b64encode(file.read()).decode('utf-8')\n",
    "\n",
    "    def download_podcast_xml(self):\n",
    "        outfile = tempfile.NamedTemporaryFile().name + '.xml'\n",
    "        raw_url = f'https://raw.githubusercontent.com/{self.org}/{self.repo}/main/{self.xml_path}'\n",
    "        response = requests.get(raw_url)\n",
    "        print(raw_url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response.text)\n",
    "        with open(outfile, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        return outfile\n",
    "\n",
    "    def update_podcast_xml(self, xml_data, file_name, episode_title, episode_description, file_length):\n",
    "        # Parse XML\n",
    "        root = ET.fromstring(xml_data)\n",
    "        channel = root.find('channel')\n",
    "\n",
    "        file_extension = os.path.splitext(file_name)[-1].lower()[1:]\n",
    "        content_type = 'audio/' + file_extension\n",
    "        \n",
    "        # Add new episode\n",
    "        item = ET.SubElement(channel, 'item')\n",
    "        ET.SubElement(item, 'title').text = episode_title\n",
    "        ET.SubElement(item, 'description').text = episode_description\n",
    "        ET.SubElement(item, 'pubDate').text = dt.datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT')\n",
    "        ET.SubElement(item, 'enclosure', {\n",
    "            'url': f'https://{self.org}.github.io/{file_name}',\n",
    "            'type': content_type,\n",
    "            'length': str(file_length),\n",
    "        })\n",
    "        ET.SubElement(item, 'guid').text = str(uuid.uuid4())\n",
    "\n",
    "        # Convert back to string and pretty-format\n",
    "        pretty_xml = minidom.parseString(ET.tostring(root)).toprettyxml(indent='  ')\n",
    "        # Remove extra newlines\n",
    "        pretty_xml = os.linesep.join([s for s in pretty_xml.splitlines() if s.strip()])\n",
    "        return pretty_xml\n",
    "    \n",
    "    def upload_episode(self, file_path, file_name, episode_title, episode_description):\n",
    "        # Authenticate with GitHub\n",
    "        token = os.environ.get(\"GH_KEY\", None) or open(\"/Users/jong/.gh_token\").read().strip()\n",
    "        gh = Github(token)\n",
    "\n",
    "        # Get the repository\n",
    "        try:\n",
    "            repo = gh.get_user().get_repo(self.repo)\n",
    "        except:\n",
    "            repo = gh.get_organization(self.org).get_repo(self.repo)\n",
    "\n",
    "        # Upload the audio file\n",
    "        podsha = None\n",
    "        try:\n",
    "            podsha = repo.get_contents(file_name).sha\n",
    "        except:\n",
    "            pass\n",
    "        with open(file_path, 'rb') as audio_file:\n",
    "            audio_data = audio_file.read()\n",
    "            self.upload_to_github(file_name, audio_data, f'Upload new episode: {file_name}', podsha)\n",
    "\n",
    "        # Update and upload the podcast.xml file\n",
    "        file_length = os.path.getsize(file_path)\n",
    "        podcast_xml = repo.get_contents(self.xml_path)\n",
    "        xml_data = base64.b64decode(podcast_xml.content).decode('utf-8')\n",
    "        xml_data = self.update_podcast_xml(xml_data, file_name, episode_title, episode_description, file_length)\n",
    "        self.upload_to_github(self.xml_path, xml_data, f'Update podcast.xml with new episode: {file_name}', podcast_xml.sha)\n",
    "\n",
    "    def upload_to_github(self, file_name, file_content, commit_message, sha=None):\n",
    "        # Prepare API request headers\n",
    "        token = os.environ.get(\"GH_KEY\", None) or open(\"/Users/jong/.gh_token\").read().strip()\n",
    "        gh = Github(token)\n",
    "        # Get the repository\n",
    "        try:\n",
    "            repo = gh.get_user().get_repo(self.repo)\n",
    "        except:\n",
    "            repo = gh.get_organization(self.org).get_repo(self.repo)\n",
    "\n",
    "        if sha:\n",
    "            repo.update_file(file_name, commit_message, file_content, sha)\n",
    "        else:\n",
    "            repo.create_file(file_name, commit_message, file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6fea16c5-3d56-4277-a6eb-d74667b64435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Episode:\n",
    "    def __init__(self, episode_type='narration', podcast_args=(\"JonathanGrant\", \"jonathangrant.github.io\", \"podcasts/podcast.xml\"), text_model=DEFAULT_MODEL, **chat_kwargs):\n",
    "        \"\"\"\n",
    "        Kinds of episodes:\n",
    "            pure narration - simple TTS\n",
    "            simple podcast - Text to Podcast\n",
    "            complex podcast?\n",
    "        \"\"\"\n",
    "        self.episode_type = episode_type\n",
    "        self.chat = PodcastChat(**chat_kwargs)\n",
    "        self.chat_kwargs = chat_kwargs\n",
    "        self.pod = PodcastRSSFeed(*podcast_args)\n",
    "        self.text_model = text_model\n",
    "        self.sounds = []\n",
    "        self.texts = []\n",
    "\n",
    "    def get_outline(self, n, topic=None):\n",
    "        if topic is None: topic = self.chat._topic\n",
    "        chat = Chat(f\"\"\"Write \n",
    "a concise plaintext outline with exactly {n} parts for a podcast titled {self.chat._podcast}.\n",
    "Only return the parts and nothing else.\n",
    "Do not include a conclusion or intro.\n",
    "Do not write more than {n} parts.\n",
    "Format it like this: 1. insert-title-here, 2. another-title-here, ...\"\"\".replace(\"\\n\", \" \"))\n",
    "        resp = chat.message(model=self.text_model)\n",
    "        chapter_pattern = re.compile(r'\\d+\\.\\s+.*')\n",
    "        chapters = chapter_pattern.findall(resp)\n",
    "        if not chapters:\n",
    "            logger.warning(f'Could not parse message for chapters! Message:\\n{resp}')\n",
    "        return chapters\n",
    "\n",
    "    def step(self, msg=None, nparts=3):\n",
    "        include = f\" Remember to respond with the hosts names like {self.chat._hosts[0]}: and {self.chat._hosts[1]}:\"\n",
    "        msg = msg or self.chat._topic\n",
    "        if self.episode_type == 'narration':\n",
    "            outline = self.get_outline(msg, nparts)\n",
    "            logger.info(f\"Outline: {outline}\")\n",
    "            intro_txt, intro_aud = self.chat.step(f\"Write the intro for a podcast about {msg}. The outline for the podcast is {', '.join(outline)}. Only write the introduction.{include}\", model=self.text_model)\n",
    "            self.sounds.append(intro_aud)\n",
    "            self.texts.append(intro_txt)\n",
    "            # Get parts\n",
    "            for part in outline:\n",
    "                logger.info(f\"Part: {part}\")\n",
    "                part_txt, part_aud = self.chat.step(f\"Write the next part: {part}.{include}\", model=self.text_model)\n",
    "                self.sounds.append(part_aud)\n",
    "                self.texts.append(part_txt)\n",
    "            # Get conclusion\n",
    "            logger.info(\"Conclusion\")\n",
    "            part_txt, part_aud = self.chat.step(f\"Write the conclusion. Remember, the outline was: {', '.join(outline)}.{include}\", model=self.text_model)\n",
    "            self.sounds.append(part_aud)\n",
    "            self.texts.append(part_txt)\n",
    "        elif self.episode_type == 'pure_tts':\n",
    "            outline = None\n",
    "            audio = self.chat.text2speech(\"\\n\".join([self.chat._hosts[i%2]+\": \"+x for i,x in enumerate(msg)]))\n",
    "            self.sounds.append(audio)\n",
    "            self.texts.extend(msg)\n",
    "        return outline, '\\n'.join(self.texts)\n",
    "\n",
    "    def upload(self, title, descr):\n",
    "        title_small = title.lower().replace(\" \", \"_\")[:16] + str(uuid.uuid4())  # I had a filename too long once\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            tmppath = os.path.join(tmpdir, \"audio_file.mp3\")\n",
    "            with open(tmppath, \"wb\") as f:\n",
    "                f.write(merge_mp3s(self.sounds))\n",
    "            self.pod.upload_episode(tmppath, f\"podcasts/audio/{title_small}.mp3\", title, descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "75d0a7d1-de77-4129-a15c-537c7919b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_mp3s(mp3_bytes_list):\n",
    "    \"\"\"\n",
    "    Merges multiple MP3 bytestrings into a single MP3 bytestring.\n",
    "    \n",
    "    :param mp3_bytes_list: List of MP3 bytestrings\n",
    "    :return: Merged MP3 as bytestring\n",
    "    \"\"\"\n",
    "    # Convert the first MP3 bytestring to an AudioSegment\n",
    "    combined = pydub.AudioSegment.from_file(io.BytesIO(mp3_bytes_list[0]), format=\"mp3\")\n",
    "\n",
    "    # Loop through the rest of the MP3 bytestrings and append them\n",
    "    for mp3_bytes in mp3_bytes_list[1:]:\n",
    "        next_segment = pydub.AudioSegment.from_file(io.BytesIO(mp3_bytes), format=\"mp3\")\n",
    "        combined += next_segment\n",
    "\n",
    "    # Export the combined audio to a bytestring\n",
    "    combined_buffer = io.BytesIO()\n",
    "    combined.export(combined_buffer, format=\"mp3\")\n",
    "    return combined_buffer.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1132bda8-7767-4f78-9789-046c4af27a72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ep = Episode(\n",
    "#     episode_type='narration',\n",
    "#     topic=\"Hidden History: Unraveling 3 of History's Funniest Mysteries\",\n",
    "#     max_length=10_000,\n",
    "#     # text_model='gpt-4-1106-preview',\n",
    "#     text_model='MISTRAL/mistral-medium',\n",
    "# )\n",
    "# outline, txt = ep.step(nparts='3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "37bef16d-9efb-45b2-b2ac-36dd3f63ae3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom: Welcome to our podcast, where we dive deep into the rabbit hole of history to uncover some of the most intriguing and often hilarious mysteries that have left historians scratching their heads for centuries. I'm Tom, your co-host and guide through the world of hidden history.\n",
      "\n",
      "Jen: And I'm Jen, your other host and fellow history enthusiast. Today's episode is all about unraveling three of history's funniest mysteries, and we're excited to take you on this entertaining journey with us.\n",
      "\n",
      "Tom: Our first stop is 11th century Scandinavia, where we'll explore the strange and bizarre tale of the Great Viking Cat Massacre of 1014. It's a story that will leave you wondering just what those Vikings were thinking.\n",
      "\n",
      "Jen: Next, we'll fast forward to the 20th century and travel all the way to Oregon, where we'll dive into the truly absurd and unforgettable Curious Case of the Exploding Whale. Trust us, you won't believe your ears when you hear this one.\n",
      "\n",
      "Tom: And finally, we'll journey back in time to the Renaissance period, where we'll uncover the truth behind one of the most iconic paintings of all time, the Mona Lisa. Specifically, we'll focus on the Mysterious Identity of the Mona Lisa's Smile and what it reveals about the painting's hidden history.\n",
      "\n",
      "Jen: So grab your headphones and get ready for a wild ride as we delve into the world of hidden history and uncover some of the funniest and most bizarre mysteries that history has to offer. Let's get started!\n",
      "Tom: Alright, let's dive right in and start with our first mystery, the Great Viking Cat Massacre of 1014. Now, at first glance, you might think this is just a silly or trivial topic, but trust us, there's more to this story than meets the eye.\n",
      "\n",
      "Jen: That's right, Tom. The massacre took place in Dublin, which was then a thriving Viking settlement. According to historical records, the Viking ruler of Dublin, named Olaf Haraldsson, ordered the killing of every single cat in the city.\n",
      "\n",
      "Tom: And when we say every single cat, we mean it. The Viking warriors went door to door, rounding up every last feline and slaughtering them in the streets. It's estimated that thousands of cats were killed during this massacre.\n",
      "\n",
      "Jen: But why, you might be wondering, would anyone order such a thing? Well, that's where things get interesting. According to one theory, the massacre was part of a larger religious ritual. Cats were often associated with witchcraft and paganism in Viking culture, so Olaf may have seen the cats as a threat to his Christian faith.\n",
      "\n",
      "Tom: Another theory is that the cats were seen as a symbol of the Viking's enemies, who were also known to keep cats as pets. By killing the cats, Olaf may have been sending a message to his enemies that he was not to be trifled with.\n",
      "\n",
      "Jen: Whatever the reason, the Great Viking Cat Massacre of 1014 remains one of the most bizarre and inexplicable events in history. And it just goes to show that even the most seemingly trivial topics can have deep and fascinating histories behind them.\n",
      "\n",
      "Tom: Up next, we'll explore the truly absurd and unforgettable Curious Case of the Exploding Whale. Trust us, you won't believe your ears when you hear this one. So stay tuned!\n",
      "Jen: Alright, it's time to move on to our second mystery, the Curious Case of the Exploding Whale. Now, this is a story that's so bizarre, it's hard to believe it's true.\n",
      "\n",
      "Tom: That's right, Jen. It all started back in 1970, when a dead whale washed up on a beach in Oregon. The local authorities were faced with a tough decision - how to dispose of the massive carcass.\n",
      "\n",
      "Jen: They eventually decided to blow it up using dynamite, in the hopes that the explosion would break the whale into smaller pieces that could be easily removed. But things didn't quite go as planned.\n",
      "\n",
      "Tom: When the dynamite was detonated, the whale exploded with such force that it sent massive chunks of blubber and entrails flying through the air, raining down on spectators and nearby buildings.\n",
      "\n",
      "Jen: The explosion was so powerful that it even damaged a nearby car, which was crushed by a flying chunk of whale flesh. Fortunately, no one was seriously injured, but the incident became a national laughing stock and has been remembered as one of the most absurd and ill-conceived public works projects in history.\n",
      "\n",
      "Tom: But the Curious Case of the Exploding Whale doesn't end there. Despite the obvious failures of the initial explosion, the authorities decided to try again, this time using even more dynamite.\n",
      "\n",
      "Jen: The second explosion was just as disastrous as the first, sending even more whale guts flying through the air and causing even more damage to nearby buildings.\n",
      "\n",
      "Tom: In the end, it took several days and multiple explosions to finally remove the whale carcass from the beach. And the incident has gone down in history as one of the most bizarre and memorable events in Oregon's history.\n",
      "\n",
      "Jen: Up next, we'll explore the mysterious identity of the Mona Lisa's smile and what it reveals about the painting's hidden history. So stay tuned!\n",
      "Tom: Alright, it's time to move on to our final mystery, the Mysterious Identity of the Mona Lisa's Smile. This is one of the most iconic paintings in the world, and yet there's still so much we don't know about it.\n",
      "\n",
      "Jen: That's right, Tom. The Mona Lisa, also known as La Gioconda, was painted by the Renaissance master Leonardo da Vinci in the early 16th century. It's a portrait of a woman with an enigmatic smile that has captivated viewers for centuries.\n",
      "\n",
      "Tom: But who exactly is the woman in the painting? That's the question that has puzzled historians and art lovers for generations.\n",
      "\n",
      "Jen: One popular theory is that the Mona Lisa is a portrait of Lisa Gherardini, the wife of a wealthy Florentine merchant named Francesco del Giocondo. This theory is based on records that suggest Leonardo was commissioned to paint Lisa's portrait by her husband.\n",
      "\n",
      "Tom: But other theories suggest that the Mona Lisa might be a portrait of a different woman entirely. Some have suggested that she might be a self-portrait of Leonardo himself, or even a portrait of a mysterious Catholic saint.\n",
      "\n",
      "Jen: What's interesting is that the Mona Lisa's smile has been the subject of much debate and speculation. Some say that the smile is a symbol of the Renaissance's fascination with human emotion and expression, while others see it as a more mystical or spiritual symbol.\n",
      "\n",
      "Tom: In the end, the true identity of the Mona Lisa may never be known for sure. But that's part of what makes her such an enduring and fascinating figure in the world of art and history.\n",
      "\n",
      "Jen: Well, that's all the time we have for today. We hope you've enjoyed this journey into the world of hidden history and the funny mysteries that have captured our imagination.\n",
      "\n",
      "Tom: Thanks for listening, and we'll see you next time on our podcast!\n",
      "Jen: Well, that's all the time we have for today. We hope you've enjoyed this journey into the world of hidden history and the funny mysteries that have captured our imagination.\n",
      "\n",
      "Tom: From the strange and bizarre tale of the Great Viking Cat Massacre of 1014, to the truly absurd and unforgettable Curious Case of the Exploding Whale, and finally to the Mysterious Identity of the Mona Lisa's Smile, we've covered a lot of ground today.\n",
      "\n",
      "Jen: And while these mysteries may never be fully solved, they serve as a reminder of just how fascinating and complex the world of history can be.\n",
      "\n",
      "Tom: We want to thank you for joining us on this journey, and we hope you'll tune in next time for even more hidden history and funny mysteries.\n",
      "\n",
      "Jen: Until then, keep exploring, keep learning, and keep wondering about the world around you. Who knows what strange and wonderful secrets you might uncover!\n",
      "\n",
      "Tom: Thanks again for listening, and we'll see you next time on our podcast!\n"
     ]
    }
   ],
   "source": [
    "# print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "026550a4-c033-45be-89f8-67d010ed7b24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom: \n",
      "Hello, and welcome to another episode of \"Hidden History: Unraveling Mysteries of the Past\"! I'm your host, Tom, and joining me is my co-host, the lovely and witty, Jen.\n",
      "\n",
      "Jen: \n",
      "Hey there, history enthusiasts! Today, we're diving into a lighter side of historical mysteries. We'll be exploring three of the funniest and most perplexing enigmas that have left historians scratching their heads and chuckling in equal measure.\n",
      "\n",
      "Tom: \n",
      "That's right, Jen! We'll be discussing the Case of the Mysterious Fairy Photos, the Great Moon Hoax, and the Lost Colony of Roanoke. Each of these hilarious historical puzzles has its unique twist that's sure to leave you both informed and entertained.\n",
      "\n",
      "Jen: \n",
      "First up, we'll be investigating the enigma of the Cottingley Fairies. In the early 20th century, two young girls from England managed to deceive some of the world's most esteemed intellectuals with their seemingly authentic photographs of fairies. We'll delve into their clever trickery and explore how they managed to fool the masses.\n",
      "\n",
      "Tom: \n",
      "Next, we'll be unraveling the outrageous story of the Great Moon Hoax. In 1835, the New York Sun newspaper published a series of articles claiming that life had been discovered on the moon. These articles were filled with fantastical descriptions of moon dwellers, including bipedal beavers and man-bats. We'll learn about the impact of this hoax on the public and the journalistic world, and discuss why it was so successful.\n",
      "\n",
      "Jen: \n",
      "Lastly, we'll be delving into the bizarre disappearance of the Roanoke colonists. In 1587, over 100 English settlers arrived on Roanoke Island, off the coast of present-day North Carolina. When their leader returned three years later, he found the colony completely abandoned, with no trace of the colonists. We'll explore some of the more comedic theories surrounding their fate, including the possibility that they were abducted by aliens or turned into zombies.\n",
      "\n",
      "Tom: \n",
      "So buckle up, history buffs! It's time to embark on a whimsical journey through the annals of time, uncovering the hidden humor in some of history's most intriguing mysteries. Let's get started with the Case of the Mysterious Fairy Photos!\n",
      "\n",
      "Jen: \n",
      "Stay tuned, and don't forget to subscribe to \"Hidden History: Unraveling Mysteries of the Past\" for more fascinating tales from the past. If you enjoy our show, please leave us a rating and review on your favorite podcast platform. Thanks for listening!\n",
      "\n",
      "Tom:\n",
      "And as always, we're here to make history fun and approachable for everyone, so feel free to reach out to us with any questions, comments, or ideas for future episodes. You can find us on social media or our website, hiddenhistorypodcast.com. Until next time, history aficionados! Keep exploring, and keep laughing!\n",
      "Tom: \n",
      "Alright, history detectives, it's time to dive into our first mystery: the Case of the Mysterious Fairy Photos. This fascinating tale revolves around two young cousins, Elsie Wright and Frances Griffiths, who managed to captivate the world with their photographs of what appeared to be real fairies in their garden.\n",
      "\n",
      "Jen: \n",
      "That's right, Tom! This story takes us back to Cottingley, a small village in England, during the early 1900s. Elsie, who was 16 at the time, and her younger cousin Frances, who was just 10 years old, were known for their love of nature and their vivid imaginations.\n",
      "\n",
      "Tom: \n",
      "One day, in 1917, the girls borrowed Elsie's father's camera and claimed to have taken photographs of real fairies in the nearby beck, or stream. They showed the pictures to their family, who were amazed by what they saw. The images seemed to depict tiny, winged beings dancing and frolicking among the flowers.\n",
      "\n",
      "Jen: \n",
      "Initially, the photographs were met with skepticism, but as they began to circulate among friends and acquaintances, they garnered more and more attention. Eventually, the pictures found their way to Edward Gardner, a prominent Theosophist and a leading member of the British Spiritualist movement.\n",
      "\n",
      "Tom: \n",
      "Gardner was convinced that the photographs were genuine and decided to share them with the world. He enlisted the help of Sir Arthur Conan Doyle, the famous author of the Sherlock Holmes series and a fervent believer in the existence of fairies. Together, they published the photos in a 1920 article in The Strand Magazine, titled \"Fairies Photographed: An Epoch-Making Event.\"\n",
      "\n",
      "Jen: \n",
      "The article caused a sensation, with people all over the world debating the authenticity of the Cottingley Fairies. Some experts, including renowned photographer Harold Snelling, examined the photos and declared them to be genuine. Others remained skeptical, suggesting that the images were merely clever forgeries.\n",
      "\n",
      "Tom: \n",
      "Despite the controversy, the girls stuck to their story, insisting that the photographs were real. It wasn't until the late 1970s, when Elsie was in her eighties, that she finally admitted that the fairy photos were, in fact, fakes. She and Frances had used paper cutouts of fairies from a popular children's book and cleverly positioned them in the garden to create the illusion of real, living fairies.\n",
      "\n",
      "Jen: \n",
      "However, Elsie maintained that one of the photographs, known as the \"Fairy Bower\" image, was genuine. She claimed that while the other photos were fakes, this particular image had captured a real, fleeting glimpse of the supernatural world.\n",
      "\n",
      "Tom: \n",
      "So there you have it, history enthusiasts! The Case of the Mysterious Fairy Photos is a delightful tale of childhood imagination, clever trickery, and the power of belief. It's a reminder that even in the realm of history, there's always room for a little bit of whimsy and wonder.\n",
      "\n",
      "Jen: \n",
      "Absolutely, Tom! And it just goes to show that sometimes, the most enduring historical mysteries are those that spark our imaginations and make us question the boundaries of what we think is possible.\n",
      "\n",
      "Tom: \n",
      "Coming up next, we'll be discussing the Great Moon Hoax of 1835. Stay tuned as we explore this outrageous story of journalistic deception and the impact it had on the public's perception of the moon and extraterrestrial life. Don't forget to follow us on social media and visit our website, hiddenhistorypodcast.com, for more fascinating tales from the past. Thanks for listening!\n",
      "Tom: \n",
      "Welcome back, history fans! Now that we've explored the whimsical tale of the Cottingley Fairies, it's time to delve into our next historical mystery: the Great Moon Hoax of 1835. This audacious story revolves around a series of newspaper articles that convinced readers that life had been discovered on the moon, complete with fantastical descriptions of moon dwellers, including bipedal beavers and man-bats.\n",
      "\n",
      "Jen: \n",
      "That's right, Tom! The Great Moon Hoax is a fascinating example of journalistic trickery and the power of the press. The hoax was perpetrated by the New York Sun, a popular newspaper at the time, which published a series of six articles in August 1835, claiming that renowned astronomer Sir John Herschel had made incredible discoveries about the moon and its inhabitants using a powerful new telescope.\n",
      "\n",
      "Tom: \n",
      "The articles were written in a convincing and authoritative tone, complete with detailed illustrations and supposed quotes from Sir John Herschel himself. They described a lunar landscape teeming with life, including lush forests, sparkling oceans, and bizarre creatures that defied imagination.\n",
      "\n",
      "Jen: \n",
      "Readers were captivated by the stories, and the newspaper's circulation skyrocketed. People all over the city were discussing the astonishing discoveries, and some even went so far as to organize moon-viewing parties, hoping to catch a glimpse of the fantastical lunar inhabitants for themselves.\n",
      "\n",
      "Tom: \n",
      "However, it wasn't long before the truth came to light. The articles were, in fact, a clever hoax, masterminded by Richard Adams Locke, a writer for the New York Sun. Locke had fabricated the entire story, drawing inspiration from contemporary scientific theories and popular literary works, such as those of Edgar Allan Poe and Jules Verne.\n",
      "\n",
      "Jen: \n",
      "Despite the revelation that the articles were fake, the Great Moon Hoax had a lasting impact on the public's perception of the moon and extraterrestrial life. It sparked widespread interest in astronomy and fueled the imaginations of countless writers, artists, and scientists who were inspired by the idea of a living, breathing moon.\n",
      "\n",
      "Tom: \n",
      "Moreover, the hoax served as an important reminder of the power of the press and the importance of fact-checking and critical thinking. It demonstrated how easily the public could be swayed by sensational stories and underscored the need for responsible journalism in an era of rapid technological change.\n",
      "\n",
      "Jen: \n",
      "So there you have it, history lovers! The Great Moon Hoax is a cautionary tale about the dangers of believing everything you read and a testament to the power of a good story. It's a reminder that even in the realm of history, there's always room for a bit of mischief and creativity.\n",
      "\n",
      "Tom: \n",
      "Coming up next, we'll be discussing the Lost Colony of Roanoke and the comedic theories surrounding the bizarre disappearance of its inhabitants. Stay tuned as we explore the possibilities of alien abductions, zombie transformations, and more! Don't forget to follow us on social media and visit our website, hiddenhistorypodcast.com, for more fascinating tales from the past. Thanks for listening!\n",
      "Tom: \n",
      "Hey there, history enthusiasts! Welcome back to our final segment in this episode of \"Hidden History: Unraveling Mysteries of the Past.\" Today, we're going to delve into the strange and confounding case of the Lost Colony of Roanoke and explore some of the more comedic theories surrounding the fate of its inhabitants.\n",
      "\n",
      "Jen: \n",
      "That's right, Tom! The Lost Colony of Roanoke is one of the most enduring mysteries in American history. In 1587, a group of over 100 English settlers arrived on Roanoke Island, off the coast of present-day North Carolina, with the intention of establishing a permanent colony. However, when their leader, John White, returned three years later, he found the settlement completely abandoned, with no trace of the colonists.\n",
      "\n",
      "Tom: \n",
      "The only clue left behind was the word \"Croatoan\" carved into a post, leading some to believe that the colonists had fled to nearby Croatoan Island. However, no evidence of their presence was ever found, and the fate of the Roanoke colonists remains one of history's most perplexing enigmas.\n",
      "\n",
      "Jen: \n",
      "Over the years, numerous theories have been proposed to explain the colonists' disappearance, ranging from the plausible to the downright absurd. Some of the more comedic theories include the possibility that the colonists were abducted by aliens or turned into zombies.\n",
      "\n",
      "Tom: \n",
      "While these theories are certainly entertaining, it's important to approach them with a healthy dose of skepticism. There's no concrete evidence to support the idea that the colonists were abducted by extraterrestrial beings, and the concept of zombies as we know them today didn't exist in the 16th century.\n",
      "\n",
      "Jen: \n",
      "However, that hasn't stopped some enterprising writers and filmmakers from exploring these ideas in their works. For example, the 2007 horror film \"The Lost Colony\" posits that the colonists were turned into zombies by a cursed Native American artifact. And in the 2014 science fiction novel \"Roanoke\" by Amy Thomas, the colonists are abducted by aliens and taken to a distant planet.\n",
      "\n",
      "Tom: \n",
      "While these stories are undoubtedly fun and imaginative, it's important to remember that the true fate of the Roanoke colonists is still unknown. Historians continue to debate the possible explanations for their disappearance, including disease, starvation, and conflict with local Native American tribes.\n",
      "\n",
      "Jen: \n",
      "So, history detectives, while we may never know the full story behind the Lost Colony of Roanoke, it's always fun to explore the more humorous and outlandish theories that have emerged over the years. And who knows? Maybe one day, new evidence will come to light that will finally solve this centuries-old mystery.\n",
      "\n",
      "Tom: \n",
      "That's all for today's episode of \"Hidden History: Unraveling Mysteries of the Past.\" We hope you've enjoyed our exploration of three of history's funniest mysteries, and we encourage you to continue exploring the past with a sense of curiosity and wonder. Thanks for listening!\n",
      "\n",
      "Jen: \n",
      "Don't forget to subscribe to our podcast and follow us on social media for more fascinating tales from the annals of history. If you have any questions, comments, or suggestions for future episodes, feel free to reach out to us on our website, hiddenhistorypodcast.com. Until next time, history lovers! Keep exploring, and keep laughing!\n",
      "Tom: \n",
      "Well, history sleuths, that brings us to the end of our latest episode of \"Hidden History: Unraveling Mysteries of the Past.\" Today, we've explored three of history's most intriguing and humorous enigmas, from the enchanting story of the Cottingley Fairies to the outrageous Great Moon Hoax, and the baffling case of the Lost Colony of Roanoke.\n",
      "\n",
      "Jen: \n",
      "We hope you've enjoyed our journey through the annals of time and that you've learned something new along the way. Whether it's the power of imagination, the importance of critical thinking, or the enduring allure of the unknown, these stories remind us that history is a rich and complex tapestry, full of surprises and delights.\n",
      "\n",
      "Tom: \n",
      "As we've seen, sometimes the most fascinating mysteries are those that make us laugh, challenge our assumptions, and inspire our creativity. So, the next time you're faced with a seemingly insoluble puzzle, remember to approach it with an open mind, a sense of humor, and a healthy dose of skepticism.\n",
      "\n",
      "Jen: \n",
      "Thank you for joining us on this adventure through the past. If you enjoyed today's episode, please consider subscribing to our podcast and leaving a rating or review on your favorite podcast platform. Your support helps us continue to bring you the best in historical storytelling.\n",
      "\n",
      "Tom: \n",
      "And don't forget to follow us on social media and visit our website, hiddenhistorypodcast.com, for more engaging and enlightening tales from the annals of history. If you have any questions, comments, or suggestions for future episodes, please don't hesitate to reach out to us. We love hearing from our listeners!\n",
      "\n",
      "Jen: \n",
      "Until next time, history fans, keep exploring, keep questioning, and keep laughing! As always, we're grateful for the opportunity to share our love of history with you, and we can't wait to embark on our next adventure together. So long, and happy sleuthing!\n",
      "\n",
      "Tom: \n",
      "Goodbye, everyone! And remember, the past is never as far away as it seems. It's all around us, waiting to be discovered, unraveled, and appreciated. So, keep digging, keep learning, and keep sharing the wonders of history with those around you. Take care!\n"
     ]
    }
   ],
   "source": [
    "# print(txt)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
