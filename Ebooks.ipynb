{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31815dc-761f-49e6-9d51-5781c1e9fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "import tempfile\n",
    "import IPython\n",
    "import structlog\n",
    "import random\n",
    "import enum\n",
    "import re\n",
    "import requests\n",
    "import subprocess\n",
    "import concurrent.futures\n",
    "import retrying\n",
    "from github import Github\n",
    "import string, os\n",
    "import markdown2\n",
    "from ebooklib import epub\n",
    "logger = structlog.getLogger()\n",
    "openai.api_key_path = '/home/jong/.openai_key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16125502-f025-4520-91ff-15c5fde0563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chat:\n",
    "    def __init__(self, system, max_length=4096//2):\n",
    "        self._system = system\n",
    "        self._max_length = max_length\n",
    "        self._history = [\n",
    "            {\"role\": \"system\", \"content\": self._system},\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def num_tokens_from_messages(cls, messages, model=\"gpt-3.5-turbo\"):\n",
    "        \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":  # if there's a name, the role is omitted\n",
    "                    num_tokens += -1  # role is always required and always 1 token\n",
    "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "        return num_tokens\n",
    "\n",
    "    @retrying.retry(stop_max_attempt_number=5, wait_fixed=2000)\n",
    "    def message(self, next_msg=None):\n",
    "        # TODO: Optimize this if slow through easy caching\n",
    "        while len(self._history) > 1 and self.num_tokens_from_messages(self._history) > self._max_length:\n",
    "            logger.info(f'Popping message: {self._history.pop(1)}')\n",
    "        if next_msg is not None:\n",
    "            self._history.append({\"role\": \"user\", \"content\": next_msg})\n",
    "        logger.info('requesting openai.Chat...')\n",
    "        resp = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=self._history,\n",
    "        )\n",
    "        logger.info('received openai.Chat...')\n",
    "        text = resp.choices[0].message.content\n",
    "        self._history.append({\"role\": \"assistant\", \"content\": text})\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bed2feb-6161-4740-8ef0-f1a6d6841894",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image:\n",
    "    class Size(enum.Enum):\n",
    "        SMALL = \"256x256\"\n",
    "        MEDIUM = \"512x512\"\n",
    "        LARGE = \"1024x1024\"\n",
    "\n",
    "    @classmethod\n",
    "    @retrying.retry(stop_max_attempt_number=5, wait_fixed=2000)\n",
    "    def create(cls, prompt, n=1, size=Size.SMALL):\n",
    "        logger.info('requesting openai.Image...')\n",
    "        resp = openai.Image.create(prompt=prompt, n=n, size=size.value)\n",
    "        logger.info('received openai.Image...')\n",
    "        if n == 1: return resp[\"data\"][0]\n",
    "        return resp[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "25f356f9-a51b-4a3c-ba85-576250d85a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-13 18:06:21 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:06:26 [info     ] received openai.Image...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x7efc28c114f0> JSON: {\n",
       "  \"url\": \"https://oaidalleapiprodscus.blob.core.windows.net/private/org-ct6DYQ3FHyJcnH1h6OA3fR35/user-qvFBAhW3klZpvcEY1psIUyDK/img-v4XrxHZVPaa43agYj2g3PM40.png?st=2023-04-13T23%3A38%3A04Z&se=2023-04-14T01%3A38%3A04Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-04-13T17%3A15%3A26Z&ske=2023-04-14T17%3A15%3A26Z&sks=b&skv=2021-08-06&sig=HqHMc9GFzXuqWd04QhJoRAn8HApywj7jnC3HQ32lBbI%3D\"\n",
       "}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.create('Detective Yoda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93f3718f-d273-40c2-839e-63e7e9363c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_md_files_to_epub(book_id, book_title, md_files, output_file):\n",
    "    # Create a new EPUB book\n",
    "    book = epub.EpubBook()\n",
    "\n",
    "    # Set metadata\n",
    "    book.set_identifier(book_id)\n",
    "    book.set_title(book_title)\n",
    "    book.set_language('en')\n",
    "    book.add_author(\"Jonathan Grant\")\n",
    "\n",
    "    # Create a list to store the EPUB chapters\n",
    "    epub_chapters = []\n",
    "\n",
    "    # Define a CSS style for code blocks\n",
    "    code_style = '''\n",
    "    pre {\n",
    "        background-color: #f5f5f5;\n",
    "        border: 1px solid #ccc;\n",
    "        padding: 10px;\n",
    "        overflow-x: auto;\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    # Add the CSS style to the book\n",
    "    style = epub.EpubItem(\n",
    "        uid=\"code_style\",\n",
    "        file_name=\"styles/code_style.css\",\n",
    "        media_type=\"text/css\",\n",
    "        content=code_style,\n",
    "    )\n",
    "    book.add_item(style)\n",
    "\n",
    "    for idx, md_file in enumerate(md_files):\n",
    "        # Read the contents of the .md file\n",
    "        with open(md_file, 'r') as file:\n",
    "            md_content = file.read()\n",
    "\n",
    "        # Convert the .md content to HTML, enabling 'fenced-code-blocks' extra\n",
    "        html_content = markdown2.markdown(md_content, extras=[\"fenced-code-blocks\"])\n",
    "\n",
    "        # Create an EPUB chapter\n",
    "        epub_chapter = epub.EpubHtml(\n",
    "            title=f'Chapter {idx + 1}',\n",
    "            file_name=f'chapter_{idx + 1}.xhtml',\n",
    "            content=html_content,\n",
    "        )\n",
    "\n",
    "        # Link the CSS style to the chapter\n",
    "        epub_chapter.add_item(style)\n",
    "        epub_chapter.add_link(href=\"styles/code_style.css\", rel=\"stylesheet\", type=\"text/css\")\n",
    "\n",
    "        # Add the chapter to the book and the list of chapters\n",
    "        book.add_item(epub_chapter)\n",
    "        epub_chapters.append(epub_chapter)\n",
    "\n",
    "    # Create a book spine (required for the EPUB format)\n",
    "    book.spine = ['nav'] + epub_chapters\n",
    "\n",
    "    # Create and add a Table of Contents\n",
    "    book.toc = epub_chapters\n",
    "    book.add_item(epub.EpubNcx())\n",
    "    book.add_item(epub.EpubNav())\n",
    "\n",
    "    # Write the EPUB file\n",
    "    epub.write_epub(output_file, book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d00dd8d8-7ebe-40b3-adc9-9aa43a0c29d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EBookWriter:\n",
    "    theme_description = \"Make sure it's engaging, relevant, and educational. Incorporate references to published journals when relevant. Include entertaining facts or jokes when appropriate.\"\n",
    "\n",
    "    themes = {\n",
    "        \"Sherlock Holmes mystery\": theme_description,\n",
    "        \"Dracula story\": theme_description,\n",
    "        \"Frankenstein's Monster story\": theme_description,\n",
    "        \"Robin Hood story\": theme_description,\n",
    "        \"King Arthur and the Knights of the Round Table story\": theme_description,\n",
    "        \"Greek Mythology epic\": theme_description,\n",
    "        \"Alice in Wonderland trippy story\": theme_description,\n",
    "        \"Wizard of Oz parable\": theme_description,\n",
    "    }\n",
    "\n",
    "    def __init__(self, topic, nchapters=16, include_code=True, theme=None, theme_extra=None, author_style=None, include_next=True, publish={'gh', 'amazon'}):\n",
    "        self.topic = topic\n",
    "        self.nchapters = nchapters\n",
    "        self.include_code = include_code\n",
    "        self.theme = theme\n",
    "        self.theme_extra = theme_extra\n",
    "        self.author_style = author_style\n",
    "        self.include_next = include_next\n",
    "        self.publish = publish\n",
    "\n",
    "    def get_chapters(self):\n",
    "        chat = Chat(\"You are EBookGPT. Generate chapters for a textbook topic.\")\n",
    "        resp = chat.message(f'Write the table of contents for a textbook about {self.topic} involving {self.nchapters} chapters. Just return the ordered list of chapters and nothing else. Do not include a conclusion.')\n",
    "        chapter_pattern = re.compile(r'\\d+\\.\\s+.*')\n",
    "        chapters = chapter_pattern.findall(resp)\n",
    "        if not chapters:\n",
    "            logger.warning(f'Could not parse message for chapters! Message:\\n{resp}')\n",
    "        return chapters\n",
    "    \n",
    "    def get_special_guest_for_chapter(self, chapter):\n",
    "        chat = Chat(f'''You are EBookGPT. You write chapters for textbooks on {self.topic}.\n",
    "You will respond with just the name of a special guest who should appear in a chapter given to you.\n",
    "Only respond with the name. Do not say anything else.''')\n",
    "        return chat.message(f'Who is a good special guest for a chapter on {chapter}?')\n",
    "    \n",
    "    def write_chapter(self, all_chapters, curr_chapter, chapter_idx, guest_chance=0.5, image_chance=0.0):\n",
    "        text = []\n",
    "        theme = self.theme if self.theme is not None else random.choice(list(self.themes.keys()))\n",
    "        theme_extra = self.theme_extra if self.theme_extra is not None else self.themes[theme]\n",
    "        system = f'''You are EBookGPT. You write chapters for textbooks on {self.topic} in the form of a {theme}.\n",
    "The {theme} must teach and be solved by {self.topic}{\" code. Make sure to include code samples.\" if self.include_code else \"\"}.\n",
    "{theme_extra}\n",
    "Write all responses in fancy github md format.\n",
    "Do not say responses to the user such as \"sure\".'''\n",
    "        if self.author_style is not None:\n",
    "            system += f' Write in the style of {self.author_style}.'\n",
    "        chat = Chat(system)\n",
    "        msg = f'You are writing a book about {self.topic}. Write the introduction to the chapter about {curr_chapter}. Write in fancy github md format.'\n",
    "        guest = None\n",
    "        if random.uniform(0, 1) <= guest_chance:\n",
    "            guest = self.get_special_guest_for_chapter(curr_chapter)\n",
    "            msg += f' Include special guest {guest} in this chapter.'\n",
    "        if all_chapters is not None:\n",
    "            if chapter_idx > 0:\n",
    "                msg += f' The last chapter was {all_chapters[chapter_idx-1]}.'\n",
    "        resp = chat.message(msg)\n",
    "        text.append(resp)\n",
    "        msg = f'Write the {theme} to the chapter teaching {curr_chapter}. Write in fancy github md format.'\n",
    "        if guest is not None:\n",
    "            msg += f' Include special guest {guest}'\n",
    "        resp = chat.message(msg)\n",
    "        text.append(resp)\n",
    "        if self.include_code:\n",
    "            msg = f'Explain the code used to resolve the {theme}. Write in fancy github md format.'\n",
    "        else:\n",
    "            msg = \"Write a conclusion for the above.\"\n",
    "        resp = chat.message(msg)\n",
    "        text.append(resp)\n",
    "        # Add image\n",
    "        if random.uniform(0, 1) <= image_chance:\n",
    "            img_prompt = chat.message(\"Write a DALL-E image generation prompt for this chapter in less than 1000 characters.\").replace(\"\\n\", \" \")[:1000]\n",
    "            img = Image.create(img_prompt)[\"url\"]\n",
    "            # [![name](link to image on GH)](link to your URL)\n",
    "            img_md = f\"![{img_prompt}]({img})\\n\\n\"\n",
    "            text.insert(0, img_md)\n",
    "        if self.include_next:\n",
    "            # Add link to next page\n",
    "            text.append(f'\\n\\n[Next Chapter]({chapter_idx+1:02d}_Chapter{chapter_idx+1:02d}.md)')\n",
    "        return '\\n'.join(text)\n",
    "\n",
    "    def write_book(self, book):\n",
    "        topic_normal = self.topic.translate(str.maketrans('', '', string.punctuation+' '))\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            outdir = f'{tmpdir}/{topic_normal}/'\n",
    "            os.makedirs(outdir, exist_ok=True)\n",
    "            for i, page in enumerate(book):\n",
    "                page_title = f'{i:02d}_Chapter{i:02d}.md'\n",
    "                with open(outdir+page_title, 'w') as f:\n",
    "                    f.write(page)\n",
    "            if 'gh' in self.publish:\n",
    "                self.publish_book_gh(outdir, topic_normal)\n",
    "            if 'amazon' in self.publish:\n",
    "                self.publish_book_kdp(outdir, topic_normal)\n",
    "\n",
    "    def publish_book_gh(self, bookdir, topic_normal):\n",
    "        gtoken = os.environ[\"GITHUB_TOKEN\"]\n",
    "        g = Github(gtoken)\n",
    "        org = g.get_organization(\"EBookGPT\")\n",
    "        org.create_repo(topic_normal)\n",
    "        subprocess.check_output(f\"git init\", cwd=bookdir, shell=True)\n",
    "        subprocess.check_output(f\"ln -sfn 00* README.md\", cwd=bookdir, shell=True)\n",
    "        subprocess.check_output(f\"git add .\", cwd=bookdir, shell=True)\n",
    "        subprocess.check_output(f\"git remote add origin git@github.com:EBookGPT/{topic_normal}.git\", cwd=bookdir, shell=True)\n",
    "        subprocess.check_output(f\"git commit -am 'Book' && git checkout -b main && git push origin main -u\", cwd=bookdir, shell=True)\n",
    "        \n",
    "    def publish_book_kdp(self, bookdir, topic_normal):\n",
    "        # Make epub\n",
    "        epub_file = tempfile.NamedTemporaryFile().name + '.epub'\n",
    "        convert_md_files_to_epub(topic_normal, self.topic, sorted([os.path.join(bookdir, x) for x in os.listdir(bookdir) if 'Chapter' in x and not x.startswith('00_')]), epub_file)\n",
    "        logger.info(f\"EPUB ready for upload: {epub_file}\")\n",
    "        return epub_file\n",
    "\n",
    "    def make_cover(self):\n",
    "        return Image.create(f\"\"\"{self.topic}, 4k Award Winning, Concept, Digital Art\"\"\")[\"b64_json\"]\n",
    "        \n",
    "    def run(self, nthreads=None):\n",
    "        # get chapters of book\n",
    "        chapters = self.get_chapters()\n",
    "        # Add conclusion/summary\n",
    "        if \"conclusion\" not in chapters[-1].lower():\n",
    "            chapters.append(f'{len(chapters)+1}. Conclusion of {\" \".join(chapters)}')\n",
    "        # Write chapters\n",
    "        text = [None] * len(chapters)\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=nthreads or len(chapters)) as thread_pool:\n",
    "            tasks = {}\n",
    "            for i, curr_chapter in enumerate(chapters):\n",
    "                tasks[thread_pool.submit(self.write_chapter, chapters, curr_chapter, i+1)] = i\n",
    "            for future in concurrent.futures.as_completed(tasks):\n",
    "                idx = tasks[future]\n",
    "                text[idx] = future.result()\n",
    "        book = ['Table Of Contents:\\n\\n'+'\\n'.join(chapters[:-1] + [f\"{len(chapters)}. Conclusion\"])] + text\n",
    "        try:\n",
    "            self.write_book(book)\n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "        return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3133f8d-34b7-4ea3-a3b9-5115995441ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-25 23:34:46 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:21 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:28 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:28 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:28 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:28 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:28 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:28 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:28 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:28 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:28 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:28 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:28 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:28 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:28 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:28 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:29 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:42 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:42 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:50 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:50 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:51 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:51 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:51 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:51 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:51 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:51 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:51 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:51 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:51 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:51 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:52 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:52 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:52 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:52 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:53 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:53 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:53 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:53 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:53 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:53 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:53 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:53 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:53 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:53 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:54 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:54 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:54 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:54 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:54 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:54 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:54 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:54 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:55 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:55 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:55 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:55 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:58 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:58 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:35:58 [info     ] received openai.Chat...\n",
      "2023-04-25 23:35:58 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:00 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:00 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:00 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:00 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:00 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:01 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:01 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:02 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:02 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:02 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:02 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:02 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:02 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:04 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:04 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:04 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:04 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:04 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:04 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:05 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:05 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:06 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:06 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:06 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:06 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:08 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:08 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:09 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:09 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:11 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:11 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:21 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:21 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:24 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:24 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:28 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:28 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:29 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:29 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:34 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:34 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:34 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:34 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:35 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:35 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:35 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:35 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:35 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:35 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:35 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:35 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:36 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:36 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:37 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:37 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:37 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:37 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:38 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:38 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:39 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:39 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:39 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:39 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:40 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:40 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:41 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:41 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:41 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:41 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:42 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:42 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:43 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:43 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:44 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:44 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:44 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:44 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:45 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:45 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:46 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:46 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:46 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:46 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:46 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:46 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:47 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:47 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:48 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:48 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:49 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:49 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:50 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:50 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:50 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:50 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:53 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:53 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:53 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:53 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:54 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:54 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:54 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:54 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:55 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:55 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:55 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:55 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:36:58 [info     ] received openai.Chat...\n",
      "2023-04-25 23:36:58 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:37:00 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:00 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:37:02 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:02 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:02 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:37:04 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:04 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:37:12 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:12 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:12 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:37:15 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:16 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:19 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:19 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:19 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:37:21 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:24 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:24 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:26 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:27 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:28 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:31 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:31 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:37:31 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:36 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:37 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:38 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:46 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:46 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:46 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:46 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:37:46 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:47 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:47 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:48 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:48 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:50 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:52 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:54 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:54 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:57 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:59 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:59 [info     ] received openai.Chat...\n",
      "2023-04-25 23:37:59 [info     ] received openai.Chat...\n",
      "2023-04-25 23:38:01 [info     ] received openai.Chat...\n",
      "2023-04-25 23:38:03 [info     ] received openai.Chat...\n",
      "2023-04-25 23:38:07 [info     ] received openai.Chat...\n",
      "2023-04-25 23:38:08 [info     ] received openai.Chat...\n",
      "2023-04-25 23:38:12 [info     ] received openai.Chat...\n",
      "2023-04-25 23:38:12 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:38:18 [info     ] received openai.Chat...\n",
      "2023-04-25 23:38:22 [info     ] received openai.Chat...\n",
      "2023-04-25 23:38:27 [info     ] received openai.Chat...\n",
      "2023-04-25 23:38:27 [info     ] Popping message: {'role': 'user', 'content': \"You are writing a book about Frankenstein's Neural Network: Deep Learning with Victor Frankenstein and TensorFlow. Write the introduction to the chapter about 15. Autoencoders and Generative Models. Write in fancy github md format. Include special guest Ian Goodfellow in this chapter. The last chapter was 15. Autoencoders and Generative Models.\"}\n",
      "2023-04-25 23:38:27 [info     ] Popping message: {'role': 'assistant', 'content': \"# Chapter 16: Ian Goodfellow and the Generative Models\\n\\nAs Victor further delves into the intricate world of neural networks, he realizes the potential of the generative models - a type of neural network that creates something new. What could be better than a machine that learns from past experiences and generates new content of its own?\\n\\nAs Victor continues his work on Generative models, he runs into troubles with the math behind these models, making him feel a pang of regret for avoiding math in his earlier days. But as luck would have it, he came across a book written by an expert in the field. It was none other than Ian Goodfellow!\\n\\nVictor was eager to learn from his distinct writing style and exhaustive knowledge on the topic. \\n\\nIan Goodfellow is the creator of the Generative Adversarial Network (GAN) which revolutionized the field of generative models by pitting two neural networks against each other, one generates the content while the other criticizes it. \\n\\nAs Victor delves deeper into Goodfellow’s work, he realizes that he can implement this in his own work. With his renewed enthusiasm, he decides to team up with Ian and adapt his adversarial techniques in his work. They work together to fine-tune the generator to produce content that is indiscernible from that produced by a human.\\n\\n```python\\n# Ian Goodfellow's GANs in TensorFlow\\nimport tensorflow as tf\\n\\n# Generator model\\ndef generator(z, output_dim=784, n_hidden=128, n_layer=2, reuse=False):\\n    with tf.variable_scope('generator', reuse=reuse):\\n        h = z    \\n        for layer in range(n_layer):\\n            h = tf.layers.dense(inputs=h, units=n_hidden, activation=tf.nn.relu)\\n        out = tf.layers.dense(inputs=h, units=output_dim, activation=tf.nn.tanh)\\n    return out\\n\\n# Critic model\\ndef critic(x, n_hidden=128, n_layer=2, reuse=False):\\n    with tf.variable_scope('critic', reuse=reuse):\\n        h = x\\n        for layer in range(n_layer):\\n            h = tf.layers.dense(inputs=h, units=n_hidden, activation=tf.nn.relu)\\n        out = tf.layers.dense(inputs=h, units=1, activation=None)\\n    return out\\n\\n# Inputs\\nx_real = tf.placeholder(tf.float32, shape=(None, 784))\\nz = tf.placeholder(tf.float32, shape=(None, 100))\\n\\n# Generator\\nG = generator(z)\\n\\n# Critic\\nD_real = critic(x_real)\\nD_fake = critic(G, reuse=True)\\n\\n# Losses\\nD_loss = tf.reduce_mean(D_fake) - tf.reduce_mean(D_real)\\nG_loss = -tf.reduce_mean(D_fake)\\n\\n# Optimizers\\nlearning_rate = 0.0001\\nbeta1 = 0.5\\ntheta_D = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='critic')\\ntheta_G = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\\nD_solver = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(D_loss, var_list=theta_D)\\nG_solver = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(G_loss, var_list=theta_G)\\n```\\n\\nFollowing Goodfellow's expert advice, Victor implements a GAN model in TensorFlow, with the goal of generating realistic images of dogs. But little did he know that his generator would backfire on him, and produce some grotesque images, causing him to rethink his data and models yet again!\\n\\nThe journey towards creating a perfect model is a challenging, winding road full of pitfalls and triumphs, but with the help of experts like Ian Goodfellow, it can be made easier.\"}\n",
      "2023-04-25 23:38:27 [info     ] requesting openai.Chat...\n",
      "2023-04-25 23:38:27 [info     ] received openai.Chat...\n",
      "2023-04-25 23:38:34 [info     ] received openai.Chat...\n",
      "2023-04-25 23:38:48 [info     ] received openai.Chat...\n",
      "2023-04-25 23:38:51 [info     ] received openai.Chat...\n",
      "2023-04-25 23:39:13 [info     ] received openai.Chat...\n",
      "2023-04-25 23:40:09 [info     ] received openai.Chat...\n",
      "2023-04-25 23:40:11 [info     ] EPUB ready for upload: /tmp/tmpgzrc0xw6.epub\n"
     ]
    }
   ],
   "source": [
    "topic = \"Frankenstein's Neural Network: Deep Learning with Victor Frankenstein and TensorFlow\"\n",
    "nchapters = 42\n",
    "include_code = True\n",
    "theme = \"Frankenstein story\"\n",
    "theme_extra = \"Make sure it's engaging, relevant, and educational. Incorporate references to published journals when relevant. Include entertaining facts or jokes when appropriate.\"\n",
    "author_style = \"Mary Shelley and Andrew Ng\"\n",
    "writer = EBookWriter(topic, nchapters, include_code, theme, theme_extra, author_style, include_next=False, publish={'amazon'})\n",
    "book = writer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d9aa854-458a-403e-939c-d8d759297278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Table Of Contents:\\n\\n1. Introduction to Artificial Intelligence and Neural Networks\\n2. Understanding TensorFlow and its Applications\\n3. Overview of Frankenstein's Neural Network\\n4. Victor Frankenstein's Contribution to Neural Networks\\n5. Historical Development of Deep Learning\\n6. Key Principles and Concepts in Deep Learning\\n7. Preprocessing of Data for Neural Networks\\n8. Designing a Neural Network Architecture\\n9. Supervised Learning with Frankenstein's Neural Network\\n10. Unsupervised Learning with Frankenstein's Neural Network\\n11. Reinforcement Learning with Frankenstein's Neural Network\\n12. Convolutional Neural Networks for Image Recognition\\n13. Natural Language Processing with Frankenstein's Neural Network\\n14. Recurrent Neural Networks for Sequential Data\\n15. Autoencoders and Generative Models\\n16. Deep Q-Learning for Reinforcement Learning\\n17. Transfer Learning and Fine-Tuning in Neural Networks\\n18. Hyperparameter Tuning and Optimization\\n19. Visualization and Interpretation of Neural Networks\\n20. Ethics and Governance in Artificial Intelligence\\n21. Advancements in Frankenstein's Neural Network\\n22. Building a Neural Network from Scratch\\n23. Training and Testing a Neural Network\\n24. Evaluating the Performance of a Neural Network\\n25. Gradient Descent and Backpropagation\\n26. Regularization and Dropout Techniques\\n27. Cross-Validation and Ensemble Learning\\n28. Understanding Bias and Variance in Neural Networks\\n29. Handling Overfitting and Underfitting in Neural Networks\\n30. Building a Neural Network for Time Series Forecasting\\n31. Deep Learning for Anomaly Detection\\n32. Artificial Intelligence for Medical Applications\\n33. Cognitive Computing and Artificial General Intelligence\\n34. Incorporating Capsule Networks into Neural Networks \\n35. Adversarial Attacks and Defenses in Neural Networks\\n36. Quantum Computing and Neural Networks\\n37. Computer Vision Applications of Frankenstein's Neural Network\\n38. Future Directions and Emerging Trends\\n39. Using Frankenstein's Neural Network as a Tool for Creativity\\n40. Applications and Case Studies of Frankenstein's Neural Network\\n41. Challenges and Limitations in the Implementation of Frankenstein's Neural Network\\n42. Conclusion\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f674d2dc-c82a-431b-857f-a48a5e87447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Github(os.environ[\"GITHUB_TOKEN\"])\n",
    "org = g.get_organization(\"EBookGPT\")\n",
    "# Create the repository under the organization\n",
    "repo = org.create_repo(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfeeb4a1-6520-4a6b-8005-de02e7d398f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Make sure it's engaging, relevant, and educational. Incorporate references to published journals when relevant. Include entertaining facts or jokes when appropriate.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EBookWriter.theme_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0a984f8-7d31-423b-b90b-9daff007b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cover Art from Dall-e/other as part of repo\n",
    "# TODO: Amazon KDP integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab8b78a4-d841-4b72-8a51-17bf62d7b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e = EBookWriter('Fine Tuning Large Language Models in PyTorch', include_code=True, nchapters=16)\n",
    "# book = e.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4220b5cd-a859-400b-bb9c-b58493eb19e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3323bbad-2f97-4cf7-b11a-008c432a3236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91b42690-8bfa-458f-8272-b10c3447c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdir = \"/home/jong/bookout/SurveyofLossFunctionsforDeepLearning\"\n",
    "convert_md_files_to_epub('id1234567890', \"Survey of Loss Functions for Deep Learning: A Sherlock Holmes Mystery\", sorted([os.path.join(bdir, x) for x in os.listdir(bdir) if 'Chapter' in x]), \"/home/jong/loss.epub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc57617-83c2-4059-97f7-02313956162b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6332164-2b7e-4140-8f22-abee42e6a293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
