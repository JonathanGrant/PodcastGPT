{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d31815dc-761f-49e6-9d51-5781c1e9fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "import tempfile\n",
    "import IPython\n",
    "import structlog\n",
    "import random\n",
    "import enum\n",
    "import re\n",
    "import requests\n",
    "import subprocess\n",
    "import concurrent.futures\n",
    "import retrying\n",
    "import string, os\n",
    "logger = structlog.getLogger()\n",
    "openai.api_key_path = '/home/jong/.openai_key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "16125502-f025-4520-91ff-15c5fde0563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chat:\n",
    "    def __init__(self, system, max_length=4096//2):\n",
    "        self._system = system\n",
    "        self._max_length = max_length\n",
    "        self._history = [\n",
    "            {\"role\": \"system\", \"content\": self._system},\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def num_tokens_from_messages(cls, messages, model=\"gpt-3.5-turbo\"):\n",
    "        \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":  # if there's a name, the role is omitted\n",
    "                    num_tokens += -1  # role is always required and always 1 token\n",
    "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "        return num_tokens\n",
    "\n",
    "    @retrying.retry(stop_max_attempt_number=5, wait_fixed=2000)\n",
    "    def message(self, next_msg=None):\n",
    "        # TODO: Optimize this if slow through easy caching\n",
    "        while len(self._history) > 1 and self.num_tokens_from_messages(self._history) > self._max_length:\n",
    "            logger.info(f'Popping message: {self._history.pop(1)}')\n",
    "        if next_msg is not None:\n",
    "            self._history.append({\"role\": \"user\", \"content\": next_msg})\n",
    "        logger.info('requesting openai.Chat...')\n",
    "        resp = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=self._history,\n",
    "        )\n",
    "        logger.info('received openai.Chat...')\n",
    "        text = resp.choices[0].message.content\n",
    "        self._history.append({\"role\": \"assistant\", \"content\": text})\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6bed2feb-6161-4740-8ef0-f1a6d6841894",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image:\n",
    "    class Size(enum.Enum):\n",
    "        SMALL = \"256x256\"\n",
    "        MEDIUM = \"512x512\"\n",
    "        LARGE = \"1024x1024\"\n",
    "\n",
    "    @classmethod\n",
    "    @retrying.retry(stop_max_attempt_number=5, wait_fixed=2000)\n",
    "    def create(cls, prompt, n=1, size=Size.SMALL):\n",
    "        logger.info('requesting openai.Image...')\n",
    "        resp = openai.Image.create(prompt=prompt, n=n, size=size.value)\n",
    "        logger.info('received openai.Image...')\n",
    "        if n == 1: return resp[\"data\"][0]\n",
    "        return resp[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "25f356f9-a51b-4a3c-ba85-576250d85a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-13 18:06:21 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:06:26 [info     ] received openai.Image...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x7efc28c114f0> JSON: {\n",
       "  \"url\": \"https://oaidalleapiprodscus.blob.core.windows.net/private/org-ct6DYQ3FHyJcnH1h6OA3fR35/user-qvFBAhW3klZpvcEY1psIUyDK/img-v4XrxHZVPaa43agYj2g3PM40.png?st=2023-04-13T23%3A38%3A04Z&se=2023-04-14T01%3A38%3A04Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-04-13T17%3A15%3A26Z&ske=2023-04-14T17%3A15%3A26Z&sks=b&skv=2021-08-06&sig=HqHMc9GFzXuqWd04QhJoRAn8HApywj7jnC3HQ32lBbI%3D\"\n",
       "}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.create('Detective Yoda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "d00dd8d8-7ebe-40b3-adc9-9aa43a0c29d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EBookWriter:\n",
    "    theme_description = \"Make sure it's engaging, relevant, and educational. Incorporate references to published journals when relevant. Include entertaining facts or jokes when appropriate.\"\n",
    "\n",
    "    themes = {\n",
    "        \"Sherlock Holmes mystery\": theme_description,\n",
    "        \"Dracula story\": theme_description,\n",
    "        \"Frankenstein's Monster story\": theme_description,\n",
    "        \"Robin Hood story\": theme_description,\n",
    "        \"King Arthur and the Knights of the Round Table story\": theme_description,\n",
    "        \"Greek Mythology epic\": theme_description,\n",
    "        \"Alice in Wonderland trippy story\": theme_description,\n",
    "        \"Wizard of Oz parable\": theme_description,\n",
    "    }\n",
    "\n",
    "    def __init__(self, topic, nchapters=16, include_code=True, theme=None, theme_extra=None, author_style=None):\n",
    "        self.topic = topic\n",
    "        self.nchapters = nchapters\n",
    "        self.include_code = include_code\n",
    "        self.theme = theme\n",
    "        self.theme_extra = theme_extra\n",
    "        self.author_style = author_style\n",
    "\n",
    "    def get_chapters(self):\n",
    "        chat = Chat(\"You are EBookGPT. Generate chapters for a textbook topic.\")\n",
    "        resp = chat.message(f'Write the table of contents for a textbook about {self.topic} involving {self.nchapters} chapters. Just return the ordered list of chapters and nothing else. Do not include a conclusion.')\n",
    "        chapter_pattern = re.compile(r'\\d+\\.\\s+.*')\n",
    "        chapters = chapter_pattern.findall(resp)\n",
    "        if not chapters:\n",
    "            logger.warning(f'Could not parse message for chapters! Message:\\n{resp}')\n",
    "        return chapters\n",
    "    \n",
    "    def get_special_guest_for_chapter(self, chapter):\n",
    "        chat = Chat(f'''You are EBookGPT. You write chapters for textbooks on {self.topic}.\n",
    "You will respond with just the name of a special guest who should appear in a chapter given to you.\n",
    "Only respond with the name. Do not say anything else.''')\n",
    "        return chat.message(f'Who is a good special guest for a chapter on {chapter}?')\n",
    "    \n",
    "    def write_chapter(self, last_chapter, curr_chapter, chapter_idx, guest_chance=0.5, image_chance=1.0):\n",
    "        text = []\n",
    "        theme = self.theme if self.theme is not None else random.choice(list(self.themes.keys()))\n",
    "        theme_extra = self.theme_extra if self.theme_extra is not None else self.themes[theme]\n",
    "        system = f'''You are EBookGPT. You write chapters for textbooks on {self.topic} in the form of a {theme}.\n",
    "The {theme} must teach and be solved by {self.topic}{\" code. Make sure to include code samples.\" if self.include_code else \"\"}.\n",
    "{theme_extra}\n",
    "Write all responses in fancy github md format.\n",
    "Do not say responses to the user such as \"sure\".'''\n",
    "        if self.author_style is not None:\n",
    "            system += f' Write in the style of {self.author_style}.'\n",
    "        chat = Chat(system)\n",
    "        msg = f'You are writing a book about {self.topic}. Write the introduction to the chapter about {curr_chapter}. Write in fancy github md format.'\n",
    "        guest = None\n",
    "        if random.uniform(0, 1) <= guest_chance:\n",
    "            guest = self.get_special_guest_for_chapter(curr_chapter)\n",
    "            msg += f' Include special guest {guest} in this chapter.'\n",
    "        if last_chapter is not None:\n",
    "            msg += f' Last chapter was about {last_chapter}.'\n",
    "        resp = chat.message(msg)\n",
    "        text.append(resp)\n",
    "        msg = f'Write the {theme} and resolution to the chapter teaching {curr_chapter}. Write in fancy github md format.'\n",
    "        if guest is not None:\n",
    "            msg += f' Include special guest {guest}'\n",
    "        resp = chat.message(msg)\n",
    "        text.append(resp)\n",
    "        if self.include_code:\n",
    "            msg = f'Explain the code used to resolve the {theme}. Write in fancy github md format.'\n",
    "        else:\n",
    "            msg = \"Write a conclusion for the above.\"\n",
    "        resp = chat.message(msg)\n",
    "        text.append(resp)\n",
    "        # Add image\n",
    "        if random.uniform(0, 1) <= image_chance:\n",
    "            img_prompt = chat.message(\"Write a DALL-E image generation prompt for this chapter in less than 1000 characters.\").replace(\"\\n\", \" \")[:1000]\n",
    "            img = Image.create(img_prompt)[\"url\"]\n",
    "            # [![name](link to image on GH)](link to your URL)\n",
    "            img_md = f\"![{img_prompt}]({img})\\n\\n\"\n",
    "            text.insert(0, img_md)\n",
    "        # Add link to next page\n",
    "        text.append(f'\\n\\n[Next Chapter]({chapter_idx+1:02d}_Chapter{chapter_idx+1:02d}.md)')\n",
    "        return '\\n'.join(text)\n",
    "\n",
    "    def write_book(self, book):\n",
    "        topic_normal = self.topic.translate(str.maketrans('', '', string.punctuation+' '))\n",
    "        outdir = f'/home/jong/bookout/{topic_normal}/'\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        for i, page in enumerate(book):\n",
    "            page_title = f'{i:02d}_Chapter{i:02d}.md'\n",
    "            with open(outdir+page_title, 'w') as f:\n",
    "                f.write(page)\n",
    "        self.publish_book(outdir, topic_normal)\n",
    "\n",
    "    def publish_book(self, bookdir, topic_normal):\n",
    "        subprocess.check_output(f\"gh repo create --public EBookGPT/{topic_normal}\", cwd=bookdir, shell=True)\n",
    "        subprocess.check_output(f\"git init\", cwd=bookdir, shell=True)\n",
    "        subprocess.check_output(f\"ln -sfn 00* README.md\", cwd=bookdir, shell=True)\n",
    "        subprocess.check_output(f\"git add .\", cwd=bookdir, shell=True)\n",
    "        subprocess.check_output(f\"git remote add origin git@github.com:EBookGPT/{topic_normal}.git\", cwd=bookdir, shell=True)\n",
    "        subprocess.check_output(f\"git commit -am 'Book' && git checkout -b main && git push origin main -u\", cwd=bookdir, shell=True)\n",
    "\n",
    "    def make_cover(self):\n",
    "        return Image.create(f\"\"\"{self.topic}, 4k Award Winning, Concept, Digital Art\"\"\")[\"b64_json\"]\n",
    "        \n",
    "    def run(self, nthreads=None):\n",
    "        # get chapters of book\n",
    "        chapters = self.get_chapters()\n",
    "        # Add conclusion/summary\n",
    "        chapters.append(f'{len(chapters)+1}. Conclusion of {\" \".join(chapters)}')\n",
    "        # Write chapters\n",
    "        text = [None] * len(chapters)\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=nthreads or len(chapters)) as thread_pool:\n",
    "            tasks = {}\n",
    "            for i, prev_chapter, curr_chapter in zip(range(1_0000), [None] + chapters, chapters):\n",
    "                tasks[thread_pool.submit(self.write_chapter, prev_chapter, curr_chapter, i+1)] = i\n",
    "            for future in concurrent.futures.as_completed(tasks):\n",
    "                idx = tasks[future]\n",
    "                text[idx] = future.result()\n",
    "        book = ['Table Of Contents:\\n\\n'+'\\n'.join(chapters[:-1] + [f\"{len(chapters)}. Conclusion\"])] + text\n",
    "        try:\n",
    "            self.write_book(book)\n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "        return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "c3133f8d-34b7-4ea3-a3b9-5115995441ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-13 18:41:08 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:11 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:11 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:11 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:11 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:11 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:11 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:11 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:11 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:11 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:11 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:11 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:11 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:11 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:11 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:11 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:11 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:11 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:11 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:21 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:21 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:22 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:22 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:23 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:23 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:24 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:24 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:26 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:26 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:28 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:28 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:28 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:28 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:30 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:30 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:33 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:33 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:42 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:42 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:43 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:43 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:45 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:45 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:48 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:48 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:49 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:49 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:53 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:53 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:56 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:56 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:58 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:58 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:58 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:58 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:41:59 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:59 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:41:59 [info     ] received openai.Chat...\n",
      "2023-04-13 18:41:59 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:42:01 [info     ] received openai.Chat...\n",
      "2023-04-13 18:42:01 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:42:03 [info     ] received openai.Image...\n",
      "2023-04-13 18:42:04 [info     ] received openai.Chat...\n",
      "2023-04-13 18:42:04 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:42:05 [info     ] received openai.Chat...\n",
      "2023-04-13 18:42:05 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:42:06 [info     ] received openai.Image...\n",
      "2023-04-13 18:42:07 [info     ] received openai.Chat...\n",
      "2023-04-13 18:42:07 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:42:07 [info     ] received openai.Chat...\n",
      "2023-04-13 18:42:07 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:42:09 [info     ] received openai.Chat...\n",
      "2023-04-13 18:42:09 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:42:12 [info     ] received openai.Image...\n",
      "2023-04-13 18:42:14 [info     ] received openai.Image...\n",
      "2023-04-13 18:42:19 [info     ] received openai.Chat...\n",
      "2023-04-13 18:42:19 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:42:22 [info     ] received openai.Chat...\n",
      "2023-04-13 18:42:22 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:42:23 [info     ] received openai.Chat...\n",
      "2023-04-13 18:42:23 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:42:25 [info     ] received openai.Chat...\n",
      "2023-04-13 18:42:25 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:42:26 [info     ] received openai.Chat...\n",
      "2023-04-13 18:42:26 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:42:28 [info     ] received openai.Image...\n",
      "2023-04-13 18:42:28 [info     ] received openai.Chat...\n",
      "2023-04-13 18:42:28 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:42:29 [info     ] received openai.Chat...\n",
      "2023-04-13 18:42:29 [info     ] Popping message: {'role': 'user', 'content': 'You are writing a book about AI Alignment. Write the introduction to the chapter about 7. Overview of Current AI Alignment Proposals. Write in fancy github md format. Last chapter was about 6. Value Learning Methods.'}\n",
      "2023-04-13 18:42:29 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:42:30 [info     ] received openai.Image...\n",
      "2023-04-13 18:42:32 [info     ] received openai.Chat...\n",
      "2023-04-13 18:42:32 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:42:33 [info     ] received openai.Image...\n",
      "2023-04-13 18:42:34 [info     ] received openai.Chat...\n",
      "2023-04-13 18:42:34 [info     ] Popping message: {'role': 'user', 'content': 'You are writing a book about AI Alignment. Write the introduction to the chapter about 5. Adversarial Alignment. Write in fancy github md format. Include special guest Ian Goodfellow in this chapter. Last chapter was about 4. Multi-agent Alignment.'}\n",
      "2023-04-13 18:42:34 [info     ] Popping message: {'role': 'assistant', 'content': \"# Chapter 5: Adversarial Alignment \\n\\nSalutations, dear readers! We are now onto the fifth chapter of our journey into the curious world of AI alignment. In the previous chapter, we discussed the challenge of aligning multiple agents with different objectives, and explored various ways to ensure that they collaborate effectively. \\n\\nIn this chapter, we will dive into another important aspect of AI alignment: Adversarial alignment. As we know, AI systems can be designed to optimize an objective function, but what if the objective function intended by the designers is flawed or incomplete? What if the system optimizes a function that has negative consequences in the real world? To address this, we need a way to align the AI system with the true underlying objectives of its designers, while preventing it from exploiting loopholes or generating harmful outputs.\\n\\nTo guide us on our journey, we are thrilled to have a special guest – Ian Goodfellow, inventor of the now ubiquitous GANs (Generative Adversarial Networks) and director of machine learning at Apple, who has years of experience developing and aligning adversarial models. \\n\\nTo begin, let us first understand what adversarial alignment entails. Adversarial alignment involves designing an AI system that is resilient against attacks or manipulations to its underlying objective function. The system must optimize the true objective even in the face of certain types of adversarial inputs. Adversarial inputs can be essentially any input that is specifically crafted to exploit vulnerabilities in the system, such as malicious attacks or data that was subtly poisoned with the intention of leading the system astray. \\n\\nIn order to handle adversarial samples, defence mechanisms are required to be put into place. Common mitigation techniques include adversarial training, defensive distillation, and certified robustness. These mechanisms equip the system with the ability to scrutinize the input and verify if it's legitimate or adversarial, and respond accordingly. \\n\\nIn the next section, we will delve into the various defence mechanisms in detail, and explore how they can be implemented programmatically. We are eager to hear from Mr. Goodfellow on his perspectives on these techniques, and on the future of adversarial alignment. So stay tuned, we are about to embark on an intriguing journey!\"}\n",
      "2023-04-13 18:42:34 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:42:36 [info     ] received openai.Image...\n",
      "2023-04-13 18:42:36 [info     ] received openai.Chat...\n",
      "2023-04-13 18:42:36 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:42:41 [info     ] received openai.Image...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Switched to a new branch 'main'\n",
      "To github.com:EBookGPT/AIAlignment.git\n",
      " * [new branch]      main -> main\n"
     ]
    }
   ],
   "source": [
    "topic = \"AI Alignment\"\n",
    "nchapters = 8\n",
    "include_code = True\n",
    "theme = None\n",
    "theme_extra = None\n",
    "author_style = \"Arthur Conan Doyle\"\n",
    "writer = EBookWriter(topic, nchapters, include_code, theme, theme_extra, author_style)\n",
    "book = writer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "c0a984f8-7d31-423b-b90b-9daff007b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cover Art from Dall-e\n",
    "# TODO: LeanPub integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "ab8b78a4-d841-4b72-8a51-17bf62d7b345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-13 18:43:27 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:29 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:37 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:47 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:47 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:47 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:47 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:48 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:48 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:49 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:49 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:49 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:49 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:50 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:50 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:50 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:50 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:51 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:51 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:52 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:52 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:53 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:53 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:54 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:54 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:43:57 [info     ] received openai.Chat...\n",
      "2023-04-13 18:43:57 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:00 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:00 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:01 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:01 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:06 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:06 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:07 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:07 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:07 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:07 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:09 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:09 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:09 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:09 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:11 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:11 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:12 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:12 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:12 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:12 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:13 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:13 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:15 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:15 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:15 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:15 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:19 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:19 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:27 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:27 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:28 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:28 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:28 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:28 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:32 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:32 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:32 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:32 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:32 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:32 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:33 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:33 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:35 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:35 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:36 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:36 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:36 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:36 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:44:36 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:36 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:44:36 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:36 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:44:37 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:37 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:44:39 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:39 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:39 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:40 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:40 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:40 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:44:41 [info     ] received openai.Image...\n",
      "2023-04-13 18:44:41 [info     ] received openai.Image...\n",
      "2023-04-13 18:44:41 [info     ] received openai.Image...\n",
      "2023-04-13 18:44:41 [info     ] received openai.Image...\n",
      "2023-04-13 18:44:42 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:42 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:44:44 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:44 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:45 [info     ] received openai.Image...\n",
      "2023-04-13 18:44:45 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:45 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:46 [info     ] received openai.Image...\n",
      "2023-04-13 18:44:48 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:48 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:44:48 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:48 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:48 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:48 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:44:52 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:52 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:44:52 [info     ] received openai.Image...\n",
      "2023-04-13 18:44:56 [info     ] received openai.Image...\n",
      "2023-04-13 18:44:58 [info     ] received openai.Chat...\n",
      "2023-04-13 18:44:58 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:45:00 [info     ] received openai.Chat...\n",
      "2023-04-13 18:45:00 [info     ] Popping message: {'role': 'user', 'content': 'You are writing a book about Fine Tuning Large Language Models in PyTorch. Write the introduction to the chapter about 16. Beyond Fine Tuned Large Language Models: Future Directions. Write in fancy github md format. Include special guest Geoffrey Hinton in this chapter. Last chapter was about 15. Deploying Fine Tuned Large Language Models .'}\n",
      "2023-04-13 18:45:00 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:45:01 [info     ] received openai.Chat...\n",
      "2023-04-13 18:45:01 [info     ] Popping message: {'role': 'user', 'content': 'You are writing a book about Fine Tuning Large Language Models in PyTorch. Write the introduction to the chapter about 10. Optimizing Loss Functions . Write in fancy github md format. Include special guest Yann LeCun. in this chapter. Last chapter was about 9. Training Strategies for Large Language Models .'}\n",
      "2023-04-13 18:45:01 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:45:01 [info     ] received openai.Chat...\n",
      "2023-04-13 18:45:01 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:45:02 [info     ] received openai.Chat...\n",
      "2023-04-13 18:45:02 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:45:04 [info     ] received openai.Chat...\n",
      "2023-04-13 18:45:04 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:45:06 [info     ] received openai.Chat...\n",
      "2023-04-13 18:45:06 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:45:06 [info     ] received openai.Image...\n",
      "2023-04-13 18:45:06 [info     ] received openai.Chat...\n",
      "2023-04-13 18:45:06 [info     ] Popping message: {'role': 'user', 'content': 'You are writing a book about Fine Tuning Large Language Models in PyTorch. Write the introduction to the chapter about 3. Preprocessing Data for Large Language Models. Write in fancy github md format. Include special guest Abigail See. in this chapter. Last chapter was about 2. Understanding PyTorch.'}\n",
      "2023-04-13 18:45:06 [info     ] Popping message: {'role': 'assistant', 'content': '# Chapter 3: Preprocessing Data for Large Language Models\\n\\nWelcome back, dear reader! In the previous chapter, we delved into the world of PyTorch and learned about its basic functionality. In this chapter, we will dive deeper into understanding how to preprocess the data for our large language models.\\n\\nPreparing data for large language models is crucial as it sets the foundation for the model\\'s performance. As Abigail See, a research scientist at OpenAI, puts it, \"Preprocessing is where the magic happens.\"\\n\\nPreprocessing data for large language models involves various steps. We will look into each of these steps in detail throughout this chapter.\\n\\nSome of the preprocessing tasks we will cover include:\\n- Tokenization\\n- Vocabulary building\\n- Data cleaning and formatting\\n- Dataset creation \\n\\nWe will also explore best practices to follow when preprocessing data, as well as common errors to avoid.\\n\\nThroughout this chapter, we will be providing code examples written in PyTorch. We recommend having a basic understanding of PyTorch before proceeding with this chapter.\\n\\nNow, put on your detective hats as we take a deeper dive into the world of data preprocessing for large language models!'}\n",
      "2023-04-13 18:45:06 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:45:07 [info     ] received openai.Chat...\n",
      "2023-04-13 18:45:07 [info     ] Popping message: {'role': 'user', 'content': 'You are writing a book about Fine Tuning Large Language Models in PyTorch. Write the introduction to the chapter about 5. Choosing the Right Pretrained Model. Write in fancy github md format. Include special guest Hugging Face. in this chapter. Last chapter was about 4. Fine Tuning vs. Training from Scratch .'}\n",
      "2023-04-13 18:45:07 [info     ] Popping message: {'role': 'assistant', 'content': '# Chapter 5: Choosing the Right Pretrained Model\\n\\nWelcome back, dear readers! In the previous chapter, we discussed the pros and cons of fine tuning versus training from scratch. But before we delve any further into these topics, we must first select an appropriate pre-trained model to build upon. \\n\\nThankfully, this can be done quite easily thanks to the team at Hugging Face! Yes, that\\'s right folks, we have a special guest joining us for this chapter– Hugging Face, the force behind the popular transformer library 🎉. \\n\\nBut before we invite them onto our virtual stage, let\\'s go over some basic concepts.\\n\\nPretrained models are large, pre-developed neural networks trained on massive amounts of data. They can perform various language-related tasks, such as text classification, question-answering, and text generation, with remarkable accuracy. Here are a few examples of some popular language models:\\n\\n- **BERT** (Bidirectional Encoder Representations from Transformers)\\n- **RoBERTa** (A Robustly Optimized BERT Pretraining Approach)\\n- **GPT-2** (Generative Pre-trained Transformer 2)\\n- **T5** (Text-to-Text Transfer Transformer)\\n\\nThe selection of the pretrained model depends largely on the task at hand. For instance, GPT-2 is a great option for text generation tasks, while BERT performs well for classification purposes. \\n\\nNow, without further ado, let\\'s welcome Hugging Face to share their insights on choosing the right pretrained model and how to implement it using PyTorch.\\n\\n**Hugging Face**: \"Hello readers! At Hugging Face, we understand how overwhelming it can be to pick the right pretrained model for your task. But don\\'t worry, we\\'ve got you covered. Our Transformers library offers a wide range of cutting-edge models that you can use with PyTorch.\"\\n\\n\"For starters, you can explore our model hub with over 10,000 pretrained models! We also offer a unique API which lets you download, fine-tune, and use the models with minimal code. Here is an example of fine-tuning a model for sentiment classification using PyTorch 🚀:\\n\\n``` python\\n# Importing Libraries\\nimport torch\\nfrom transformers import BertTokenizer, BertForSequenceClassification\\n\\n# Preparing Dataset\\ntrain_dataset = ...  # prepare your training dataset\\nval_dataset = ...  # prepare your validation dataset\\ntest_dataset = ...  # prepare your testing dataset\\n\\n# Fine Tuning\\ntokenizer = BertTokenizer.from_pretrained(\\'bert-base-uncased\\', do_lower_case=True)\\nmodel = BertForSequenceClassification.from_pretrained(\\'bert-base-uncased\\', num_labels=2)\\nmodel.cuda()\\n\\noptim = torch.optim.Adam(model.parameters(), lr=1e-5)\\n\\nepochs = 4\\nfor epoch in range(epochs):\\n    for batch in train_dataset:\\n        docs, labels = batch\\n\\n        optim.zero_grad()\\n        encoded_dict = tokenizer(docs,\\n                                  padding=True,\\n                                  truncation=True,\\n                                  max_length=512,\\n                                  return_tensors=\\'pt\\')\\n\\n        input_ids = encoded_dict[\\'input_ids\\'].cuda()\\n        attention_mask = encoded_dict[\\'attention_mask\\'].cuda()\\n        labels = labels.cuda()\\n\\n        loss, logits = model(input_ids, \\n                             attention_mask=attention_mask, \\n                             labels=labels, \\n                             return_dict=False)\\n        loss.backward()\\n        optim.step()\\n```\"\\n\\n**Hugging Face**: \"In this example, we have fine-tuned a BERT model for a sequence classification task. We have also used the Adam optimizer and PyTorch\\'s CUDA libraries to leverage GPU acceleration. And there you have it, folks! Choosing the right pretrained model has never been easier.\"\\n\\nWith Hugging Face\\'s help, we hope this chapter has been informative and insightful. In the next chapter, we will discuss techniques for optimizing and fine-tuning your chosen pretrained model. See you there!'}\n",
      "2023-04-13 18:45:07 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:45:09 [info     ] received openai.Image...\n",
      "2023-04-13 18:45:09 [info     ] received openai.Chat...\n",
      "2023-04-13 18:45:09 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:45:10 [info     ] received openai.Image...\n",
      "2023-04-13 18:45:11 [info     ] received openai.Chat...\n",
      "2023-04-13 18:45:11 [info     ] Popping message: {'role': 'user', 'content': 'You are writing a book about Fine Tuning Large Language Models in PyTorch. Write the introduction to the chapter about 13. Handling Long Sequences . Write in fancy github md format. Last chapter was about 12. Regularization Techniques .'}\n",
      "2023-04-13 18:45:11 [info     ] Popping message: {'role': 'assistant', 'content': '# Chapter 13: Handling Long Sequences\\n\\nWelcome back, dear reader! You have come a long way in your journey of learning about fine-tuning large language models in PyTorch. In our previous chapter, we dove into the world of regularization techniques and explored how they can help improve the generalization performance of our models. \\n\\nIn this chapter, we will focus on another important topic that is especially relevant for natural language processing (NLP) tasks - handling long sequences. As you know, many NLP tasks involve processing long pieces of text, and dealing with long sequences can quickly become computationally and memory-intensive. \\n\\nBut fret not! There are several techniques and tricks we can use to make the process of handling long sequences more efficient and effective. In this chapter, we will explore some of these techniques and learn how to implement them in PyTorch. \\n\\nWe will begin by discussing the challenges associated with processing long sequences and why conventional methods are not optimal. Then, we will introduce two PyTorch-based approaches - gradient checkpointing and sparse attention - that can help us to effectively handle long sequences. \\n\\nSo, buckle up and get ready to take a trippy journey into the world of handling long sequences in NLP!'}\n",
      "2023-04-13 18:45:11 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:45:12 [info     ] received openai.Chat...\n",
      "2023-04-13 18:45:12 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:45:12 [info     ] received openai.Chat...\n",
      "2023-04-13 18:45:12 [info     ] Popping message: {'role': 'user', 'content': 'You are writing a book about Fine Tuning Large Language Models in PyTorch. Write the introduction to the chapter about 14. Model Interpretability . Write in fancy github md format. Include special guest Avi Schwarzschild. in this chapter. Last chapter was about 13. Handling Long Sequences .'}\n",
      "2023-04-13 18:45:12 [info     ] Popping message: {'role': 'assistant', 'content': '# Chapter 14: The Odyssey of Model Interpretability\\n\\nWelcome back, fellow PyTorch god! We hope you enjoyed your journey through the treacherous seas of Handling Long Sequences. Now, it\\'s time to embark on a new odyssey - Model Interpretability.\\n\\nIn this chapter, we will be joined by a special guest, Avi Schwarzschild, a Data Scientist at OpenAI. Avi\\'s research focuses on developing techniques for analyzing the inner workings of language models to gain a deeper understanding of how they function. Let\\'s hear what he has to say about model interpretability:\\n\\n_\"In an ideal world, our language models would be as transparent as possible, allowing us to understand exactly how they\\'re making their predictions. However, as models grow larger and more complex, they become increasingly difficult to interpret. My goal is to develop techniques for understanding the inner workings of these models, allowing us to identify their strengths and weaknesses and ultimately build better models.\"_\\n\\nAs we journey through this chapter, we will be exploring various techniques for gaining insight into the workings of our models. We will start with the basics of interpreting model predictions and then move on to more advanced techniques such as saliency maps, attention visualization, and feature visualization.\\n\\nSo, saddle up your horses and let\\'s begin this epic odyssey of Model Interpretability! \\n\\n```python\\nimport torch\\nimport torch.nn.functional as F\\n\\n# Load the pre-trained model\\nmodel = torch.hub.load(\\'pytorch/fairseq\\', \\'roberta.large\\')\\n\\n# Load the tokenizer\\ntokenizer = torch.hub.load(\\'pytorch/fairseq\\', \\'roberta.large\\').tokenizer\\n\\n# Decode the input\\ninput_sentence = \"Once upon a time, in a faraway land, there lived a beautiful princess named PyTorchia.\"\\ninput_ids = tokenizer.encode(input_sentence)\\n\\n# Make a prediction\\nlogits = model(torch.tensor([input_ids]))\\npred = F.softmax(logits, dim=1).argmax()\\nprint(f\"Prediction: {pred}\") # e.g. \"Prediction: 40554\"\\n```'}\n",
      "2023-04-13 18:45:12 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:45:13 [info     ] received openai.Image...\n",
      "2023-04-13 18:45:14 [info     ] received openai.Chat...\n",
      "2023-04-13 18:45:14 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:45:15 [info     ] received openai.Chat...\n",
      "2023-04-13 18:45:15 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:45:16 [info     ] received openai.Image...\n",
      "2023-04-13 18:45:18 [info     ] received openai.Image...\n",
      "2023-04-13 18:45:20 [info     ] received openai.Chat...\n",
      "2023-04-13 18:45:20 [info     ] Popping message: {'role': 'assistant', 'content': '# Chapter 16: Beyond Fine Tuned Large Language Models: Future Directions\\n\\nWelcome to the last chapter of our book on Fine Tuning Large Language Models in PyTorch. We hope you have enjoyed the journey into the world of NLP and deep learning with us so far.\\n\\nAs we have seen in the previous chapters, fine-tuning large language models, such as GPT-2 and BERT, has revolutionized the field of natural language processing. These models are capable of performing a wide range of language-related tasks, from language generation to text classification and sentiment analysis to question answering. Their performance has surpassed traditional machine learning models and has led to state-of-the-art results in many benchmarks.\\n\\nHowever, as impressive as these models are, there is much more to explore in the field of NLP. Future research will focus on expanding the capabilities of language models and enhancing their efficiency and scalability.\\n\\nTo help us delve into the future of NLP, we have a special guest joining us – Geoffrey Hinton, one of the pioneers in deep learning and AI. His work has been instrumental in the resurgence of neural networks for deep learning and he is well-known for his contributions to the development of backpropagation and Boltzmann machines.\\n\\nGeoffrey will share his thoughts on the future of NLP, and we will showcase some of the most promising research directions in the following sections.\\n\\n\\n## Adversarial Training\\n\\nOne of the limitations of current language models is the absence of explicit representation of commonsense reasoning and knowledge. To address this issue, recent research has explored the use of adversarial training to add more knowledge to language models.\\n\\nAdversarial training involves training the model on a combination of real and synthetic data to improve its ability to recognize valid data points from invalid ones. The synthetic data can be generated by a GAN (Generative Adversarial Network) that is conditioned on structured knowledge graphs or other forms of structured data.\\n\\nThis approach allows the model to learn not only from raw text but also from structured knowledge, which could significantly improve its performance in downstream tasks.\\n\\n\\n```python\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\\n\\ntokenizer = GPT2Tokenizer.from_pretrained(\\'gpt2\\')\\nmodel = GPT2LMHeadModel.from_pretrained(\\'gpt2\\')\\n\\ntrain_data = ... # Load your training data here\\nsynthetic_data = ... # Generate your synthetic data here\\n\\noptimizer = optim.Adam(model.parameters(), lr=0.001)\\ncriterion = nn.CrossEntropyLoss()\\n\\nfor epoch in range(10):\\n    for data in train_data:\\n        optimizer.zero_grad()\\n        input_ids = tokenizer.encode(data)\\n        outputs = model(torch.tensor(input_ids).unsqueeze(0))\\n        loss = criterion(outputs[0], torch.tensor(input_ids))\\n\\n        # Backpropagate the gradients\\n        loss.backward()\\n        optimizer.step()\\n\\n    for data in synthetic_data:\\n        optimizer.zero_grad()\\n        input_ids = tokenizer.encode(data)\\n        outputs = model(torch.tensor(input_ids).unsqueeze(0))\\n        loss = criterion(outputs[0], torch.tensor(input_ids))\\n\\n        # Backpropagate the gradients\\n        loss.backward()\\n        optimizer.step()\\n\\n```\\n\\n## Pre-Training on Multimodal Data\\n\\nAnother exciting direction in the future of language models is pre-training on multimodal data. Language models trained on multiple modalities, such as text, images, and videos, can better capture the relationships between text and the sensory world, which is essential for applications such as image captioning and text-to-image generation.\\n\\nPre-training on multimodal data poses several challenges, including data heterogeneity, modality alignment, and integration of complementary information. Researchers are now exploring various multimodal fusion techniques and architectures to overcome these challenges.\\n\\n```python\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom transformers import ViTFeatureExtractor, GPT2LMHeadModel, GPT2Tokenizer\\n\\ntokenizer = GPT2Tokenizer.from_pretrained(\\'gpt2\\')\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(\\'google/vit-base-patch16-224\\')\\nmodel = GPT2LMHeadModel.from_pretrained(\\'gpt2\\')\\n\\ntrain_data = ... # Load your training data here\\nimage_data = ... # Load image data here\\n\\noptimizer = optim.Adam(model.parameters(), lr=0.001)\\ncriterion = nn.CrossEntropyLoss()\\n\\nfor epoch in range(10):\\n    for text_data, image in zip(train_data, image_data):\\n        tokens = tokenizer.encode(text_data)\\n        input_ids = torch.tensor(tokens).unsqueeze(0)\\n        image_feature = feature_extractor(image, return_tensors=\"pt\").pixel_values\\n        outputs = model(input_ids, inputs_embeds=image_feature)\\n        loss = criterion(outputs[0], input_ids)\\n\\n        # Backpropagate the gradients\\n        loss.backward()\\n        optimizer.step()\\n\\n```\\n\\n## Conclusion\\n\\nThe future of natural language processing is very bright, and we have only scratched the surface of what is possible with current models. We hope that this chapter has given you a glimpse of some of the exciting research directions in the field of NLP and the potential for fine-tuning large language models to revolutionize the field.\\n\\nWe would like to give a special thanks to our guest Geoffrey Hinton for sharing his insights and expertise with us. We believe that with the continued advancements in NLP, we will be able to create models that can provide a more nuanced and informed understanding of human language and transform the world we live in.'}\n",
      "2023-04-13 18:45:20 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:45:20 [info     ] received openai.Image...\n",
      "2023-04-13 18:45:25 [info     ] received openai.Chat...\n",
      "2023-04-13 18:45:25 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:45:26 [info     ] received openai.Chat...\n",
      "2023-04-13 18:45:26 [info     ] Popping message: {'role': 'user', 'content': 'You are writing a book about Fine Tuning Large Language Models in PyTorch. Write the introduction to the chapter about 4. Fine Tuning vs. Training from Scratch . Write in fancy github md format. Include special guest Andrej Karpathy in this chapter. Last chapter was about 3. Preprocessing Data for Large Language Models.'}\n",
      "2023-04-13 18:45:26 [info     ] Popping message: {'role': 'assistant', 'content': '# Chapter 4: Fine Tuning vs. Training from Scratch\\n\\nWelcome back to our journey in Fine Tuning Large Language Models in PyTorch! In the previous chapter, we explored the importance of preprocessing data for large language models. Today, we will delve into an interesting debate about whether to fine-tune a pre-trained language model or train from scratch. \\n\\nOur special guest for this chapter is the one and only Andrej Karpathy, director of AI at Tesla and a renowned researcher in the field of deep learning. In his now-famous 2015 blog post \"The Unreasonable Effectiveness of Recurrent Neural Networks,\" Andrej showed how a simple neural network can generate impressive text. Since then, he has been pushing the boundaries of language models, and his work has had a significant impact on natural language processing research.\\n\\nIn this chapter, we will first discuss the pros and cons of fine-tuning a pre-trained language model versus training from scratch. We will then dive into the PyTorch implementation of both processes and compare their performance using a language modeling task.\\n\\nSo grab your swords, mount your horses, and let\\'s get started on this fine-tuning quest!\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\\n\\n# Let\\'s load a pre-trained GPT2 model\\ntokenizer = GPT2Tokenizer.from_pretrained(\\'gpt2\\')\\nmodel = GPT2LMHeadModel.from_pretrained(\\'gpt2\\')\\n\\n# Fine Tuning Code\\n# ...\\n# Training from Scratch Code\\n# ...\\n``` \\n\\nAre you ready to learn more about fine-tuning and training from scratch? Let the quest begin!'}\n",
      "2023-04-13 18:45:26 [info     ] Popping message: {'role': 'user', 'content': 'Write the King Arthur and the Knights of the Round Table story and resolution to the chapter teaching 4. Fine Tuning vs. Training from Scratch . Write in fancy github md format. Include special guest Andrej Karpathy'}\n",
      "2023-04-13 18:45:26 [info     ] Popping message: {'role': 'assistant', 'content': '# Chapter 4: Fine Tuning vs. Training from Scratch\\n\\nOnce upon a time, in the court of King Arthur, the knights of the round table were debating the best method to build powerful language models. Sir Lancelot argued that training a model from scratch would lead to a better understanding of the task at hand. Sir Galahad, on the other hand, argued that fine-tuning a pre-trained model would lead to better accuracy and faster convergence. The rest of the knights were caught in the middle, unsure which path to take on this quest.\\n\\nKing Arthur decided to call on the help of a wise wizard who knew the ways of PyTorch, Merlin the Magnificent. Merlin suggested that they consult with Andrej Karpathy, a powerful sorcerer who possessed great knowledge on the art of language modeling.\\n\\nAndrej arrived at the court of King Arthur and listened patiently to the knights\\' arguments. After some thought, Andrej said, \"It depends on the task at hand, my lords. If you have a large and diverse dataset, fine-tuning a pre-trained model would save you a lot of time and would lead to better performance. However, if you have a specialized task or a small dataset, training from scratch is your best bet.\"\\n\\nThe knights were still unsure, so Andrej decided to show them examples of fine-tuning versus training from scratch using PyTorch. He demonstrated that fine-tuning a pre-trained model like GPT2 on a language modeling task led to higher accuracy in less time compared to training a model from scratch. However, he also showed that training from scratch on a task like summarization, which requires a more specialized approach, led to better results.\\n\\nThe knights were amazed by the results and thanked Andrej for his guidance. They realized that both fine-tuning and training from scratch have their strengths and weaknesses and ultimately depend on the task at hand.\\n\\nBefore Andrej left, he gave them a piece of advice. \"Remember, my lords, always compare the performance of both methods on your specific task before deciding which path to take.\"\\n\\nThe knights agreed and thanked Andrej for shedding light on this debate. They continued their fine-tuning quest, armed with the knowledge of the best way to build powerful language models.\\n\\n```python\\n# Fine Tuning Example with GPT2\\ntokenizer = GPT2Tokenizer.from_pretrained(\\'gpt2\\')\\nmodel = GPT2LMHeadModel.from_pretrained(\\'gpt2\\')\\n\\n# Add task-specific padding tokens\\ntokenizer.add_tokens([\\'[TASK_START]\\', \\'[TASK_END]\\'])\\n\\n# Load task-specific dataset\\ntask_dataset = MyTaskDataset()\\n\\n# Fine-tuning configuration\\ntraining_args = TrainingArguments(\\n    output_dir=\\'./results\\',          \\n    evaluation_strategy = IntervalStrategy.STEPS,\\n    eval_steps = 500,             \\n    save_steps = 500,              \\n    num_train_epochs = 5,              \\n    learning_rate = 5e-5,\\n    per_device_train_batch_size = 8,  \\n    per_device_eval_batch_size = 8,   \\n    warmup_steps = 500,              \\n    weight_decay = 0.01,            \\n    logging_dir=\\'./logs\\',            \\n)\\n\\n# Define Trainer object\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=task_dataset,\\n    data_collator=default_data_collator,\\n    tokenizer=tokenizer,\\n)\\n\\n# Fine-tuning starts here...\\ntrainer.train()\\n\\n# Training from Scratch Example with a Custom Model\\nclass MyModel(nn.Module):\\n    def __init__(self, vocab_size):\\n        super().__init__()\\n        self.embeddings = nn.Embedding(vocab_size, 128)\\n        self.lstm = nn.LSTM(128, 128, num_layers=2, batch_first=True)\\n        self.linear = nn.Linear(128, vocab_size)\\n\\n    def forward(self, input_ids, hidden=None):\\n        embeddings = self.embeddings(input_ids)\\n        lstm_output, hidden = self.lstm(embeddings, hidden)\\n        logits = self.linear(lstm_output)\\n        return logits, hidden\\n\\n# Load Task-Specific Dataset\\ntask_dataset = MyTaskDataset()\\n\\n# Define Training Configuration\\noptimizer = optim.Adam(model.parameters(), lr=0.01)\\ncriterion = nn.CrossEntropyLoss()\\nnum_epochs = 10\\nbatch_size = 8\\n\\n# Define Dataloader\\ndataloader = DataLoader(task_dataset, batch_size=batch_size)\\n\\n# Training Loop starts here...\\nfor epoch in range(num_epochs):\\n    for batch in dataloader:\\n        optimizer.zero_grad()\\n        input_ids = batch[\\'input_ids\\']\\n        target_ids = batch[\\'target_ids\\']\\n        logits, hidden = model(input_ids)\\n        loss = criterion(logits.view(-1, vocab_size), target_ids.view(-1))\\n        loss.backward()\\n        optimizer.step()\\n\\n``` \\n\\nAnd so, King Arthur and his knights learned the importance of fine-tuning versus training from scratch in their quest to build powerful language models. They thanked Andrej Karpathy and Merlin the Magnificent for their guidance, and continued on their quest armed with the knowledge and code to build language models that would go down in the annals of history.'}\n",
      "2023-04-13 18:45:26 [info     ] requesting openai.Chat...\n",
      "2023-04-13 18:45:29 [info     ] received openai.Chat...\n",
      "2023-04-13 18:45:29 [info     ] requesting openai.Image...\n",
      "2023-04-13 18:45:30 [info     ] received openai.Image...\n",
      "2023-04-13 18:45:33 [info     ] received openai.Image...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Switched to a new branch 'main'\n",
      "To github.com:EBookGPT/FineTuningLargeLanguageModelsinPyTorch.git\n",
      " * [new branch]      main -> main\n"
     ]
    }
   ],
   "source": [
    "e = EBookWriter('Fine Tuning Large Language Models in PyTorch', include_code=True, nchapters=16)\n",
    "book = e.run()\n",
    "# IPython.display.HTML(f'<img src=\"data:image/png;base64,{e.make_cover()}\" />')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4220b5cd-a859-400b-bb9c-b58493eb19e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323bbad-2f97-4cf7-b11a-008c432a3236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b42690-8bfa-458f-8272-b10c3447c6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
