{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "085ba2ee-6274-4e84-8ba1-a12b2fc3170b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ChatPodcastGPT import *\n",
    "import collections\n",
    "import concurrent.futures\n",
    "import os\n",
    "import feedparser\n",
    "import logging\n",
    "import re\n",
    "import tempfile\n",
    "from PyPDF2 import PdfReader\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import retrying\n",
    "import random\n",
    "from IPython.display import Audio\n",
    "import datetime\n",
    "\n",
    "MAX_TOKENS = 4096\n",
    "JOIN_NUM_DEFAULT = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65459282-8967-442a-88d9-3812b3e8c31a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PDFEpisode(Episode):\n",
    "    PDFPart = collections.namedtuple('PDFPart', 'title text')\n",
    "\n",
    "    def __init__(self, title, model='gpt-3.5-turbo-16k', **kwargs):\n",
    "        self.title = title\n",
    "        self.model = model\n",
    "        self.topic = kwargs.pop('topic', self.title) or self.title\n",
    "        self._kwargs = kwargs\n",
    "        self.join_num = JOIN_NUM_DEFAULT\n",
    "        if 'podcast_args' in self._kwargs: self._kwargs.pop('podcast_args')\n",
    "        super().__init__(topic=self.topic, **kwargs)\n",
    "\n",
    "    def parse_pdf(self, file):\n",
    "        \"\"\"Parse a PDF and extract the text.\"\"\"\n",
    "        with open(file, \"rb\") as f:\n",
    "            pdf = PdfReader(f)\n",
    "            return ''.join(page.extract_text() for page in pdf.pages)\n",
    "\n",
    "    def split_into_parts(self, text, max_tokens=MAX_TOKENS // 2):\n",
    "        \"\"\"Split the text into parts based on titles and tokens.\"\"\"\n",
    "        lines = text.split(\"\\n\")\n",
    "        parts = []\n",
    "        current_part = []\n",
    "        current_title = 'Abstract'\n",
    "        for line in lines:\n",
    "            if re.match(r'\\d+\\s[A-Za-z]', line):\n",
    "                if current_part:\n",
    "                    parts.append(self.PDFPart(current_title, \"\\n\".join(current_part)))\n",
    "                current_title = line\n",
    "                current_part = []\n",
    "            else:\n",
    "                current_part.append(line)\n",
    "\n",
    "            while Chat.num_tokens_from_text('\\n'.join(current_part)) > max_tokens:\n",
    "                part_text = '\\n'.join(current_part)\n",
    "                shortened_part, current_part = part_text[:max_tokens * 2], [part_text[max_tokens * 2:]]\n",
    "                parts.append(self.PDFPart(current_title, shortened_part))\n",
    "\n",
    "        if current_part:\n",
    "            parts.append(self.PDFPart(current_title, \"\\n\".join(current_part)))\n",
    "        return parts\n",
    "\n",
    "    def process_pdf(self, pdf_path):\n",
    "        text = self.parse_pdf(pdf_path)\n",
    "        parts = self.split_into_parts(text)\n",
    "        return parts\n",
    "    \n",
    "    def write_one_part(self, chat_msg):\n",
    "        chat = PodcastChat(**{**self._kwargs, 'topic': self.title})\n",
    "        msg = chat.step(msg=chat_msg, model=self.model, skip_aud=True, frequency_penalty=0.5)\n",
    "        return msg\n",
    "\n",
    "    @retrying.retry(stop_max_attempt_number=3, wait_fixed=2000)\n",
    "    def concat_podcast_parts(self, texts):\n",
    "        \"\"\"Given list of texts from chatGPT about the podcast.\"\"\"\n",
    "        chat = PodcastChat(max_length=12_000, **{**self._kwargs, 'topic': self.title})\n",
    "        msg = \"Rewrite the following podcast episode as one complete single episode.\\n\"\n",
    "        msg += \"\\n\".join(texts)\n",
    "        msg, aud = chat.step(msg=msg, model='gpt-3.5-turbo-16k', ret_aud=True, frequency_penalty=0.75)\n",
    "        if len(msg) < 500:\n",
    "            raise ValueError(f\"Returned msg too short. Suspecting an error. [{msg=}]\")\n",
    "        return msg, aud\n",
    "\n",
    "    def step(self):\n",
    "        include = f\" Remember to respond with the hosts names like {self.chat._hosts[0]}: and {self.chat._hosts[1]}:\"\n",
    "        outline = self.data[0].text\n",
    "        # logger.info(f\"Outline: {outline}\")\n",
    "        intro_msg = f\"Write the intro for a podcast about a paper: {self.title}. The abstract for the paper is {outline}. Only write the introduction.{include}\"\n",
    "\n",
    "        # Get parts\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=16) as tpe:\n",
    "            jobs = [tpe.submit(self.write_one_part, intro_msg)]\n",
    "            jobs.extend([\n",
    "                tpe.submit(self.write_one_part, f\"Rewrite the text from the paper {self.title} part {part.title} into a podcast section. Explain everything other than the title as if the listener has no idea. Do not include any intro such as saying welcome back, just get right to it. The text in the paper is: {part.text}.{include}\")\n",
    "                for part in self.data\n",
    "            ])\n",
    "            job2idx = {j:i for i, j in enumerate(jobs)}\n",
    "            self.texts  = [None] * len(jobs)\n",
    "            for i, job in enumerate(concurrent.futures.as_completed(jobs)):\n",
    "                logger.info(f\"Part: {i} / {len(jobs)} = {100.0*i/len(jobs):,.5f}%\")\n",
    "                jobid = job2idx[job]\n",
    "                text = job.result()\n",
    "                self.texts[jobid] = text\n",
    "\n",
    "            # Combine texts into one podcast and ask chatGPT to re-write it.\n",
    "            jobs = [\n",
    "                tpe.submit(self.concat_podcast_parts, self.texts[txt_i*self.join_num:(txt_i+1)*self.join_num])\n",
    "                for txt_i in range(0, len(self.texts), self.join_num)\n",
    "            ]\n",
    "            job2idx = {j:i for i, j in enumerate(jobs)}\n",
    "            self.sounds, self.summary_texts = [None] * len(jobs), [None] * len(jobs)\n",
    "            for i, job in enumerate(concurrent.futures.as_completed(jobs)):\n",
    "                logger.info(f\"Join Part: {i} / {len(jobs)} = {100.0*i/len(jobs):,.5f}%\")\n",
    "                jobid = job2idx[job]\n",
    "                text, aud = job.result()\n",
    "                self.sounds[jobid], self.summary_texts[jobid] = aud, text\n",
    "\n",
    "        return outline, '\\n'.join(self.summary_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a60a8a33-5d82-4025-b68b-9a66f88c6041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArxivEpisode(PDFEpisode):\n",
    "    ArxivPart = collections.namedtuple('ArxivPart', 'title text')\n",
    "\n",
    "    def __init__(self, arxiv_id, model='gpt-3.5-turbo-16k', **kwargs):\n",
    "        self.arxiv_id = arxiv_id\n",
    "        self.model = model\n",
    "        self.data = self.process_pdf(self.arxiv_id)\n",
    "        self.title = self.arxiv_title = self.get_title(self.arxiv_id)\n",
    "        self._kwargs = kwargs\n",
    "        super().__init__(title=self.arxiv_title, topic=self.arxiv_title, **kwargs)\n",
    "\n",
    "    def split_into_parts(self, text, max_tokens=8_000):\n",
    "        \"\"\"Split the text into parts based on tokens.\"\"\"\n",
    "        lines = text.split(\"\\n\")\n",
    "        parts = []\n",
    "        current_part = [text]\n",
    "        current_title = 'Paper'\n",
    "\n",
    "        while Chat.num_tokens_from_text('\\n'.join(current_part)) > max_tokens:\n",
    "            part_text = '\\n'.join(current_part)\n",
    "            shortened_part, current_part = part_text[:max_tokens * 2], [part_text[max_tokens * 2:]]\n",
    "            parts.append(self.ArxivPart(current_title, shortened_part))\n",
    "\n",
    "        if current_part:\n",
    "            parts.append(self.ArxivPart(current_title, \"\\n\".join(current_part)))\n",
    "        return parts\n",
    "\n",
    "    def process_pdf(self, arxiv_id):\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            file = os.path.join(tmpdir, \"file.pdf\")\n",
    "            self.arxiv_download(arxiv_id, file)\n",
    "            text = self.parse_pdf(file)\n",
    "        parts = self.split_into_parts(text)\n",
    "        return parts\n",
    "    \n",
    "    def arxiv_download(self, arxiv_id, out_file):\n",
    "        url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "        response = requests.get(url)\n",
    "        with open(out_file, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    \n",
    "    def get_title(self, arxiv_id):\n",
    "        url = f\"https://arxiv.org/abs/{arxiv_id}\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.find('h1', {'class': 'title mathjax'}).text.strip().split('\\n')[-1].strip()\n",
    "        return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "903b4d61-2440-4d6d-a987-61c794961852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommercialGenerator:\n",
    "    def get_random_company(self):\n",
    "        chat = Chat(\"Return simple plaintext responses only.\")\n",
    "        with open(\"nouns.txt\") as f:\n",
    "            nouns = f.read().splitlines()\n",
    "        random_noun = random.choice(nouns)\n",
    "        return chat.message(f\"Write just 1 funny, weird, creative made up company that doesn't exist involving {random_noun}.\", temperature=1)\n",
    "\n",
    "    def generate(self, company=None):\n",
    "        if company is None:\n",
    "            company = self.get_random_company()\n",
    "        chat = PodcastChat(f\"Very short commercial for {company}\", host_voices=[GttsTTS(GttsTTS.MAN), GttsTTS(GttsTTS.WOMAN)])\n",
    "        chat._history[-1] = {\"role\": \"user\", \"content\": f\"Generate a very funny, weird, and short commercial for {company}, who is sponsoring the podcast.\"}\n",
    "        return chat.step(frequency_penalty=1.2, max_tokens=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19acd44b-933e-46a1-94f7-5a59e4db5ede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArxivRunner:\n",
    "    def __init__(self, category, start=0, limit=5):\n",
    "        self.category = category\n",
    "        self.start = start\n",
    "        self.limit = limit\n",
    "\n",
    "    def get_top(self):\n",
    "        \"\"\"Retrieve top Arxiv entries based on category.\"\"\"\n",
    "        url = f'http://export.arxiv.org/api/query?search_query=cat:{self.category}&start={self.start}' \\\n",
    "              f'&max_results={self.limit}&sortBy=lastUpdatedDate&sortOrder=descending'\n",
    "        data = feedparser.parse(url)\n",
    "        return [entry['id'].split('/')[-1] for entry in data['entries']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b28cdd3-a742-4287-86dc-f9978743dfb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "JINGLE_FILE_PATH = 'jazzstep.mp3'\n",
    "MODEL = 'gpt-3.5-turbo-16k'\n",
    "HOST_VOICES = [GttsTTS(GttsTTS.MAN), GttsTTS(GttsTTS.WOMAN)]\n",
    "PODCAST_ARGS = (\"ArxivPodcastGPT\", \"ArxivPodcastGPT.github.io\", \"podcasts/ComputerScience/Consolidated/podcast.xml\")\n",
    "\n",
    "def create_large_episode(arxiv_category, limit=5):\n",
    "    \"\"\"Create a podcast episode with Arxiv papers.\"\"\"\n",
    "    with open(JINGLE_FILE_PATH, 'rb') as jingle_file:\n",
    "        jingle_audio = jingle_file.read()\n",
    "    jingle_audio = jingle_audio[:len(jingle_audio) // 4]  # Shorten to just 4 sec\n",
    "\n",
    "    audios, texts = [jingle_audio], []\n",
    "    \n",
    "    for arxiv_id in ArxivRunner(arxiv_category, limit=limit).get_top():\n",
    "        try:\n",
    "            arxiv_episode = ArxivEpisode(arxiv_id, model=MODEL, podcast_args=PODCAST_ARGS, host_voices=HOST_VOICES)\n",
    "            outline, txt = arxiv_episode.step()\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error processing arxiv_id {arxiv_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        audios.append(b''.join(arxiv_episode.sounds))\n",
    "        audios.append(jingle_audio)\n",
    "        arxiv_title = re.sub('[^0-9a-zA-Z]+', ' ', arxiv_episode.arxiv_title)\n",
    "        texts.append(f'ChatGPT generated podcast using model={MODEL} for https://arxiv.org/abs/{arxiv_id} {arxiv_title}')\n",
    "        \n",
    "        try:\n",
    "            commercial_text, commercial_sound = CommercialGenerator().generate()\n",
    "            audios.append(commercial_sound)\n",
    "            audios.append(jingle_audio)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Unable to generate commercial\")\n",
    "            logger.exception(e)\n",
    "    \n",
    "    return audios, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f318ebbc-418f-42a6-b9e5-86a4e564761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(texts):\n",
    "    chat = Chat(\"Return just simple plaintext.\")\n",
    "    return chat.message(\n",
    "        \"Given the following papers, write a clickbait title that captures all of them. \" + \n",
    "        \", \".join(txt.split(' Title ')[-1] for txt in texts)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24c116aa-d377-45e6-ae4a-9b8cb649605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCompletedEpisode(Episode):\n",
    "    def __init__(self, sounds, podcast_args):\n",
    "        self.sounds = sounds\n",
    "        self.pod = PodcastRSSFeed(*podcast_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e281630-31a3-4bfe-be29-31e946668fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_categories = [\"AI\", \"CL\", \"CC\", \"CE\", \"CG\", \"GT\", \"CV\", \"CY\", \"CR\", \"DS\", \"DB\", \"DL\", \"DM\", \"DC\", \"ET\", \"FL\", \"GL\", \"GR\", \"AR\", \"HC\", \"IR\", \"IT\", \"LO\", \"LG\", \"MS\", \"MA\", \"MM\", \"NI\", \"NE\", \"NA\", \"OS\", \"OH\", \"PF\", \"PL\", \"RO\", \"SI\", \"SE\", \"SD\", \"SC\", \"SY\"]\n",
    "\n",
    "def run(arxiv_category):\n",
    "    # TODO: Multi thread each part\n",
    "    audios, texts = create_large_episode(arxiv_category)\n",
    "    ep = AudioCompletedEpisode(audios, podcast_args=PODCAST_ARGS)\n",
    "    ep.upload(f'{datetime.datetime.now():%Y-%m-%d} {arxiv_category}: {get_title(texts)}', '\\n\\n'.join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb999590-295d-4181-a2db-e5791e0234a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"TODO:\n",
    "\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d70f5c1-4f60-4a2e-9825-406481b143d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2023-06-28 06:18:41\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mrequesting openai...\u001b[0m\n",
      "\u001b[2m2023-06-28 06:18:43\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mreceived openai...\u001b[0m\n",
      "\u001b[2m2023-06-28 06:18:43\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mrequesting openai...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# a, b = CommercialGenerator().generate()\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42913b5-c074-4563-a2e7-292b40eb42f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11",
   "language": "python",
   "name": "3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
