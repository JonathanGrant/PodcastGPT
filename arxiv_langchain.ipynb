{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "085ba2ee-6274-4e84-8ba1-a12b2fc3170b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ChatPodcastGPT import *\n",
    "import collections\n",
    "import concurrent.futures\n",
    "import os\n",
    "import feedparser\n",
    "import logging\n",
    "import re\n",
    "import tempfile\n",
    "from PyPDF2 import PdfReader\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import retrying\n",
    "import random\n",
    "from IPython.display import Audio\n",
    "import datetime\n",
    "\n",
    "MAX_TOKENS = 60_000 # GPT4-128k\n",
    "JOIN_NUM_DEFAULT = 300\n",
    "DEFAULT_TEXTGEN_MODEL = 'gpt-4-1106-preview'\n",
    "JINGLE_FILE_PATH = 'jazzstep.mp3'\n",
    "with open(JINGLE_FILE_PATH, 'rb') as jingle_file:\n",
    "    JINGLE_AUDIO = jingle_file.read()\n",
    "JINGLE_AUDIO = JINGLE_AUDIO[:len(JINGLE_AUDIO) // 4]  # Shorten to just 4 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65459282-8967-442a-88d9-3812b3e8c31a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PDFEpisode(Episode):\n",
    "    PDFPart = collections.namedtuple('PDFPart', 'title text')\n",
    "\n",
    "    def __init__(self, title, model=DEFAULT_TEXTGEN_MODEL, **kwargs):\n",
    "        self.title = title\n",
    "        self.model = model\n",
    "        self.topic = kwargs.pop('topic', self.title) or self.title\n",
    "        self._kwargs = kwargs\n",
    "        self.join_num = JOIN_NUM_DEFAULT\n",
    "        if 'podcast_args' in self._kwargs: self._kwargs.pop('podcast_args')\n",
    "        super().__init__(topic=self.topic, **kwargs)\n",
    "\n",
    "    def parse_pdf(self, file):\n",
    "        \"\"\"Parse a PDF and extract the text.\"\"\"\n",
    "        with open(file, \"rb\") as f:\n",
    "            pdf = PdfReader(f)\n",
    "            return ''.join(page.extract_text() for page in pdf.pages)\n",
    "\n",
    "    def split_into_parts(self, text, max_tokens=MAX_TOKENS):\n",
    "        \"\"\"Split the text into parts based on titles and tokens.\"\"\"\n",
    "        lines = text.split(\"\\n\")\n",
    "        parts = []\n",
    "        current_part = []\n",
    "        current_title = 'Paper'\n",
    "        for line in lines:\n",
    "            current_part.append(line)\n",
    "\n",
    "            while Chat.num_tokens_from_text('\\n'.join(current_part)) > max_tokens:\n",
    "                part_text = '\\n'.join(current_part)\n",
    "                shortened_part, current_part = part_text[:max_tokens * 2], [part_text[max_tokens * 2:]]\n",
    "                logger.info(\"PartAdd1\")\n",
    "                parts.append(self.PDFPart(current_title, shortened_part))\n",
    "\n",
    "        if current_part:\n",
    "            logger.info(\"PartAdd2\")\n",
    "            parts.append(self.PDFPart(current_title, \"\\n\".join(current_part)))\n",
    "        return parts\n",
    "\n",
    "    def process_pdf(self, pdf_path):\n",
    "        text = self.parse_pdf(pdf_path)\n",
    "        parts = self.split_into_parts(text)\n",
    "        return parts\n",
    "\n",
    "    @retrying.retry(stop_max_attempt_number=3, wait_fixed=2000)\n",
    "    def write_one_part(self, chat_msg, with_commercial=False):\n",
    "        chat = PodcastChat(**{**self._kwargs, 'topic': self.title})\n",
    "        msg, aud = chat.step(msg=chat_msg, model=self.model, ret_aud=True)\n",
    "        chat._history.pop(1)\n",
    "        com_msg, com_aud = chat.step(msg=\"Generate a funny, weird, and concise commercial for a company that now exists as a result of this paper.\", model=self.model, ret_aud=True)\n",
    "        msg = '\\n'.join([msg, com_msg])\n",
    "        aud = b''.join([aud, JINGLE_AUDIO, com_aud])\n",
    "        return msg, aud\n",
    "\n",
    "    def step(self):\n",
    "        outline = self.data[0].text\n",
    "\n",
    "        # Get parts\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=16) as tpe:\n",
    "            jobs = ([\n",
    "                tpe.submit(self.write_one_part, f\"\"\"Explain the paper \\\"{self.title}\\\" completely, in full verbose detail.\n",
    "Assume the listener doesn't know anything.\n",
    "Follow this guide:\n",
    "\n",
    "Introduction (500+ words)\n",
    "Contextual background: Why this paper is significant in its field.\n",
    "Key Concepts and Background: Explanation of the main scientific concepts or theories addressed in the paper.\n",
    "(Optional) Breakdown of complex vocabulary used.\n",
    "\n",
    "Core (2500+ words)\n",
    "Detailed discussion of the research paperâ€™s objectives.\n",
    "Methodology and techniques used.\n",
    "Key findings and results.\n",
    "\n",
    "Implications and Applications (500+ words)\n",
    "Analysis of the potential impact of these findings on the field.\n",
    "\n",
    "Conclusion (500+ words)\n",
    "Recap of the main points discussed in the episode.\n",
    "Personal reflections on the paper and its broader relevance.\n",
    "\n",
    "Respond with the hosts names before each line like {self.chat._hosts[0]}: and {self.chat._hosts[1]}:\n",
    "The text in the paper is:\n",
    "{part.text}\n",
    "\"\"\", with_commercial=True)\n",
    "                for part in self.data\n",
    "            ])\n",
    "            job2idx = {j:i for i, j in enumerate(jobs)}\n",
    "            self.sounds, self.summary_texts = [None] * len(jobs), [None] * len(jobs)\n",
    "            for i, job in enumerate(concurrent.futures.as_completed(jobs)):\n",
    "                logger.info(f\"Part: {i} / {len(jobs)} = {100.0*i/len(jobs):,.5f}%\")\n",
    "                jobid = job2idx[job]\n",
    "                text, aud = job.result()\n",
    "                self.summary_texts[jobid] = text\n",
    "                self.sounds[jobid] = aud\n",
    "\n",
    "        return outline, '\\n'.join(self.summary_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a60a8a33-5d82-4025-b68b-9a66f88c6041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArxivEpisode(PDFEpisode):\n",
    "    ArxivPart = collections.namedtuple('ArxivPart', 'title text')\n",
    "\n",
    "    def __init__(self, arxiv_id, id_is_url=False, title=None, model=DEFAULT_TEXTGEN_MODEL, **kwargs):\n",
    "        self.arxiv_id = arxiv_id\n",
    "        self.id_is_url = id_is_url\n",
    "        self.title = title\n",
    "        self.model = model\n",
    "        self.data = self.process_pdf(self.arxiv_id)\n",
    "        self.title = self.arxiv_title = self.get_title(self.arxiv_id)\n",
    "        self._kwargs = kwargs\n",
    "        super().__init__(title=self.arxiv_title, topic=self.arxiv_title, **kwargs)\n",
    "\n",
    "    def split_into_parts(self, text, max_tokens=MAX_TOKENS):\n",
    "        \"\"\"Split the text into parts based on tokens.\"\"\"\n",
    "        lines = text.split(\"\\n\")\n",
    "        parts = []\n",
    "        current_part = [text]\n",
    "        current_title = 'Paper'\n",
    "\n",
    "        while Chat.num_tokens_from_text('\\n'.join(current_part)) > max_tokens:\n",
    "            part_text = '\\n'.join(current_part)\n",
    "            shortened_part, current_part = part_text[:max_tokens * 2], [part_text[max_tokens * 2:]]\n",
    "            logger.info(\"PartAdd3\")\n",
    "            parts.append(self.ArxivPart(current_title, shortened_part))\n",
    "\n",
    "        if current_part:\n",
    "            logger.info(f\"PartAdd4, {len(parts)}\")\n",
    "            parts.append(self.ArxivPart(current_title, \"\\n\".join(current_part)))\n",
    "        if len(parts) > 1:\n",
    "            raise Exception(\"More than 1 part, giving up\")\n",
    "        return parts\n",
    "\n",
    "    def process_pdf(self, arxiv_id):\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            file = os.path.join(tmpdir, \"file.pdf\")\n",
    "            self.arxiv_download(arxiv_id, file)\n",
    "            text = self.parse_pdf(file)\n",
    "        parts = self.split_into_parts(text)\n",
    "        return parts\n",
    "    \n",
    "    def arxiv_download(self, arxiv_id, out_file):\n",
    "        if self.id_is_url:\n",
    "            url = arxiv_id\n",
    "        else:\n",
    "            url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "        response = requests.get(url)\n",
    "        with open(out_file, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    def get_title(self, arxiv_id):\n",
    "        if self.title is not None:\n",
    "            return self.title\n",
    "        url = f\"https://arxiv.org/abs/{arxiv_id}\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.find('h1', {'class': 'title mathjax'}).text.strip().split('\\n')[-1].strip()\n",
    "        return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "903b4d61-2440-4d6d-a987-61c794961852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommercialGenerator:\n",
    "    def get_random_company(self):\n",
    "        chat = Chat(\"Return simple plaintext responses only.\")\n",
    "        with open(\"nouns.txt\") as f:\n",
    "            nouns = f.read().splitlines()\n",
    "        random_noun = random.choice(nouns)\n",
    "        return chat.message(f\"Write just 1 funny, weird, creative made up company that doesn't exist involving {random_noun}.\", temperature=1)\n",
    "\n",
    "    def generate(self, company=None):\n",
    "        if company is None:\n",
    "            company = self.get_random_company()\n",
    "        chat = PodcastChat(f\"Very short commercial for {company}\", host_voices=[OpenAITTS(OpenAITTS.MAN), OpenAITTS(OpenAITTS.WOMAN)])\n",
    "        chat._history[-1] = {\"role\": \"user\", \"content\": f\"Generate a very funny, weird, and short commercial for {company}, who is sponsoring the podcast.\"}\n",
    "        return chat.step(model=DEFAULT_TEXTGEN_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "19acd44b-933e-46a1-94f7-5a59e4db5ede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArxivRunner:\n",
    "    def __init__(self, category, start=0, limit=5):\n",
    "        self.category = category\n",
    "        self.start = start\n",
    "        self.limit = limit\n",
    "\n",
    "    def get_top(self):\n",
    "        \"\"\"Retrieve top Arxiv entries based on category.\"\"\"\n",
    "        if self.category == 'psyarxiv': return self.get_top_psyarxiv()\n",
    "        url = f'https://arxiv.org/list/{self.category}/recent'\n",
    "        print(url)\n",
    "        html = requests.get(url).content\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        articles = []\n",
    "        \n",
    "        for item in soup.find_all('dt'):\n",
    "            title = item.find_next_sibling('dd').find('div', class_='list-title').text.replace('Title:', '').strip()\n",
    "            identifier = item.find('span', class_='list-identifier').a.text\n",
    "            pdf_link = 'https://arxiv.org' + item.find('span', class_='list-identifier').find('a', title='Download PDF')['href']\n",
    "        \n",
    "            articles.append({\n",
    "                'title': title,\n",
    "                'ID': identifier,\n",
    "                'pdf': pdf_link\n",
    "            })\n",
    "        return [a[\"pdf\"].split('/')[-1] for a in articles]\n",
    "\n",
    "    def get_top_psyarxiv(self):\n",
    "        url = 'https://share.osf.io/api/v3/index-card-search?cardSearchFilter%5BresourceType%5D=Preprint&cardSearchFilter%5Bpublisher%5D%5B%5D=https%3A%2F%2Fosf.io%2Fpreprints%2Fpsyarxiv&cardSearchFilter%5BaccessService%5D=https%3A%2F%2Fosf.io%2F&cardSearchText%5B*%2Ccreator.name%2CisContainedBy.creator.name%5D=&page%5Bcursor%5D=&page%5Bsize%5D=10&sort=-dateCreated'\n",
    "        print(url)\n",
    "        data = requests.get(url, headers={'Accept': 'application/vnd.api+json'}).json()\n",
    "        data = [x for x in data['included'] if x[\"type\"] == \"index-card\"]\n",
    "        return [(f\"{x['attributes']['resourceIdentifier'][0]}/download/\", x['attributes']['resourceMetadata']['title'][0]['@value']) for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8b28cdd3-a742-4287-86dc-f9978743dfb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL = DEFAULT_TEXTGEN_MODEL\n",
    "HOST_VOICES = [OpenAITTS(OpenAITTS.MAN), OpenAITTS(OpenAITTS.WOMAN)]\n",
    "PODCAST_ARGS = (\"ArxivPodcastGPT\", \"ArxivPodcastGPT.github.io\", \"podcasts/ComputerScience/Consolidated/podcast.xml\")\n",
    "\n",
    "def create_large_episode(arxiv_category, limit=5, add_commercials=False):\n",
    "    \"\"\"Create a podcast episode with Arxiv papers.\"\"\"\n",
    "    audios, texts = [JINGLE_AUDIO], []\n",
    "    successes = 0\n",
    "    \n",
    "    for arxiv_id in ArxivRunner(arxiv_category, limit=limit).get_top():\n",
    "        if successes >= limit:\n",
    "            break\n",
    "\n",
    "        arxiv_kwargs = {'id_is_url': False}\n",
    "        if isinstance(arxiv_id, tuple):\n",
    "            arxiv_id, arxiv_title = arxiv_id\n",
    "            arxiv_kwargs['title'] = arxiv_title\n",
    "            arxiv_kwargs['id_is_url'] = True\n",
    "            \n",
    "        logger.info(f\"Trying arxiv ID {arxiv_id} in {arxiv_category} with {successes}/{limit}\")\n",
    "        try:\n",
    "            arxiv_episode = ArxivEpisode(arxiv_id, model=MODEL, podcast_args=PODCAST_ARGS, host_voices=HOST_VOICES, **arxiv_kwargs)\n",
    "            outline, txt = arxiv_episode.step()\n",
    "            logger.info(f\"Got outline: {outline[:500]}\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error processing arxiv_id {arxiv_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        audios.append(b''.join(arxiv_episode.sounds))\n",
    "        audios.append(JINGLE_AUDIO)\n",
    "        arxiv_title = re.sub('[^0-9a-zA-Z]+', ' ', arxiv_episode.arxiv_title)\n",
    "        texts.append(f'ChatGPT generated podcast using model={MODEL} for https://arxiv.org/abs/{arxiv_id} {arxiv_title}')\n",
    "        successes += 1\n",
    "        logger.info(texts[-1])\n",
    "\n",
    "        if not add_commercials:\n",
    "            continue\n",
    "        try:\n",
    "            commercial_text, commercial_sound = CommercialGenerator().generate()\n",
    "            audios.append(commercial_sound)\n",
    "            audios.append(JINGLE_AUDIO)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Unable to generate commercial\")\n",
    "            logger.exception(e)\n",
    "    \n",
    "    return audios, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f318ebbc-418f-42a6-b9e5-86a4e564761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(texts):\n",
    "    chat = Chat(\"Return just simple plaintext.\")\n",
    "    return chat.message(\n",
    "        \"Given the following papers, write a clickbait title that captures all of them. \" + \n",
    "        \", \".join(txt.split(' Title ')[-1] for txt in texts)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "24c116aa-d377-45e6-ae4a-9b8cb649605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCompletedEpisode(Episode):\n",
    "    def __init__(self, sounds, podcast_args):\n",
    "        self.sounds = sounds\n",
    "        self.pod = PodcastRSSFeed(*podcast_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e281630-31a3-4bfe-be29-31e946668fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_categories = [\"AI\", \"CL\", \"CC\", \"CE\", \"CG\", \"GT\", \"CV\", \"CY\", \"CR\", \"DS\", \"DB\", \"DL\", \"DM\", \"DC\", \"ET\", \"FL\", \"GL\", \"GR\", \"AR\", \"HC\", \"IR\", \"IT\", \"LO\", \"LG\", \"MS\", \"MA\", \"MM\", \"NI\", \"NE\", \"NA\", \"OS\", \"OH\", \"PF\", \"PL\", \"RO\", \"SI\", \"SE\", \"SD\", \"SC\", \"SY\"]\n",
    "\n",
    "def run(arxiv_category, upload=True, limit=5):\n",
    "    audios, texts = create_large_episode(arxiv_category, limit=limit)\n",
    "    ep = AudioCompletedEpisode(audios, podcast_args=PODCAST_ARGS)\n",
    "    if upload:\n",
    "        ep.upload(f'{datetime.datetime.now():%Y-%m-%d} {arxiv_category}: {get_title(texts)}', '\\n\\n'.join(texts))\n",
    "    return ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb999590-295d-4181-a2db-e5791e0234a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ep = run(\"psyarxiv\", upload=True, limit=5)\n",
    "# IPython.display.Audio(b''.join(ep.sounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d70f5c1-4f60-4a2e-9825-406481b143d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ep.upload(f'{datetime.datetime.now():%Y-%m-%d} astro-ph', '\\n\\n'.join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42913b5-c074-4563-a2e7-292b40eb42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython.display.Audio(b''.join(ep.sounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bdc3eea-4804-4d02-a77c-3972e03062e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fe9503-17d2-4db1-be40-65b66d4497f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
