{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "085ba2ee-6274-4e84-8ba1-a12b2fc3170b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ChatPodcastGPT import *\n",
    "import collections\n",
    "import concurrent.futures\n",
    "import os\n",
    "import feedparser\n",
    "import logging\n",
    "import re\n",
    "import tempfile\n",
    "from PyPDF2 import PdfReader\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import retrying\n",
    "import random\n",
    "from IPython.display import Audio\n",
    "import datetime\n",
    "\n",
    "MAX_TOKENS = 120_000 # GPT4-128k\n",
    "JOIN_NUM_DEFAULT = 300\n",
    "DEFAULT_TEXTGEN_MODEL = 'gpt-4-1106-preview'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65459282-8967-442a-88d9-3812b3e8c31a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PDFEpisode(Episode):\n",
    "    PDFPart = collections.namedtuple('PDFPart', 'title text')\n",
    "\n",
    "    def __init__(self, title, model=DEFAULT_TEXTGEN_MODEL, **kwargs):\n",
    "        self.title = title\n",
    "        self.model = model\n",
    "        self.topic = kwargs.pop('topic', self.title) or self.title\n",
    "        self._kwargs = kwargs\n",
    "        self.join_num = JOIN_NUM_DEFAULT\n",
    "        if 'podcast_args' in self._kwargs: self._kwargs.pop('podcast_args')\n",
    "        super().__init__(topic=self.topic, **kwargs)\n",
    "\n",
    "    def parse_pdf(self, file):\n",
    "        \"\"\"Parse a PDF and extract the text.\"\"\"\n",
    "        with open(file, \"rb\") as f:\n",
    "            pdf = PdfReader(f)\n",
    "            return ''.join(page.extract_text() for page in pdf.pages)\n",
    "\n",
    "    def split_into_parts(self, text, max_tokens=MAX_TOKENS):\n",
    "        \"\"\"Split the text into parts based on titles and tokens.\"\"\"\n",
    "        lines = text.split(\"\\n\")\n",
    "        parts = []\n",
    "        current_part = []\n",
    "        current_title = 'Paper'\n",
    "        for line in lines:\n",
    "            current_part.append(line)\n",
    "\n",
    "            while Chat.num_tokens_from_text('\\n'.join(current_part)) > max_tokens:\n",
    "                part_text = '\\n'.join(current_part)\n",
    "                shortened_part, current_part = part_text[:max_tokens * 2], [part_text[max_tokens * 2:]]\n",
    "                logger.info(\"PartAdd1\")\n",
    "                parts.append(self.PDFPart(current_title, shortened_part))\n",
    "\n",
    "        if current_part:\n",
    "            logger.info(\"PartAdd2\")\n",
    "            parts.append(self.PDFPart(current_title, \"\\n\".join(current_part)))\n",
    "        return parts\n",
    "\n",
    "    def process_pdf(self, pdf_path):\n",
    "        text = self.parse_pdf(pdf_path)\n",
    "        parts = self.split_into_parts(text)\n",
    "        return parts\n",
    "\n",
    "    @retrying.retry(stop_max_attempt_number=3, wait_fixed=2000)\n",
    "    def write_one_part(self, chat_msg):\n",
    "        chat = PodcastChat(**{**self._kwargs, 'topic': self.title})\n",
    "        msg, aud = chat.step(msg=chat_msg, model=self.model, ret_aud=True)\n",
    "        return msg, aud\n",
    "\n",
    "    def step(self):\n",
    "        outline = self.data[0].text\n",
    "\n",
    "        # Get parts\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=16) as tpe:\n",
    "            jobs = ([\n",
    "                tpe.submit(self.write_one_part, f\"\"\"Explain the paper \\\"{self.title}\\\" in depth.\n",
    "Explain everything as if the listener has no idea.\n",
    "Respond with the hosts names before each line like {self.chat._hosts[0]}: and {self.chat._hosts[1]}:\n",
    "The text in the paper is:\n",
    "{part.text}\n",
    "\"\"\")\n",
    "                for part in self.data\n",
    "            ])\n",
    "            job2idx = {j:i for i, j in enumerate(jobs)}\n",
    "            self.sounds, self.summary_texts = [None] * len(jobs), [None] * len(jobs)\n",
    "            for i, job in enumerate(concurrent.futures.as_completed(jobs)):\n",
    "                logger.info(f\"Part: {i} / {len(jobs)} = {100.0*i/len(jobs):,.5f}%\")\n",
    "                jobid = job2idx[job]\n",
    "                text, aud = job.result()\n",
    "                self.summary_texts[jobid] = text\n",
    "                self.sounds[jobid] = aud\n",
    "\n",
    "        return outline, '\\n'.join(self.summary_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a8a33-5d82-4025-b68b-9a66f88c6041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArxivEpisode(PDFEpisode):\n",
    "    ArxivPart = collections.namedtuple('ArxivPart', 'title text')\n",
    "\n",
    "    def __init__(self, arxiv_id, model=DEFAULT_TEXTGEN_MODEL, **kwargs):\n",
    "        self.arxiv_id = arxiv_id\n",
    "        self.model = model\n",
    "        self.data = self.process_pdf(self.arxiv_id)\n",
    "        self.title = self.arxiv_title = self.get_title(self.arxiv_id)\n",
    "        self._kwargs = kwargs\n",
    "        super().__init__(title=self.arxiv_title, topic=self.arxiv_title, **kwargs)\n",
    "\n",
    "    def split_into_parts(self, text, max_tokens=MAX_TOKENS):\n",
    "        \"\"\"Split the text into parts based on tokens.\"\"\"\n",
    "        lines = text.split(\"\\n\")\n",
    "        parts = []\n",
    "        current_part = [text]\n",
    "        current_title = 'Paper'\n",
    "\n",
    "        while Chat.num_tokens_from_text('\\n'.join(current_part)) > max_tokens:\n",
    "            part_text = '\\n'.join(current_part)\n",
    "            shortened_part, current_part = part_text[:max_tokens * 2], [part_text[max_tokens * 2:]]\n",
    "            logger.info(\"PartAdd3\")\n",
    "            parts.append(self.ArxivPart(current_title, shortened_part))\n",
    "\n",
    "        if current_part:\n",
    "            logger.info(f\"PartAdd4, {len(parts)}\")\n",
    "            parts.append(self.ArxivPart(current_title, \"\\n\".join(current_part)))\n",
    "        if len(parts) > 1:\n",
    "            raise Exception(\"More than 1 part, giving up\")\n",
    "        return parts\n",
    "\n",
    "    def process_pdf(self, arxiv_id):\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            file = os.path.join(tmpdir, \"file.pdf\")\n",
    "            self.arxiv_download(arxiv_id, file)\n",
    "            text = self.parse_pdf(file)\n",
    "        parts = self.split_into_parts(text)\n",
    "        return parts\n",
    "    \n",
    "    def arxiv_download(self, arxiv_id, out_file):\n",
    "        url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "        response = requests.get(url)\n",
    "        with open(out_file, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    \n",
    "    def get_title(self, arxiv_id):\n",
    "        url = f\"https://arxiv.org/abs/{arxiv_id}\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.find('h1', {'class': 'title mathjax'}).text.strip().split('\\n')[-1].strip()\n",
    "        return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "903b4d61-2440-4d6d-a987-61c794961852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommercialGenerator:\n",
    "    def get_random_company(self):\n",
    "        chat = Chat(\"Return simple plaintext responses only.\")\n",
    "        with open(\"nouns.txt\") as f:\n",
    "            nouns = f.read().splitlines()\n",
    "        random_noun = random.choice(nouns)\n",
    "        return chat.message(f\"Write just 1 funny, weird, creative made up company that doesn't exist involving {random_noun}.\", temperature=1)\n",
    "\n",
    "    def generate(self, company=None):\n",
    "        if company is None:\n",
    "            company = self.get_random_company()\n",
    "        chat = PodcastChat(f\"Very short commercial for {company}\", host_voices=[OpenAITTS(OpenAITTS.MAN), OpenAITTS(OpenAITTS.WOMAN)])\n",
    "        chat._history[-1] = {\"role\": \"user\", \"content\": f\"Generate a very funny, weird, and short commercial for {company}, who is sponsoring the podcast.\"}\n",
    "        return chat.step(model=DEFAULT_TEXTGEN_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19acd44b-933e-46a1-94f7-5a59e4db5ede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArxivRunner:\n",
    "    def __init__(self, category, start=0, limit=5):\n",
    "        self.category = category\n",
    "        self.start = start\n",
    "        self.limit = limit\n",
    "\n",
    "    def get_top(self):\n",
    "        \"\"\"Retrieve top Arxiv entries based on category.\"\"\"\n",
    "        # url = f'http://export.arxiv.org/api/query?search_query=cat:{self.category}&start={self.start}' \\\n",
    "        #       f'&max_results={self.limit}&sortBy=submittedDate&sortOrder=descending'\n",
    "        url = f'https://arxiv.org/list/{self.category}/recent'\n",
    "        print(url)\n",
    "        html = requests.get(url).content\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        articles = []\n",
    "        \n",
    "        for item in soup.find_all('dt')[:self.limit]:\n",
    "            title = item.find_next_sibling('dd').find('div', class_='list-title').text.replace('Title:', '').strip()\n",
    "            identifier = item.find('span', class_='list-identifier').a.text\n",
    "            pdf_link = 'https://arxiv.org' + item.find('span', class_='list-identifier').find('a', title='Download PDF')['href']\n",
    "        \n",
    "            articles.append({\n",
    "                'title': title,\n",
    "                'ID': identifier,\n",
    "                'pdf': pdf_link\n",
    "            })\n",
    "        return [a[\"pdf\"].split('/')[-1] for a in articles]\n",
    "        \n",
    "        # data = feedparser.parse(url)\n",
    "        # return [entry['id'].split('/')[-1] for entry in data['entries']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b28cdd3-a742-4287-86dc-f9978743dfb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "JINGLE_FILE_PATH = 'jazzstep.mp3'\n",
    "MODEL = DEFAULT_TEXTGEN_MODEL\n",
    "HOST_VOICES = [OpenAITTS(OpenAITTS.MAN), OpenAITTS(OpenAITTS.WOMAN)]\n",
    "PODCAST_ARGS = (\"ArxivPodcastGPT\", \"ArxivPodcastGPT.github.io\", \"podcasts/ComputerScience/Consolidated/podcast.xml\")\n",
    "\n",
    "def create_large_episode(arxiv_category, limit=5):\n",
    "    \"\"\"Create a podcast episode with Arxiv papers.\"\"\"\n",
    "    with open(JINGLE_FILE_PATH, 'rb') as jingle_file:\n",
    "        jingle_audio = jingle_file.read()\n",
    "    jingle_audio = jingle_audio[:len(jingle_audio) // 4]  # Shorten to just 4 sec\n",
    "\n",
    "    audios, texts = [jingle_audio], []\n",
    "    \n",
    "    for arxiv_id in ArxivRunner(arxiv_category, limit=limit).get_top():\n",
    "        logger.info(f\"Trying arxiv ID {arxiv_id} in {arxiv_category} with limit {limit}\")\n",
    "        try:\n",
    "            arxiv_episode = ArxivEpisode(arxiv_id, model=MODEL, podcast_args=PODCAST_ARGS, host_voices=HOST_VOICES)\n",
    "            outline, txt = arxiv_episode.step()\n",
    "            logger.info(f\"Got outline: {outline}\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error processing arxiv_id {arxiv_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        audios.append(b''.join(arxiv_episode.sounds))\n",
    "        audios.append(jingle_audio)\n",
    "        arxiv_title = re.sub('[^0-9a-zA-Z]+', ' ', arxiv_episode.arxiv_title)\n",
    "        texts.append(f'ChatGPT generated podcast using model={MODEL} for https://arxiv.org/abs/{arxiv_id} {arxiv_title}')\n",
    "        \n",
    "        try:\n",
    "            commercial_text, commercial_sound = CommercialGenerator().generate()\n",
    "            audios.append(commercial_sound)\n",
    "            audios.append(jingle_audio)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Unable to generate commercial\")\n",
    "            logger.exception(e)\n",
    "    \n",
    "    return audios, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f318ebbc-418f-42a6-b9e5-86a4e564761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(texts):\n",
    "    chat = Chat(\"Return just simple plaintext.\")\n",
    "    return chat.message(\n",
    "        \"Given the following papers, write a clickbait title that captures all of them. \" + \n",
    "        \", \".join(txt.split(' Title ')[-1] for txt in texts)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24c116aa-d377-45e6-ae4a-9b8cb649605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCompletedEpisode(Episode):\n",
    "    def __init__(self, sounds, podcast_args):\n",
    "        self.sounds = sounds\n",
    "        self.pod = PodcastRSSFeed(*podcast_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e281630-31a3-4bfe-be29-31e946668fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_categories = [\"AI\", \"CL\", \"CC\", \"CE\", \"CG\", \"GT\", \"CV\", \"CY\", \"CR\", \"DS\", \"DB\", \"DL\", \"DM\", \"DC\", \"ET\", \"FL\", \"GL\", \"GR\", \"AR\", \"HC\", \"IR\", \"IT\", \"LO\", \"LG\", \"MS\", \"MA\", \"MM\", \"NI\", \"NE\", \"NA\", \"OS\", \"OH\", \"PF\", \"PL\", \"RO\", \"SI\", \"SE\", \"SD\", \"SC\", \"SY\"]\n",
    "\n",
    "def run(arxiv_category, upload=True, limit=5):\n",
    "    # TODO: Multi thread each part\n",
    "    audios, texts = create_large_episode(arxiv_category, limit=limit)\n",
    "    ep = AudioCompletedEpisode(audios, podcast_args=PODCAST_ARGS)\n",
    "    if upload:\n",
    "        ep.upload(f'{datetime.datetime.now():%Y-%m-%d} {arxiv_category}: {get_title(texts)}', '\\n\\n'.join(texts))\n",
    "    return ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb999590-295d-4181-a2db-e5791e0234a4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ep = run(\"econ\", upload=False, limit=5)\n",
    "# ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d70f5c1-4f60-4a2e-9825-406481b143d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ep.upload(f'{datetime.datetime.now():%Y-%m-%d} econ: {get_title(texts)}', '\\n\\n'.join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e42913b5-c074-4563-a2e7-292b40eb42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython.display.Audio(b''.join(ep.sounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc3eea-4804-4d02-a77c-3972e03062e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
